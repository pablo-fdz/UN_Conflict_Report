{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a1249d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install googlenewsdecoder\n",
    "# !pip install pygooglenews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8824c57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\blanc\\miniconda3\\envs\\tfm\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pygooglenews import GoogleNews\n",
    "import pprint\n",
    "from itertools import islice\n",
    "from googlenewsdecoder import gnewsdecoder\n",
    "from datetime import datetime, timedelta\n",
    "import polars as pl\n",
    "from tqdm.auto import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ec669f",
   "metadata": {},
   "source": [
    "We are going to define here the search term and start and end dates for our query as well as the language and country of the GoogleNews class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0297682",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching batches:  10%|█         | 1/10 [00:01<00:10,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ 100 articles between 2024-01-19 and 2024-01-22. Splitting batch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \n",
      "                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ 100 articles between 2024-01-10 and 2024-01-13. Splitting batch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                                \n",
      "                                                                \n",
      "                                                                \n",
      "                                                                \n",
      "\u001b[A                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ 100 articles between 2024-01-16 and 2024-01-19. Splitting batch...\n",
      "⚠️ 100 articles between 2024-01-01 and 2024-01-04. Splitting batch...\n",
      "⚠️ 100 articles between 2024-01-04 and 2024-01-07. Splitting batch...\n",
      "⚠️ 100 articles between 2024-01-13 and 2024-01-16. Splitting batch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \n",
      "                                                                \n",
      "Fetching batches:  10%|█         | 1/10 [00:01<00:10,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ 100 articles between 2024-01-07 and 2024-01-10. Splitting batch...\n",
      "⚠️ 100 articles between 2024-01-28 and 2024-01-31. Splitting batch...\n",
      "⚠️ 100 articles between 2024-01-22 and 2024-01-25. Splitting batch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Fetching batches: 100%|██████████| 3/3 [00:01<00:00,  2.01it/s]\n",
      "Fetching batches:  20%|██        | 2/10 [00:02<00:11,  1.41s/it]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Fetching batches: 100%|██████████| 3/3 [00:01<00:00,  2.11it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Fetching batches:  30%|███       | 3/10 [00:03<00:07,  1.09s/it]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Fetching batches: 100%|██████████| 3/3 [00:01<00:00,  1.89it/s]\n",
      "Fetching batches:  40%|████      | 4/10 [00:03<00:04,  1.34it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Fetching batches: 100%|██████████| 3/3 [00:01<00:00,  1.70it/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Fetching batches:  50%|█████     | 5/10 [00:03<00:02,  1.85it/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Fetching batches: 100%|██████████| 3/3 [00:02<00:00,  1.45it/s]\n",
      "Fetching batches: 100%|██████████| 3/3 [00:02<00:00,  1.47it/s]\n",
      "Fetching batches: 100%|██████████| 3/3 [00:02<00:00,  1.43it/s]\n",
      "Fetching batches: 100%|██████████| 3/3 [00:02<00:00,  1.46it/s]\n",
      "Fetching batches: 100%|██████████| 3/3 [00:02<00:00,  1.46it/s]]\n",
      "Fetching batches: 100%|██████████| 10/10 [00:04<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "TOTAL NUMBER OF ARTICLES: 851\n",
      "Average articles per day: 27\n",
      "shape: (5, 4)\n",
      "┌──────────────────────────┬──────────────────────────┬──────────────────┬─────────────────────────┐\n",
      "│ title                    ┆ google_link              ┆ published        ┆ source                  │\n",
      "│ ---                      ┆ ---                      ┆ ---              ┆ ---                     │\n",
      "│ str                      ┆ str                      ┆ str              ┆ str                     │\n",
      "╞══════════════════════════╪══════════════════════════╪══════════════════╪═════════════════════════╡\n",
      "│ UN peacekeeper among 52  ┆ https://news.google.com/ ┆ Mon, 29 Jan 2024 ┆ Anadolu Ajansı          │\n",
      "│ killed…                  ┆ rss/ar…                  ┆ 08:00:00 GMT     ┆                         │\n",
      "│ World Report 2024:       ┆ https://news.google.com/ ┆ Thu, 11 Jan 2024 ┆ Human Rights Watch      │\n",
      "│ Rights Tren…             ┆ rss/ar…                  ┆ 15:12:41 GMT     ┆                         │\n",
      "│ Uplifting Vulnerable     ┆ https://news.google.com/ ┆ Tue, 09 Jan 2024 ┆ World Bank              │\n",
      "│ South Sud…               ┆ rss/ar…                  ┆ 08:00:00 GMT     ┆                         │\n",
      "│ IOM Displacement         ┆ https://news.google.com/ ┆ Thu, 04 Jan 2024 ┆ ReliefWeb               │\n",
      "│ Tracking Matr…           ┆ rss/ar…                  ┆ 08:00:00 GMT     ┆                         │\n",
      "│ The United Nations Is    ┆ https://news.google.com/ ┆ Fri, 19 Jan 2024 ┆ Welcome to the United   │\n",
      "│ Not Leav…                ┆ rss/ar…                  ┆ 08:00:00 GMT     ┆ Nations                 │\n",
      "└──────────────────────────┴──────────────────────────┴──────────────────┴─────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "# 1. SET THE SEARCH PARAMETERS\n",
    "search_term = 'Sudan'\n",
    "start_date, end_date = '2024-01-01', '2024-01-31' # 1 month for now\n",
    "language, country = 'en', 'US' # country must be the isocode of the country\n",
    "\n",
    "# 2. INITIALIZE GOOGLE NEWS\n",
    "gn = GoogleNews(lang = language, country=country)\n",
    "\n",
    "# 3. SET UP DATE RANGE AND BATCHES\n",
    "start, end = datetime.strptime(start_date, '%Y-%m-%d'), datetime.strptime(end_date, '%Y-%m-%d')\n",
    "timestep = timedelta(days=1)  # because we want to get 100 articles per day\n",
    "days_per_batch = 3 # queries will be made for 3-day intervals\n",
    "\n",
    "def date_range_batches(start, end, days_per_batch):\n",
    "    current = start\n",
    "    while current < end:\n",
    "        yield current, min(current + timedelta(days=days_per_batch), end)\n",
    "        current += timedelta(days=days_per_batch)\n",
    "\n",
    "# 4. SET UP THE QUERY\n",
    "def get_articles(start_date, end_date):\n",
    "    try:\n",
    "        query = gn.search(query=search_term, from_=start_date, to_=end_date)\n",
    "        return query[\"entries\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Error on {start_date} to {end_date} query: {e}\")\n",
    "        return []\n",
    "    \n",
    "\n",
    "# 5. FETCH BATCHES OF ARTICLES \n",
    "max_workers = 16\n",
    "\n",
    "def fetch_batch(batch_start, batch_end, days_per_batch):\n",
    "    start_str = batch_start.strftime(\"%Y-%m-%d\")\n",
    "    end_str = batch_end.strftime(\"%Y-%m-%d\")\n",
    "    articles = get_articles(start_str, end_str)\n",
    "\n",
    "    if len(articles) >= 100 and days_per_batch > 1:\n",
    "        tqdm.write(f\"⚠️ 100 articles between {start_str} and {end_str}. Splitting batch...\")\n",
    "        return collect_articles(batch_start, batch_end, 1)\n",
    "    return articles\n",
    "\n",
    "def collect_articles(start_date, end_date, days_per_batch):\n",
    "    results = []\n",
    "    batches = date_range_batches(start_date, end_date, days_per_batch)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_range = {\n",
    "            executor.submit(fetch_batch, start, end, days_per_batch): (start, end)\n",
    "            for start, end in batches\n",
    "        }\n",
    "\n",
    "        for future in tqdm(as_completed(future_to_range), total=len(future_to_range), desc=\"Fetching batches\", dynamic_ncols=False, leave=True):\n",
    "            batch_results = future.result()\n",
    "            results.extend(batch_results)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run it\n",
    "articles = collect_articles(start, end, days_per_batch)\n",
    "\n",
    "# Format into Polars DataFrame\n",
    "rows = [\n",
    "    {\n",
    "        \"title\": article.get(\"title\"),\n",
    "        \"google_link\": article.get(\"link\"),\n",
    "        \"published\": article.get(\"published\"),\n",
    "        \"source\": article.get(\"source\", {}).get(\"title\") if article.get(\"source\") else None,\n",
    "    }\n",
    "    for article in articles\n",
    "]\n",
    "\n",
    "df = pl.DataFrame(rows).unique(subset=[\"google_link\"])\n",
    "\n",
    "print('------------------------------------------------------')\n",
    "print('TOTAL NUMBER OF ARTICLES:' , len(df['title']))  \n",
    "print('Average articles per day:', round(len(df['title']) / ((end - start).days + 1)))\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01764096",
   "metadata": {},
   "source": [
    "This method seems to be working really well -- we extract articles in 3-day batches and for those batches with 100 articles (max number) we scale down the batch size to one day. We could even do that again and get a few-hours batch size for days with a lot of activity, but that might just welcome a lot of noise and multiple articles about the same event. Consider.\n",
    "The time was 5-6seconds for a 31-day interval from which we obtained 850 articles.\n",
    "\n",
    "Next, we decode the URLs from Google News format to \"regular\" format, so we can obtain the text in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1124c7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decoding URLs: 100%|██████████| 851/851 [02:37<00:00,  5.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL NUMBER OF URLs DECODED: 851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "indexed_urls = list(enumerate(df['google_link'].to_list()))  # this will make a list of tuples (index, url)\n",
    "\n",
    "def decode_one_url(idx_url_tuple):\n",
    "    idx, url = idx_url_tuple\n",
    "    interval_time = 1  # interval is optional, default is None\n",
    "    #proxy = \"http://user:pass@localhost:8080\" # proxy is optional, default is None\n",
    "\n",
    "    try:\n",
    "        decoded_url = gnewsdecoder(url, \n",
    "                                   interval=interval_time, \n",
    "                                   #proxy=proxy\n",
    "                                   )\n",
    "        if decoded_url.get(\"status\"):\n",
    "            return (idx, decoded_url[\"decoded_url\"])\n",
    "        else:\n",
    "            tqdm.write(f\"Error: {decoded_url['message']}\")\n",
    "    except Exception as e:\n",
    "        tqdm.write(f\"Error occurred: {e}\")\n",
    "    return (idx, None)\n",
    "\n",
    "def decode_urls_concurrently(indexed_url_list, max_workers=16):\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(decode_one_url, (idx, url)): idx for idx, url in indexed_url_list}\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc='decoding URLs', dynamic_ncols=False, leave=True):\n",
    "            result = future.result() # (index, decoded_url)\n",
    "            if result:\n",
    "                results.append(result)\n",
    "    return results # results will be a list of tuples (index, decoded_url)\n",
    "\n",
    "decoded_urls = decode_urls_concurrently(indexed_urls)\n",
    "\n",
    "decoded_dict = dict(decoded_urls)\n",
    "final_decoded_list = [decoded_dict.get(i, None) for i in range(len(df))]\n",
    "\n",
    "df = df.with_columns(pl.Series(name=\"decoded_url\", values=final_decoded_list))\n",
    "\n",
    "print('TOTAL NUMBER OF URLs DECODED:', len(decoded_urls))\n",
    "#print('averge urls per day:', round(len(decoded_urls) / ((end - start).days + 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7128aa",
   "metadata": {},
   "source": [
    "This process is kinda slow -- took exactly 3min for 100 articles (31 min per 850 articles).\n",
    "\n",
    "We updated it to be done with ThreadPoolExecutor and then it took **4min30 for 850 articles** (see [info](https://www.digitalocean.com/community/tutorials/how-to-use-threadpoolexecutor-in-python-3-es) and [documentation](https://docs.python.org/3/library/concurrent.futures.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d499f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique decoded URLs: 851\n"
     ]
    }
   ],
   "source": [
    "decoded_urls = list(set(decoded_urls))  # Remove duplicates\n",
    "print(f\"Number of unique decoded URLs: {len(decoded_urls)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ac2e13",
   "metadata": {},
   "source": [
    "# 2. Extracting the articles' content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1aba641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install trafilatura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ec46447",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import trafilatura\n",
    "import polars as pl\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfc196cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching:  11%|███▍                            | 93/851 [00:07<00:55, 13.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[453] Error fetching article: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1010)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching:  50%|███████████████▋               | 429/851 [00:27<00:19, 22.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[383] Error fetching article: The read operation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching:  69%|█████████████████████▎         | 586/851 [00:36<00:15, 17.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6] Error fetching article: The read operation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching:  93%|████████████████████████████▋  | 789/851 [00:47<00:02, 21.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[232] Error fetching article: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1010)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching: 100%|███████████████████████████████| 851/851 [00:57<00:00, 14.85it/s]\n"
     ]
    }
   ],
   "source": [
    "# Fetch function\n",
    "def fetch_one_article(idx_url_tuple, timeout=15):\n",
    "    idx, url = idx_url_tuple\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 Chrome/114.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "    try:\n",
    "        with httpx.Client(headers=headers, follow_redirects=True, timeout=timeout) as client:\n",
    "            response = client.get(url)\n",
    "            if response.status_code == 200:\n",
    "                html = response.text\n",
    "                article_text = trafilatura.extract(html)\n",
    "                return (idx, article_text)\n",
    "    except Exception as e:\n",
    "        tqdm.write(f\"[{idx}] Error fetching article: {e}\")\n",
    "    return (idx, None)\n",
    "    \n",
    "\n",
    "# run concurrently and add results to a list\n",
    "full_text_tuples = []\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=16) as executor:\n",
    "    futures = {executor.submit(fetch_one_article, (idx, url)): idx for idx, url in decoded_urls}\n",
    "    full_text_tuples = [future.result() for future in tqdm(as_completed(futures), total=len(futures), desc=\"Fetching\", ncols=80)]\n",
    "\n",
    "# add to our dataframe\n",
    "full_texts_dict = dict(full_text_tuples)\n",
    "final_full_texts = [full_texts_dict.get(i, None) for i in range(len(df))]\n",
    "df = df.with_columns(pl.Series(name=\"full_text\", values=final_full_texts))\n",
    "df = df.unique(subset=[\"decoded_url\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38c8f0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRECTLY FETCHED ARTICLES: 650/851 (76.38%)\n"
     ]
    }
   ],
   "source": [
    "nulls = df.null_count()[\"full_text\"].item()\n",
    "percentage = ((len(df) - nulls) / len(df)) * 100\n",
    "print(f\"CORRECTLY FETCHED ARTICLES: {len(df) - nulls}/{len(df)} ({percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04659bc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>title</th><th>google_link</th><th>published</th><th>source</th><th>decoded_url</th><th>full_text</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;Algeria expresses solidarity w…</td><td>&quot;https://news.google.com/rss/ar…</td><td>&quot;Mon, 29 Jan 2024 08:00:00 GMT&quot;</td><td>&quot;Xinhua&quot;</td><td>&quot;https://english.news.cn/202401…</td><td>&quot;ALGIERS, Jan. 28 (Xinhua) -- A…</td></tr><tr><td>&quot;War in Sudan displaces over 50…</td><td>&quot;https://news.google.com/rss/ar…</td><td>&quot;Mon, 29 Jan 2024 08:00:00 GMT&quot;</td><td>&quot;African Business&quot;</td><td>&quot;https://african.business/2024/…</td><td>&quot;Download logo\n",
       "“More than 500,0…</td></tr><tr><td>&quot;Sudan Regional Crisis: Emergen…</td><td>&quot;https://news.google.com/rss/ar…</td><td>&quot;Tue, 30 Jan 2024 08:00:00 GMT&quot;</td><td>&quot;ReliefWeb&quot;</td><td>&quot;https://reliefweb.int/report/s…</td><td>&quot;OVERVIEW\n",
       "Throughout December 2…</td></tr><tr><td>&quot;Sudan conflict has displaced m…</td><td>&quot;https://news.google.com/rss/ar…</td><td>&quot;Sun, 07 Jan 2024 08:00:00 GMT&quot;</td><td>&quot;Sudan Tribune&quot;</td><td>&quot;https://sudantribune.com/artic…</td><td>null</td></tr><tr><td>&quot;54 killed in clashes in area c…</td><td>&quot;https://news.google.com/rss/ar…</td><td>&quot;Tue, 30 Jan 2024 08:00:00 GMT&quot;</td><td>&quot;Arab News&quot;</td><td>&quot;https://www.arabnews.com/node/…</td><td>null</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 6)\n",
       "┌────────────────┬────────────────┬────────────────┬───────────────┬───────────────┬───────────────┐\n",
       "│ title          ┆ google_link    ┆ published      ┆ source        ┆ decoded_url   ┆ full_text     │\n",
       "│ ---            ┆ ---            ┆ ---            ┆ ---           ┆ ---           ┆ ---           │\n",
       "│ str            ┆ str            ┆ str            ┆ str           ┆ str           ┆ str           │\n",
       "╞════════════════╪════════════════╪════════════════╪═══════════════╪═══════════════╪═══════════════╡\n",
       "│ Algeria        ┆ https://news.g ┆ Mon, 29 Jan    ┆ Xinhua        ┆ https://engli ┆ ALGIERS, Jan. │\n",
       "│ expresses      ┆ oogle.com/rss/ ┆ 2024 08:00:00  ┆               ┆ sh.news.cn/20 ┆ 28 (Xinhua)   │\n",
       "│ solidarity w…  ┆ ar…            ┆ GMT            ┆               ┆ 2401…         ┆ -- A…         │\n",
       "│ War in Sudan   ┆ https://news.g ┆ Mon, 29 Jan    ┆ African       ┆ https://afric ┆ Download logo │\n",
       "│ displaces over ┆ oogle.com/rss/ ┆ 2024 08:00:00  ┆ Business      ┆ an.business/2 ┆ “More than    │\n",
       "│ 50…            ┆ ar…            ┆ GMT            ┆               ┆ 024/…         ┆ 500,0…        │\n",
       "│ Sudan Regional ┆ https://news.g ┆ Tue, 30 Jan    ┆ ReliefWeb     ┆ https://relie ┆ OVERVIEW      │\n",
       "│ Crisis:        ┆ oogle.com/rss/ ┆ 2024 08:00:00  ┆               ┆ fweb.int/repo ┆ Throughout    │\n",
       "│ Emergen…       ┆ ar…            ┆ GMT            ┆               ┆ rt/s…         ┆ December 2…   │\n",
       "│ Sudan conflict ┆ https://news.g ┆ Sun, 07 Jan    ┆ Sudan Tribune ┆ https://sudan ┆ null          │\n",
       "│ has displaced  ┆ oogle.com/rss/ ┆ 2024 08:00:00  ┆               ┆ tribune.com/a ┆               │\n",
       "│ m…             ┆ ar…            ┆ GMT            ┆               ┆ rtic…         ┆               │\n",
       "│ 54 killed in   ┆ https://news.g ┆ Tue, 30 Jan    ┆ Arab News     ┆ https://www.a ┆ null          │\n",
       "│ clashes in     ┆ oogle.com/rss/ ┆ 2024 08:00:00  ┆               ┆ rabnews.com/n ┆               │\n",
       "│ area c…        ┆ ar…            ┆ GMT            ┆               ┆ ode/…         ┆               │\n",
       "└────────────────┴────────────────┴────────────────┴───────────────┴───────────────┴───────────────┘"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cbd802",
   "metadata": {},
   "source": [
    "# 3. Knowledge Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac12ef3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "# !pip install \"openai\" \"langchain\" \"neo4j\" \"pydantic\" \"tiktoken\"\n",
    "# !pip install fsspec langchain-text-splitters tiktoken python-dotenv numpy torch neo4j-graphrag google-genai langchain-google-genai \"neo4j-graphrag[sentence-transformers]\" polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a9dd52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import neo4j\n",
    "from langchain_google_genai import GoogleGenerativeAI # Import LangChain's GoogleGenerativeAI\n",
    "from neo4j_graphrag.generation import GraphRAG\n",
    "from neo4j_graphrag.llm import LLMInterface, LLMResponse\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from typing import Dict, Any, Optional\n",
    "import asyncio\n",
    "from typing import Any, List, Optional, Union\n",
    "from neo4j_graphrag.message_history import MessageHistory\n",
    "from neo4j_graphrag.types import LLMMessage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61c8b1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALL KEYS OBTAINED CORRECTLY\n"
     ]
    }
   ],
   "source": [
    "NEO4J_URI = os.getenv('NEO4J_URI')\n",
    "NEO4J_USERNAME = os.getenv('NEO4J_USERNAME')\n",
    "NEO4J_PASSWORD = os.getenv('NEO4J_PASSWORD')\n",
    "GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')\n",
    "\n",
    "if NEO4J_PASSWORD is not None and GEMINI_API_KEY is not None and NEO4J_URI is not None and NEO4J_USERNAME is not None:\n",
    "    print('ALL KEYS OBTAINED CORRECTLY')\n",
    "else:\n",
    "    print('There is an error with one of the keys!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dafc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = neo4j.GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
    "\n",
    "MODEL = \"gemini-2.5-flash-preview-04-17\"\n",
    "\n",
    "class GeminiLLM(LLMInterface):\n",
    "    \"\"\"\n",
    "    A custom LLM class for Google Gemini models that implements the Neo4j GraphRAG LLMInterface.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        model_name: str, \n",
    "        google_api_key: str, \n",
    "        model_params: Dict[str, Any] = None,\n",
    "        default_system_instruction: Optional[str] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the Gemini LLM.\n",
    "        \n",
    "        Args:\n",
    "            model_name: The name of the Gemini model to use (e.g., \"gemini-2.5-flash-preview-04-17\")\n",
    "            google_api_key: The Google API key to authenticate with Gemini\n",
    "            model_params: Optional parameters to pass to the model (e.g., temperature)\n",
    "            default_system_instruction: Default system prompt to use when none is provided\n",
    "        \"\"\"\n",
    "        # Initialize the parent class\n",
    "        super().__init__(model_name=model_name, model_params=model_params or {})\n",
    "        \n",
    "        # Store the API key\n",
    "        self.google_api_key = google_api_key\n",
    "        \n",
    "        # Store the default system instruction\n",
    "        self.default_system_instruction = default_system_instruction or \"You are a helpful AI assistant.\"\n",
    "        \n",
    "        # Initialize the LangChain Gemini model\n",
    "        self.llm = GoogleGenerativeAI(\n",
    "            model=self.model_name,\n",
    "            google_api_key=self.google_api_key,\n",
    "            **self.model_params\n",
    "        )\n",
    "    \n",
    "    def invoke(\n",
    "        self,\n",
    "        input: str,\n",
    "        message_history: Optional[Union[List[LLMMessage], MessageHistory]] = None,\n",
    "        system_instruction: Optional[str] = None,\n",
    "    ) -> LLMResponse:\n",
    "        \"\"\"\n",
    "        Invoke the Gemini model synchronously.\n",
    "        \n",
    "        Args:\n",
    "            input: The text prompt to send to the model\n",
    "            \n",
    "        Returns:\n",
    "            LLMResponse: An object containing the model's response\n",
    "        \"\"\"\n",
    "         # Implement how to handle system_instruction\n",
    "        # For example:\n",
    "        effective_system_instruction = system_instruction or self.default_system_instruction\n",
    "        \n",
    "        try:\n",
    "            # Get the response from the model\n",
    "            response = self.llm.invoke(input)\n",
    "            \n",
    "            # Return as LLMResponse object (tokens_used is not provided by the Gemini API through LangChain)\n",
    "            return LLMResponse(content=response)\n",
    "        except Exception as e:\n",
    "            # Handle any errors that might occur\n",
    "            error_message = f\"Error invoking Gemini model: {str(e)}\"\n",
    "            return LLMResponse(content=error_message)\n",
    "    \n",
    "    async def ainvoke(\n",
    "        self,\n",
    "        input: str,\n",
    "        message_history: Optional[Union[List[LLMMessage], MessageHistory]] = None,\n",
    "        system_instruction: Optional[str] = None,\n",
    "    ) -> LLMResponse:\n",
    "        \"\"\"\n",
    "        Invoke the Gemini model asynchronously.\n",
    "        \n",
    "        Args:\n",
    "            input: The text prompt to send to the model\n",
    "            \n",
    "        Returns:\n",
    "            LLMResponse: An object containing the model's response\n",
    "        \"\"\"\n",
    "        # Similar implementation for async version\n",
    "        effective_system_instruction = system_instruction or self.default_system_instruction\n",
    "\n",
    "        # Use run_in_executor to make the synchronous call asynchronous\n",
    "        # This is because the LangChain GoogleGenerativeAI doesn't have native async support\n",
    "        loop = asyncio.get_event_loop()\n",
    "        response = await loop.run_in_executor(None, self.invoke, input)\n",
    "        return response\n",
    "    \n",
    "\n",
    "llm = GeminiLLM(\n",
    "    model_name=MODEL,\n",
    "    google_api_key=GEMINI_API_KEY,\n",
    "    model_params={\"temperature\": 0.0}\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
