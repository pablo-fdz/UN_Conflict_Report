{
    "section_split": {
        "split_pattern": "(?m)^## (.+?)\\s*\\n(.*?)(?=^## |\\Z)"
    },
    "accuracy_evaluation": {
        "base_claims_prompt": "You are an AI tasked with extracting verifiable claims from a section of a report. A verifiable claim is an atomic statement that can be checked for accuracy and is relevant to the topic of the report. Here is an example:\nSection: \"The constant attacks of Group A and Group B on civilians in Country X have led to a high number of casualties as well as internally displaced persons (IDPs). In May 2025, there have been more IDPs than any other month that year.\"\n\nFrom this section, you can extract the following verifiable claims:\n1. \"Group A has been carrying constant attacks on civilians in Country X.\"\n2. \"Group B has been carrying constant attacks on civilians in Country X.\"\n3. \"In May 2025, there have been more IDPs than in any of the previous months of 2025.\"\n\nNotes on this:\n- Claims 1 and 2 have to be separate claims, because they refer to different groups, so one could be true while the other is false and that would make the first sentence incorrect.\n- The claim in the original section about a \"high number of casualties\" is not verifiable, because it is not quantifiable/specific. We do not know what \"high\" means and we do not have a reference to compare it too. Claims that are subjective or vague should not be extracted.\n- It is important that you always make all the necessary information explicit in the claims, without pronouns or terms that make references to previous sentences, so that each claim is self-contained and can be verified independently.\n- Also, when abbreviations of any kind are used, always include, if you know it with certainty, the full name/term plus the abbreviation in parentheses.\n\nYou must extract all verifiable claims from the following section of a report:\nSection: \"{section_text}\"",
        "base_questions_prompt": "You are a journalist tasked with evaluating the accuracy of a set of claims against a knowledge base.\nFor the given list of claims below, you must generate 1 to 4 questions aimed at leading you to the information needed to verify each claim.\nEach question should be specific, clear, and concise, designed to have a closed-ended objective answer.\n\nWhen abbreviations of any kind are used in the claim, always include in the question, if you know it with certainty, the full name/term plus the abbreviation in parentheses.\n\nHere is the list of claims:\n{claims_list}.",
        "base_eval_prompt": "You are an expert fact-checker. Your task is to evaluate a claim based on a set of questions and their corresponding answers, as well as a list of previously verified true claims. The answers are generated from a knowledge base. Based on all the information provided, determine if the claim is true, false, or a mixture of true and false.\n\n- **true**: The provided information fully supports the claim. The claim can also be considered true if it can be logically inferred from the previously verified true claims.\n- **false**: The provided information explicitly contradicts the claim.\n- **mixture**: The provided information partially supports the claim, supports some parts but not others, or is insufficient to make a full determination.\n\nHere is the claim and the supporting information:\n\n**Claim to Evaluate:**\n\"{claim_text}\"\n\n**Questions, Answers and Sources for the Claim:**\n{questions_and_answers_json}\n\n**Previously Verified True Claims (for context):**\n{previously_true_claims}\n\n",
        "llm_claims_config": {
            "model_name": "gemini-2.5-flash-lite-preview-06-17",
            "model_params": {
                "temperature": 0.0,
                "response_mime_type": "application/json"
            },
            "max_requests_per_minute": 15
        },
        "llm_questions_config": {
            "model_name": "gemini-2.5-flash-lite-preview-06-17",
            "model_params": {
                "temperature": 0.5,
                "response_mime_type": "application/json"
            },
            "max_requests_per_minute": 15
        },
        "llm_evaluator_config": {
                "model_name": "gemini-2.5-flash-lite-preview-06-17",
                "model_params": {
                    "temperature": 0.0,
                    "response_mime_type": "application/json"
                },
            "max_requests_per_minute": 15
            }
    },
    "retrievers": {
        "VectorRetriever": {
            "enabled": false,
            "return_properties": [
                "text"
            ],
            "search_params": {
                "top_k": 5
            }
        },
        "VectorCypherRetriever": {
            "enabled": false,
            "retrieval_query": "//1) Go out 2-3 hops in the entity graph and get relationships\nWITH node AS chunk\nMATCH (chunk)<-[:FROM_CHUNK]-()-[relList:!FROM_CHUNK]-{1,2}()\nUNWIND relList AS rel\n\n//2) collect relationships and text chunks\nWITH collect(DISTINCT chunk) AS chunks,\n collect(DISTINCT rel) AS rels\n\n//3) format and return context\nRETURN '=== text ===\\n' + apoc.text.join([c in chunks | c.text], '\\n---\\n') + '\\n\\n=== kg_rels ===\\n' +\n apoc.text.join([r in rels | startNode(r).name + ' - ' + type(r) + '(' + coalesce(r.details, '') + ')' +  ' -> ' + endNode(r).name ], '\\n---\\n') AS info",
            "search_params": {
                "top_k": 5
            }
        },
        "HybridRetriever": {
            "enabled": false,
            "return_properties": [
                "text"
            ],
            "search_params": {
                "top_k": 5,
                "ranker": "linear",
                "alpha": 0.5
            }
        },
        "HybridCypherRetriever": {
            "enabled": true,
            "retrieval_query": "//1) Go out 2-3 hops in the entity graph and get relationships\nWITH node AS chunk\nMATCH (chunk)<-[:FROM_CHUNK]-()-[relList:!FROM_CHUNK]-{1,2}()\nUNWIND relList AS rel\n\n//2) collect relationships and text chunks\nWITH collect(DISTINCT chunk) AS chunks,\n collect(DISTINCT rel) AS rels\n\n//3) format and return context\nRETURN '=== text ===\\n' + apoc.text.join([c in chunks | c.text], '\\n---\\n') + '\\n\\n=== kg_rels ===\\n' +\n apoc.text.join([r in rels | startNode(r).name + ' - ' + type(r) + '(' + coalesce(r.details, '') + ')' +  ' -> ' + endNode(r).name ], '\\n---\\n') AS info",
            "search_params": {
                    "top_k": 5,
                    "ranker": "linear",
                    "alpha": 0.5
            }
        }
    },
    "graphrag": {
        "llm_config": {
            "model_name": "gemini-2.5-flash-lite-preview-06-17",
            "model_params": {
                "temperature": 0.0,
                "response_mime_type": "application/json"
            },
            "max_requests_per_minute": 15
        },
        "rag_template_config": {
            "template": "# Question:\n{query_text}\n \n# Context:\n{context}\n \n# Examples:\n{examples}\n \n# Answer:\n",
            "system_instructions": "Answer the Question using the following Context. Only respond with information mentioned in the Context. Do not inject any speculative information not mentioned. If no examples are provided, omit the Examples section in your answer."
        },
        "query_text": "Using the provided `Context` below, answer the following questions about the `Claim`. Answer concisely and truthfully, without making assumptions or adding information beyond what is provided in the `Context`. Always provide the source of the answer, cited in the following way: \"<author (newspaper or physical person)>: <url>\". If there is not enough information in the `Context` to answer a question, state that by answering with \"Not enough information to answer this question.\". Leave the source empty in that case.\n\nHere is the claim and the questions:\nClaim: {claim}\nQuestions: {questions}",
        "examples": "Claim: \"The UN has reported a significant increase in the number of internally displaced persons (IDPs) in Country X due to ongoing conflicts.\"\nQuestions: [\"What is the current number of IDPs in Country X?\", \"What are the main causes of the increase in IDPs in Country X?\", \"How does the current situation compare to previous years?\"]\n\nExample output: {\"What is the current number of IDPs in Country X?\": \"Not enough information to answer this question.\", \"What are the main causes of the increase in IDPs in Country X?\": \"Ongoing conflicts and violence.\", \"How does the current situation compare to previous years?\": \"There has been a significant increase compared to previous years.\"}"
    },
    "rewrite_config": {
        "enabled": true,
        "rewrite_prompt": "You are an expert editor tasked with revising a report section based on a factual accuracy analysis. Your goal is to produce a corrected, well-written, and professionally toned narrative for the final report.\n\n**Instructions:**\n1.  Read the **Original Content** of the section.\n2.  Review the **Accuracy Analysis**, which contains a claim-by-claim evaluation (TRUE, FALSE, MIXTURE) of the original content, along with justifications and specific sources.\n3.  Rewrite the **Original Content** to create a new **Corrected Content**. You must:\n    -   Keep all information that was verified as TRUE.\n    -   Correct or remove information that was identified as FALSE, using the justification from the analysis.\n    -   Clarify and adjust information that was identified as a MIXTURE.\n    -   Ensure the final text is coherent, objective, and maintains a formal tone suitable for a security report.\n4.  As you rewrite, you must cite the sources provided in the **Accuracy Analysis** for all claims. Use footnote-style citations (e.g., [1], [2]).\n5.  Compile all of the sources you cited in the rewritten text. \n\n**Inputs:**\n\n**1. Section Title:**\n`{section_title}`\n\n**2. Original Content:**\n`{original_content}`\n\n**3. Accuracy Analysis:**\n`{accuracy_content}`\n\n**4. General Report Sources (for context only):**\n`{report_sources}`",
        "aggregation_prompt": "You are an expert technical editor specializing in consolidating and formatting reports. Your task is to process an intermediate markdown report, consolidate its sources, and produce a final, clean version.\n\n**Input Document Structure:**\nThe provided markdown document contains several sections, each starting with a level-2 heading (`##`). Each section contains:\n1.  Rewritten content with in-text, footnote-style citations (e.g., [1], [2]).\n2.  A subsection with a level-3 heading (`### Sources`) that lists the sources for that section. The numbering of these sources is local to each section and may be duplicated across the document.\n\n**Your Instructions:**\n1.  **Consolidate Sources:** Read all the `### Sources` subsections. Identify all unique sources from the entire document and create a single, consolidated list.\n2.  **Re-number Citations:** Re-number the consolidated sources sequentially starting from 1. Then, go through the main text of each section and update all in-text citations (e.g., `[1]`, `[2]`) to match the new, global numbering of the consolidated source list.\n3.  **Final Formatting:**\n    -   Remove all the intermediate `### Sources` subsections.\n    -   Create a single, final `## Sources` section at the very end of the document.\n    -   Populate this final section with the consolidated, de-duplicated, and correctly numbered list of sources.\n    -   Ensure the entire document is coherent and correctly formatted in markdown.\n\n**Input Document:**\n```markdown\n{intermediate_report}\n```\n\n**Output:**\nReturn only the full, corrected markdown text of the final report. Do not include any other commentary or explanation.",
        "llm_rewriter_config": {
            "model_name": "gemini-2.5-flash",
            "model_params": {
                "temperature": 0.0,
                "response_mime_type": "application/json"
            },
            "max_requests_per_minute": 10
        },
        "llm_aggregator_config": {
            "model_name": "gemini-2.5-flash",
            "model_params": {
                "temperature": 0.0
            },
            "max_requests_per_minute": 10
        },
        "save_intermediate_report": true
    }
}