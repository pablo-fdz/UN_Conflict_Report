{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35ae0c9b",
   "metadata": {},
   "source": [
    "# 0. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2a8341e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory (graphrag_pipeline) to the Python path (needed for importing\n",
    "# modules in parent directory)\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "# Utilities\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "from google import genai\n",
    "import polars as pl\n",
    "from library.kg_builder import CustomKGPipeline, build_kg_from_df\n",
    "from library.kg_builder.utilities import GeminiLLM\n",
    "from neo4j_graphrag.experimental.components.resolver import (\n",
    "    SpaCySemanticMatchResolver, FuzzyMatchResolver, SinglePropertyExactMatchResolver\n",
    ")\n",
    "import tqdm.notebook as tqdm\n",
    "\n",
    "# Neo4j and Neo4j GraphRAG imports\n",
    "import neo4j\n",
    "from neo4j_graphrag.embeddings import SentenceTransformerEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5182b7fc",
   "metadata": {},
   "source": [
    "Let's first check the available Gemini models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03b114e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='models/embedding-gecko-001' display_name='Embedding Gecko' description='Obtain a distributed representation of a text.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1024 output_token_limit=1 supported_actions=['embedText', 'countTextTokens'] default_checkpoint_id=None checkpoints=None\n",
      "name='models/gemini-1.0-pro-vision-latest' display_name='Gemini 1.0 Pro Vision' description='The original Gemini 1.0 Pro Vision model version which was optimized for image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. Move to a newer Gemini version.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=12288 output_token_limit=4096 supported_actions=['generateContent', 'countTokens'] default_checkpoint_id=None checkpoints=None\n",
      "name='models/gemini-pro-vision' display_name='Gemini 1.0 Pro Vision' description='The original Gemini 1.0 Pro Vision model version which was optimized for image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. Move to a newer Gemini version.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=12288 output_token_limit=4096 supported_actions=['generateContent', 'countTokens'] default_checkpoint_id=None checkpoints=None\n",
      "name='models/gemini-1.5-pro-latest' display_name='Gemini 1.5 Pro Latest' description='Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=2000000 output_token_limit=8192 supported_actions=['generateContent', 'countTokens'] default_checkpoint_id=None checkpoints=None\n",
      "name='models/gemini-1.5-pro-001' display_name='Gemini 1.5 Pro 001' description='Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in May of 2024.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=2000000 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'createCachedContent'] default_checkpoint_id=None checkpoints=None\n",
      "name='models/gemini-1.5-pro-002' display_name='Gemini 1.5 Pro 002' description='Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in September of 2024.' version='002' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=2000000 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'createCachedContent'] default_checkpoint_id=None checkpoints=None\n",
      "name='models/gemini-1.5-pro' display_name='Gemini 1.5 Pro' description='Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in May of 2024.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=2000000 output_token_limit=8192 supported_actions=['generateContent', 'countTokens'] default_checkpoint_id=None checkpoints=None\n",
      "name='models/gemini-1.5-flash-latest' display_name='Gemini 1.5 Flash Latest' description='Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1000000 output_token_limit=8192 supported_actions=['generateContent', 'countTokens'] default_checkpoint_id=None checkpoints=None\n",
      "name='models/gemini-1.5-flash-001' display_name='Gemini 1.5 Flash 001' description='Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in May of 2024.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1000000 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'createCachedContent'] default_checkpoint_id=None checkpoints=None\n",
      "name='models/gemini-1.5-flash-001-tuning' display_name='Gemini 1.5 Flash 001 Tuning' description='Version of Gemini 1.5 Flash that supports tuning, our fast and versatile multimodal model for scaling across diverse tasks, released in May of 2024.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=16384 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'createTunedModel'] default_checkpoint_id=None checkpoints=None\n",
      "name='models/gemini-1.5-flash' display_name='Gemini 1.5 Flash' description='Alias that points to the most recent stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1000000 output_token_limit=8192 supported_actions=['generateContent', 'countTokens'] default_checkpoint_id=None checkpoints=None\n",
      "name='models/gemini-1.5-flash-002' display_name='Gemini 1.5 Flash 002' description='Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in September of 2024.' version='002' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1000000 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'createCachedContent'] default_checkpoint_id=None checkpoints=None\n",
      "name='models/gemini-1.5-flash-8b' display_name='Gemini 1.5 Flash-8B' description='Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1000000 output_token_limit=8192 supported_actions=['createCachedContent', 'generateContent', 'countTokens'] default_checkpoint_id=None checkpoints=None\n",
      "name='models/gemini-1.5-flash-8b-001' display_name='Gemini 1.5 Flash-8B 001' description='Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1000000 output_token_limit=8192 supported_actions=['createCachedContent', 'generateContent', 'countTokens'] default_checkpoint_id=None checkpoints=None\n",
      "name='models/gemini-1.5-flash-8b-latest' display_name='Gemini 1.5 Flash-8B Latest' description='Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1000000 output_token_limit=8192 supported_actions=['createCachedContent', 'generateContent', 'countTokens'] default_checkpoint_id=None checkpoints=None\n",
      "name='models/gemini-1.5-flash-8b-exp-0827' display_name='Gemini 1.5 Flash 8B Experimental 0827' description='Experimental release (August 27th, 2024) of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model. Replaced by Gemini-1.5-flash-8b-001 (stable).' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1000000 output_token_limit=8192 supported_actions=['generateContent', 'countTokens'] default_checkpoint_id=None checkpoints=None\n",
      "name='models/gemini-1.5-flash-8b-exp-0924' display_name='Gemini 1.5 Flash 8B Experimental 0924' description='Experimental release (September 24th, 2024) of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model. Replaced by Gemini-1.5-flash-8b-001 (stable).' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1000000 output_token_limit=8192 supported_actions=['generateContent', 'countTokens'] default_checkpoint_id=None checkpoints=None\n",
      "name='models/gemini-2.5-pro-exp-03-25' display_name='Gemini 2.5 Pro Experimental 03-25' description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro' version='2.5-exp-03-25' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None\n",
      "name='models/gemini-2.5-pro-preview-03-25' display_name='Gemini 2.5 Pro Preview 03-25' description='Gemini 2.5 Pro Preview 03-25' version='2.5-preview-03-25' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None\n",
      "name='models/gemini-2.5-flash-preview-04-17' display_name='Gemini 2.5 Flash Preview 04-17' description='Preview release (April 17th, 2025) of Gemini 2.5 Flash' version='2.5-preview-04-17' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None\n",
      "name='models/gemini-2.5-flash-preview-05-20' display_name='Gemini 2.5 Flash Preview 05-20' description='Preview release (April 17th, 2025) of Gemini 2.5 Flash' version='2.5-preview-05-20' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None\n",
      "name='models/gemini-2.5-flash-preview-04-17-thinking' display_name='Gemini 2.5 Flash Preview 04-17 for cursor testing' description='Preview release (April 17th, 2025) of Gemini 2.5 Flash' version='2.5-preview-04-17' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None\n",
      "name='models/gemini-2.5-pro-preview-05-06' display_name='Gemini 2.5 Pro Preview 05-06' description='Preview release (May 6th, 2025) of Gemini 2.5 Pro' version='2.5-preview-05-06' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None\n",
      "name='models/gemini-2.5-pro-preview-06-05' display_name='Gemini 2.5 Pro Preview' description='Preview release (June 5th, 2025) of Gemini 2.5 Pro' version='2.5-preview-06-05' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None\n",
      "name='models/gemini-2.0-flash-exp' display_name='Gemini 2.0 Flash Experimental' description='Gemini 2.0 Flash Experimental' version='2.0' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'bidiGenerateContent'] default_checkpoint_id=None checkpoints=None\n",
      "name='models/gemini-2.0-flash' display_name='Gemini 2.0 Flash' description='Gemini 2.0 Flash' version='2.0' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None\n",
      "name='models/gemini-2.0-flash-001' display_name='Gemini 2.0 Flash 001' description='Stable version of Gemini 2.0 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in January of 2025.' version='2.0' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None\n",
      "name='models/gemini-2.0-flash-lite-001' display_name='Gemini 2.0 Flash-Lite 001' description='Stable version of Gemini 2.0 Flash Lite' version='2.0' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None\n",
      "name='models/gemini-2.0-flash-lite' display_name='Gemini 2.0 Flash-Lite' description='Gemini 2.0 Flash-Lite' version='2.0' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None\n",
      "name='models/gemini-2.0-flash-lite-preview-02-05' display_name='Gemini 2.0 Flash-Lite Preview 02-05' description='Preview release (February 5th, 2025) of Gemini 2.0 Flash Lite' version='preview-02-05' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None\n",
      "name='models/gemini-2.0-flash-lite-preview' display_name='Gemini 2.0 Flash-Lite Preview' description='Preview release (February 5th, 2025) of Gemini 2.0 Flash Lite' version='preview-02-05' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None\n",
      "name='models/gemini-2.0-pro-exp' display_name='Gemini 2.0 Pro Experimental' description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro' version='2.5-exp-03-25' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None\n",
      "name='models/gemini-2.0-pro-exp-02-05' display_name='Gemini 2.0 Pro Experimental 02-05' description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro' version='2.5-exp-03-25' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None\n",
      "name='models/gemini-exp-1206' display_name='Gemini Experimental 1206' description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro' version='2.5-exp-03-25' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None\n",
      "name='models/gemini-2.0-flash-thinking-exp-01-21' display_name='Gemini 2.5 Flash Preview 04-17' description='Preview release (April 17th, 2025) of Gemini 2.5 Flash' version='2.5-preview-04-17' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None\n",
      "name='models/gemini-2.0-flash-thinking-exp' display_name='Gemini 2.5 Flash Preview 04-17' description='Preview release (April 17th, 2025) of Gemini 2.5 Flash' version='2.5-preview-04-17' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None\n",
      "name='models/gemini-2.0-flash-thinking-exp-1219' display_name='Gemini 2.5 Flash Preview 04-17' description='Preview release (April 17th, 2025) of Gemini 2.5 Flash' version='2.5-preview-04-17' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None\n",
      "name='models/gemini-2.5-flash-preview-tts' display_name='Gemini 2.5 Flash Preview TTS' description='Gemini 2.5 Flash Preview TTS' version='gemini-2.5-flash-exp-tts-2025-05-19' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=32768 output_token_limit=8192 supported_actions=['countTokens', 'generateContent'] default_checkpoint_id=None checkpoints=None\n",
      "name='models/gemini-2.5-pro-preview-tts' display_name='Gemini 2.5 Pro Preview TTS' description='Gemini 2.5 Pro Preview TTS' version='gemini-2.5-pro-preview-tts-2025-05-19' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=65536 output_token_limit=65536 supported_actions=['countTokens', 'generateContent'] default_checkpoint_id=None checkpoints=None\n",
      "name='models/learnlm-2.0-flash-experimental' display_name='LearnLM 2.0 Flash Experimental' description='LearnLM 2.0 Flash Experimental' version='2.0' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=32768 supported_actions=['generateContent', 'countTokens'] default_checkpoint_id=None checkpoints=None\n",
      "name='models/gemma-3-1b-it' display_name='Gemma 3 1B' description=None version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=32768 output_token_limit=8192 supported_actions=['generateContent', 'countTokens'] default_checkpoint_id=None checkpoints=None\n",
      "name='models/gemma-3-4b-it' display_name='Gemma 3 4B' description=None version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=32768 output_token_limit=8192 supported_actions=['generateContent', 'countTokens'] default_checkpoint_id=None checkpoints=None\n",
      "name='models/gemma-3-12b-it' display_name='Gemma 3 12B' description=None version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=32768 output_token_limit=8192 supported_actions=['generateContent', 'countTokens'] default_checkpoint_id=None checkpoints=None\n",
      "name='models/gemma-3-27b-it' display_name='Gemma 3 27B' description=None version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=131072 output_token_limit=8192 supported_actions=['generateContent', 'countTokens'] default_checkpoint_id=None checkpoints=None\n",
      "name='models/gemma-3n-e4b-it' display_name='Gemma 3n E4B' description=None version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=8192 output_token_limit=2048 supported_actions=['generateContent', 'countTokens'] default_checkpoint_id=None checkpoints=None\n",
      "name='models/embedding-001' display_name='Embedding 001' description='Obtain a distributed representation of a text.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=2048 output_token_limit=1 supported_actions=['embedContent'] default_checkpoint_id=None checkpoints=None\n",
      "name='models/text-embedding-004' display_name='Text Embedding 004' description='Obtain a distributed representation of a text.' version='004' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=2048 output_token_limit=1 supported_actions=['embedContent'] default_checkpoint_id=None checkpoints=None\n",
      "name='models/gemini-embedding-exp-03-07' display_name='Gemini Embedding Experimental 03-07' description='Obtain a distributed representation of a text.' version='exp-03-07' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=8192 output_token_limit=1 supported_actions=['embedContent', 'countTextTokens'] default_checkpoint_id=None checkpoints=None\n",
      "name='models/gemini-embedding-exp' display_name='Gemini Embedding Experimental' description='Obtain a distributed representation of a text.' version='exp-03-07' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=8192 output_token_limit=1 supported_actions=['embedContent', 'countTextTokens'] default_checkpoint_id=None checkpoints=None\n",
      "name='models/aqa' display_name='Model that performs Attributed Question Answering.' description='Model trained to return answers to questions that are grounded in provided sources, along with estimating answerable probability.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=7168 output_token_limit=1024 supported_actions=['generateAnswer'] default_checkpoint_id=None checkpoints=None\n",
      "name='models/imagen-3.0-generate-002' display_name='Imagen 3.0 002 model' description='Vertex served Imagen 3.0 002 model' version='002' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=480 output_token_limit=8192 supported_actions=['predict'] default_checkpoint_id=None checkpoints=None\n",
      "name='models/veo-2.0-generate-001' display_name='Veo 2' description='Vertex served Veo 2 model. Access to this model requires billing to be enabled on the associated Google Cloud Platform account. Please visit https://console.cloud.google.com/billing to enable it.' version='2.0' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=480 output_token_limit=8192 supported_actions=['predictLongRunning'] default_checkpoint_id=None checkpoints=None\n",
      "name='models/gemini-2.5-flash-preview-native-audio-dialog' display_name='Gemini 2.5 Flash Preview Native Audio Dialog' description='Gemini 2.5 Flash Preview Native Audio Dialog' version='gemini-2.5-flash-preview-native-audio-dialog-2025-05-19' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=131072 output_token_limit=8192 supported_actions=['countTokens', 'bidiGenerateContent'] default_checkpoint_id=None checkpoints=None\n",
      "name='models/gemini-2.5-flash-preview-native-audio-dialog-rai-v3' display_name='Gemini 2.5 Flash Preview Native Audio Dialog RAI v3' description='Gemini 2.5 Flash Preview Native Audio Dialog RAI v3' version='gemini-2.5-flash-preview-native-audio-dialog-2025-05-19' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=131072 output_token_limit=8192 supported_actions=['countTokens', 'bidiGenerateContent'] default_checkpoint_id=None checkpoints=None\n",
      "name='models/gemini-2.5-flash-exp-native-audio-thinking-dialog' display_name='Gemini 2.5 Flash Exp Native Audio Thinking Dialog' description='Gemini 2.5 Flash Exp Native Audio Thinking Dialog' version='gemini-2.5-flash-exp-native-audio-thinking-dialog-2025-05-19' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=131072 output_token_limit=8192 supported_actions=['countTokens', 'bidiGenerateContent'] default_checkpoint_id=None checkpoints=None\n",
      "name='models/gemini-2.0-flash-live-001' display_name='Gemini 2.0 Flash 001' description='Gemini 2.0 Flash 001' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=131072 output_token_limit=8192 supported_actions=['bidiGenerateContent', 'countTokens'] default_checkpoint_id=None checkpoints=None\n"
     ]
    }
   ],
   "source": [
    "load_dotenv('.env', override=True)\n",
    "\n",
    "gemini_api_key = os.getenv('GEMINI_API_KEY')\n",
    "\n",
    "if gemini_api_key:\n",
    "    client = genai.Client(api_key=gemini_api_key)  # Configure the API key for genai\n",
    "else:\n",
    "    raise ValueError(\"GEMINI_API_KEY environment variable is not set.\")\n",
    "\n",
    "# Display available models\n",
    "for model in client.models.list():\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ad2805",
   "metadata": {},
   "source": [
    "We also have to make sure that the corresponding SpaCy model for text embedding used at the resolving step is installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43270dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 'en_core_web_lg' is already installed.\n"
     ]
    }
   ],
   "source": [
    "import importlib.util\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import spacy\n",
    "\n",
    "\n",
    "def ensure_spacy_model(model_name):\n",
    "    if importlib.util.find_spec(model_name) is None:\n",
    "        print(f\"Model '{model_name}' not found. Installing...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", model_name])\n",
    "    else:\n",
    "        print(f\"Model '{model_name}' is already installed.\")\n",
    "\n",
    "# Use it for 'en_core_web_lg'\n",
    "ensure_spacy_model(\"en_core_web_lg\")  # Model used for resolving entities in the KG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a20a0a27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'use_default': False,\n",
       " 'template': \"For each row in the DataFrame, create a node with the label 'Location'. Do not create nodes for Document and Text chunk. The node should have three properties: 'name', 'admin 1', and 'country'. These should be populated with the values from the DataFrame columns 'Name', 'Admin1', and 'Country' respectively.\"}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_files_path = os.path.join(os.path.dirname(os.getcwd()), 'config_files')\n",
    "\n",
    "with open(os.path.join(config_files_path, 'kg_building_config_loc.json'), 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "config['prompt_template_config']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e39afcd",
   "metadata": {},
   "source": [
    "# 1. Loading the data\n",
    "\n",
    "The data is loaded here as a reference, but it is loaded again inside the pipeline below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4320d1",
   "metadata": {},
   "source": [
    "## 1.2. Factal sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d29a4e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 9)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>id</th><th>Published date</th><th>Severity</th><th>Text</th><th>Translated text</th><th>Original language</th><th>Source URL</th><th>Status</th><th>Country</th></tr><tr><td>str</td><td>str</td><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;1&quot;</td><td>&quot;2025-06-03 15:28:28.271179+00:…</td><td>3</td><td>&quot;WFP and UNICEF now say five me…</td><td>null</td><td>null</td><td>&quot;https://www.unicef.org/press-r…</td><td>&quot;published&quot;</td><td>&quot;Sudan&quot;</td></tr><tr><td>&quot;2&quot;</td><td>&quot;2025-06-03 10:10:04.994458+00:…</td><td>3</td><td>&quot;&quot;Multiple casualties&quot; after WF…</td><td>null</td><td>null</td><td>&quot;https://www.reuters.com/world/…</td><td>&quot;published&quot;</td><td>&quot;Sudan&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 9)\n",
       "┌─────┬──────────────┬──────────┬──────────────┬───┬──────────┬──────────────┬───────────┬─────────┐\n",
       "│ id  ┆ Published    ┆ Severity ┆ Text         ┆ … ┆ Original ┆ Source URL   ┆ Status    ┆ Country │\n",
       "│ --- ┆ date         ┆ ---      ┆ ---          ┆   ┆ language ┆ ---          ┆ ---       ┆ ---     │\n",
       "│ str ┆ ---          ┆ i64      ┆ str          ┆   ┆ ---      ┆ str          ┆ str       ┆ str     │\n",
       "│     ┆ str          ┆          ┆              ┆   ┆ str      ┆              ┆           ┆         │\n",
       "╞═════╪══════════════╪══════════╪══════════════╪═══╪══════════╪══════════════╪═══════════╪═════════╡\n",
       "│ 1   ┆ 2025-06-03   ┆ 3        ┆ WFP and      ┆ … ┆ null     ┆ https://www. ┆ published ┆ Sudan   │\n",
       "│     ┆ 15:28:28.271 ┆          ┆ UNICEF now   ┆   ┆          ┆ unicef.org/p ┆           ┆         │\n",
       "│     ┆ 179+00:…     ┆          ┆ say five me… ┆   ┆          ┆ ress-r…      ┆           ┆         │\n",
       "│ 2   ┆ 2025-06-03   ┆ 3        ┆ \"Multiple    ┆ … ┆ null     ┆ https://www. ┆ published ┆ Sudan   │\n",
       "│     ┆ 10:10:04.994 ┆          ┆ casualties\"  ┆   ┆          ┆ reuters.com/ ┆           ┆         │\n",
       "│     ┆ 458+00:…     ┆          ┆ after WF…    ┆   ┆          ┆ world/…      ┆           ┆         │\n",
       "└─────┴──────────────┴──────────┴──────────────┴───┴──────────┴──────────────┴───────────┴─────────┘"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global df1\n",
    "\n",
    "df1 = pl.read_csv(os.path.join('sample_data', 'factal_single_topic_report-2025-05-01-2025-06-05.csv'))\n",
    "df1 = df1.rename({\"Associated topics\": \"Country\"})\n",
    "df1 = df1.rename({\"Published text\": \"Text\"})\n",
    "\n",
    "df1=df1.head(20)\n",
    "\n",
    "# Create an index for each row\n",
    "df1 = df1.with_row_index(name=\"id\", offset=1)\n",
    "# Convert the \"id\" to a string to ensure it is treated as a document ID\n",
    "df1 = df1.with_columns(pl.col('id').cast(pl.String))\n",
    "    \n",
    "df1.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c640450",
   "metadata": {},
   "source": [
    "### Load Admin1 locations from HDX database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7cbcb93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matia\\AppData\\Local\\Temp\\ipykernel_51644\\1507764610.py:33: DeprecationWarning: the `default` parameter for `replace` is deprecated. Use `replace_strict` instead to set a default while replacing values.\n",
      "(Deprecated in version 1.0.0)\n",
      "  pl.col(\"Parent P-Code\").replace(sudan_states_mapping, default=pl.col(\"Parent P-Code\"))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>id</th><th>Name</th><th>Admin1</th><th>Country</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;1&quot;</td><td>&quot;Khartoum&quot;</td><td>&quot;Khartoum&quot;</td><td>&quot;Sudan&quot;</td></tr><tr><td>&quot;2&quot;</td><td>&quot;North Darfur&quot;</td><td>&quot;North Darfur&quot;</td><td>&quot;Sudan&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 4)\n",
       "┌─────┬──────────────┬──────────────┬─────────┐\n",
       "│ id  ┆ Name         ┆ Admin1       ┆ Country │\n",
       "│ --- ┆ ---          ┆ ---          ┆ ---     │\n",
       "│ str ┆ str          ┆ str          ┆ str     │\n",
       "╞═════╪══════════════╪══════════════╪═════════╡\n",
       "│ 1   ┆ Khartoum     ┆ Khartoum     ┆ Sudan   │\n",
       "│ 2   ┆ North Darfur ┆ North Darfur ┆ Sudan   │\n",
       "└─────┴──────────────┴──────────────┴─────────┘"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global df2\n",
    "admin1 = pl.read_csv(r\"C:\\Users\\matia\\Downloads\\global_pcodes_adm_1_2.csv\")\n",
    "df2 = admin1.filter(pl.col('Location') == 'SDN').clone()\n",
    "df2 = df2.with_columns(pl.lit('Sudan').alias('Country'))\n",
    "df2 = df2.rename({\"Name\": \"Name\"})\n",
    "\n",
    "sudan_states_mapping = {\n",
    "    \"SD01\": \"Khartoum\",\n",
    "    \"SD02\": \"North Darfur\",\n",
    "    \"SD03\": \"South Darfur\",\n",
    "    \"SD04\": \"West Darfur\",\n",
    "    \"SD05\": \"East Darfur\",\n",
    "    \"SD06\": \"Central Darfur\",\n",
    "    \"SD07\": \"South Kordofan\",\n",
    "    \"SD08\": \"Blue Nile\",\n",
    "    \"SD09\": \"White Nile\",\n",
    "    \"SD10\": \"Red Sea\",\n",
    "    \"SD11\": \"Kassala\",\n",
    "    \"SD12\": \"Gedaref\",\n",
    "    \"SD13\": \"North Kordofan\",\n",
    "    \"SD14\": \"Sennar\",\n",
    "    \"SD15\": \"Aj Jazirah\",\n",
    "    \"SD16\": \"River Nile\",\n",
    "    \"SD17\": \"Northern\",\n",
    "    \"SD18\": \"West Kordofan\",\n",
    "    \"SD19\": \"Abyei PCA\"\n",
    "}\n",
    "\n",
    "df2 = df2.with_columns(\n",
    "    pl.when(pl.col(\"Parent P-Code\") == \"SDN\")\n",
    "    .then(pl.col(\"Name\"))\n",
    "    .otherwise(\n",
    "        pl.col(\"Parent P-Code\").replace(sudan_states_mapping, default=pl.col(\"Parent P-Code\"))\n",
    "    )\n",
    "    .alias(\"Admin1\")\n",
    ")\n",
    "\n",
    "df2 = df2.select(['Name', 'Admin1', 'Country'])\n",
    "\n",
    "# Create an index for each row if df2 doesn't already have an 'id' column\n",
    "if 'id' not in df2.columns:\n",
    "    df2 = df2.with_row_index(name=\"id\", offset=1)\n",
    "    # Convert the \"id\" to a string to ensure it is treated as a document ID\n",
    "    df2 = df2.with_columns(pl.col('id').cast(pl.String))\n",
    "        \n",
    "df2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cdf4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to convert using Polars' built-in method\n",
    "try:\n",
    "\tpandas_df2 = df2.to_pandas()\n",
    "except ImportError:\n",
    "\t# If PyArrow is not available, convert manually \n",
    "\t# by using the dict-based conversion which doesn't require PyArrow\n",
    "\tpandas_df2 = pd.DataFrame({col: df2[col].to_list() for col in df2.columns})\n",
    "\n",
    "# Display the first few rows of the pandas DataFrame\n",
    "print(pandas_df2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0646aee4",
   "metadata": {},
   "source": [
    "# 2. Running the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e872a9",
   "metadata": {},
   "source": [
    "## 2.2. With a data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c023659",
   "metadata": {},
   "source": [
    "### A. Using the `SpaCySemanticMatchResolver`\n",
    "\n",
    "More useful information about the resolvers can be found in the [user guide](https://neo4j.com/docs/neo4j-graphrag-python/current/user_guide_kg_builder.html#entity-resolver). Below, we use different resolvers (from the most aggressive - spaCy to the most conservative - exact matching) to get a broad overview of the performance results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d30b0b9",
   "metadata": {},
   "source": [
    "### Embed Locations from list of Admin units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e4fe998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c1d454ebe1c4277a84bc37b1004c2cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating location nodes:   0%|          | 0/208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 208 nodes\n"
     ]
    }
   ],
   "source": [
    "async def embed_locations(df):\n",
    "    \n",
    "    # Find path to config_files folder\n",
    "    config_files_path = os.path.join(os.path.dirname(os.getcwd()), 'config_files')\n",
    "\n",
    "    # Load environment variables from .env file\n",
    "    load_dotenv(os.path.join(config_files_path, '.env'), override=True)\n",
    "    \n",
    "    with open(os.path.join(config_files_path, 'kg_building_config_loc.json'), 'r') as f:\n",
    "        config = json.load(f)\n",
    "        \n",
    "    neo4j_uri = os.getenv('NEO4J_URI')\n",
    "    neo4j_username = os.getenv('NEO4J_USERNAME')\n",
    "    neo4j_password = os.getenv('NEO4J_PASSWORD')\n",
    "    gemini_api_key = os.getenv('GEMINI_API_KEY')\n",
    "    \n",
    "    # Create Neo4j driver\n",
    "    driver = GraphDatabase.driver(neo4j_uri, auth=(neo4j_username, neo4j_password))\n",
    "    \n",
    "    # Keep track of processed nodes\n",
    "    processed_nodes = 0\n",
    "\n",
    "    def create_location_nodes(tx, name, admin1, country):\n",
    "        query = \"\"\"\n",
    "        MERGE (l:Location {name: $name, `admin 1`: $admin1, country: $country})\n",
    "        \"\"\"\n",
    "        tx.run(query, name=name, admin1=admin1, country=country)\n",
    "        return 1  # Return count of node processed\n",
    "\n",
    "    # Main logic to insert nodes from the DataFrame\n",
    "    try:\n",
    "        with driver.session() as session:\n",
    "            # Add progress bar\n",
    "            for row in tqdm.tqdm(df.iter_rows(named=True), total=len(df), desc=\"Creating location nodes\"):\n",
    "                # Process each row and count it\n",
    "                processed_nodes += session.execute_write(\n",
    "                    create_location_nodes, \n",
    "                    row[\"Name\"], \n",
    "                    row[\"Admin1\"], \n",
    "                    row[\"Country\"]\n",
    "                )\n",
    "    finally:\n",
    "        driver.close()\n",
    "    \n",
    "    # Return the count of processed nodes\n",
    "    return processed_nodes\n",
    "\n",
    "# Run the main function\n",
    "results = await embed_locations(df2)\n",
    "print(f\"Processed {results} nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2a91f2",
   "metadata": {},
   "source": [
    "#### With Factal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bff7dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing the first dataframe...\n",
      "Processing row 1 of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM response has improper format for chunk_index=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: run_id='197dd09d-8bc2-4d81-8d1d-ead100f418e6' result={'resolver': {'number_of_nodes_to_resolve': 0, 'number_of_created_nodes': 0}}\n",
      "Elapsed time: 3.79 seconds\n",
      "Estimated time remaining: 71.98 seconds\n",
      "\n",
      "Processing row 2 of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM response has improper format for chunk_index=0\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCancelledError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 110\u001b[39m\n\u001b[32m    107\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m all_results\n\u001b[32m    109\u001b[39m \u001b[38;5;66;03m# Asyncio event loop to run the main function in a Jupyter notebook\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m all_results = \u001b[38;5;28;01mawait\u001b[39;00m main()\n\u001b[32m    111\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProcessed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(all_results)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m documents\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    113\u001b[39m \u001b[38;5;66;03m# # Asyncio event loop to run the main function in a script\u001b[39;00m\n\u001b[32m    114\u001b[39m \u001b[38;5;66;03m# if __name__ == \"__main__\":\u001b[39;00m\n\u001b[32m    115\u001b[39m \u001b[38;5;66;03m#     results = asyncio.run(main())\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;66;03m#     print(f\"Processed {len(results)} documents\")\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 85\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     77\u001b[39m     \u001b[38;5;66;03m# metadata_mapping2 = {\u001b[39;00m\n\u001b[32m     78\u001b[39m     \u001b[38;5;66;03m#     \"name\": \"Name\",\u001b[39;00m\n\u001b[32m     79\u001b[39m     \u001b[38;5;66;03m#     \"admin 1\": \"Admin1\",\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     82\u001b[39m \n\u001b[32m     83\u001b[39m     \u001b[38;5;66;03m# Process the First dataframe\u001b[39;00m\n\u001b[32m     84\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mProcessing the first dataframe...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     results_df1 = \u001b[38;5;28;01mawait\u001b[39;00m build_kg_from_df(\n\u001b[32m     86\u001b[39m         kg_pipeline=kg_pipeline,\n\u001b[32m     87\u001b[39m         df=df1,\n\u001b[32m     88\u001b[39m         document_base_field=\u001b[33m'\u001b[39m\u001b[33mid\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     89\u001b[39m         text_column=\u001b[33m'\u001b[39m\u001b[33mText\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     90\u001b[39m         document_metadata_mapping=metadata_mapping1,\n\u001b[32m     91\u001b[39m         document_id_column=\u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# Use default document ID generation\u001b[39;00m\n\u001b[32m     92\u001b[39m     )\n\u001b[32m     93\u001b[39m     all_results.extend(results_df1)\n\u001b[32m     95\u001b[39m \u001b[38;5;66;03m#     # Process the Second dataframe\u001b[39;00m\n\u001b[32m     96\u001b[39m \u001b[38;5;66;03m#     print(\"Processing the second dataframe...\")\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m#     results_df2 = await build_kg_from_df(\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    104\u001b[39m \u001b[38;5;66;03m#     )\u001b[39;00m\n\u001b[32m    105\u001b[39m \u001b[38;5;66;03m#     all_results.extend(results_df2)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\graphrag_pipeline\\library\\kg_builder\\build_kg_from_df.py:72\u001b[39m, in \u001b[36mbuild_kg_from_df\u001b[39m\u001b[34m(kg_pipeline, df, document_base_field, text_column, document_metadata_mapping, document_id_column)\u001b[39m\n\u001b[32m     69\u001b[39m doc_id = row.get(document_id_column) \u001b[38;5;28;01mif\u001b[39;00m document_id_column \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(uuid.uuid4())\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# Process the text with the pipeline\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m kg_pipeline.run_async(\n\u001b[32m     73\u001b[39m     text=text,\n\u001b[32m     74\u001b[39m     document_base_field=doc_base_field,\n\u001b[32m     75\u001b[39m     document_metadata=processed_metadata, \n\u001b[32m     76\u001b[39m     document_id=doc_id\n\u001b[32m     77\u001b[39m )\n\u001b[32m     78\u001b[39m results.append(result)\n\u001b[32m     79\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mResult: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\graphrag_pipeline\\library\\kg_builder\\custom_kg_pipeline.py:209\u001b[39m, in \u001b[36mCustomKGPipeline.run_async\u001b[39m\u001b[34m(self, text, document_base_field, document_metadata, document_id)\u001b[39m\n\u001b[32m    201\u001b[39m pipe_inputs[\u001b[33m\"\u001b[39m\u001b[33mextractor\u001b[39m\u001b[33m\"\u001b[39m] = {  \u001b[38;5;66;03m# Define additional inputs for the entity relation extractor component\u001b[39;00m\n\u001b[32m    202\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mexamples\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.examples,  \u001b[38;5;66;03m# Examples for few-shot learning in the prompt\u001b[39;00m\n\u001b[32m    203\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mlexical_graph_config\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.lexical_graph_config,  \u001b[38;5;66;03m# Lexical graph configuration. Oddly enough, this must be included even if we are not creating a lexical graph in the extractor component, but this is needed to link the lexical graph created in the LexicalGraphBuilder component to the entity graph created in the LLMEntityRelationExtractor component.\u001b[39;00m\n\u001b[32m    204\u001b[39m     }\n\u001b[32m    206\u001b[39m \u001b[38;5;66;03m# Run the pipeline asynchronously (.run() method of the Pipeline class)\u001b[39;00m\n\u001b[32m    207\u001b[39m \u001b[38;5;66;03m# already handles the execution of all components in the pipeline in\u001b[39;00m\n\u001b[32m    208\u001b[39m \u001b[38;5;66;03m# an asynchronous manner, so we don't need to worry about that.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m pipe.run(pipe_inputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j_graphrag\\experimental\\pipeline\\pipeline.py:573\u001b[39m, in \u001b[36mPipeline.run\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m    571\u001b[39m orchestrator = Orchestrator(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    572\u001b[39m logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPIPELINE ORCHESTRATOR: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00morchestrator.run_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m573\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m orchestrator.run(data)\n\u001b[32m    574\u001b[39m end_time = default_timer()\n\u001b[32m    575\u001b[39m logger.debug(\n\u001b[32m    576\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPIPELINE FINISHED \u001b[39m\u001b[38;5;132;01m{\u001b[39;00morchestrator.run_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_time\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    577\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j_graphrag\\experimental\\pipeline\\orchestrator.py:270\u001b[39m, in \u001b[36mOrchestrator.run\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.event_notifier.notify_pipeline_started(\u001b[38;5;28mself\u001b[39m.run_id, data)\n\u001b[32m    269\u001b[39m tasks = [\u001b[38;5;28mself\u001b[39m.run_task(root, data) \u001b[38;5;28;01mfor\u001b[39;00m root \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pipeline.roots()]\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(*tasks)\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.event_notifier.notify_pipeline_finished(\n\u001b[32m    272\u001b[39m     \u001b[38;5;28mself\u001b[39m.run_id, \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pipeline.get_final_results(\u001b[38;5;28mself\u001b[39m.run_id)\n\u001b[32m    273\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j_graphrag\\experimental\\pipeline\\orchestrator.py:93\u001b[39m, in \u001b[36mOrchestrator.run_task\u001b[39m\u001b[34m(self, task, data)\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.event_notifier.notify_task_finished(\u001b[38;5;28mself\u001b[39m.run_id, task.name, res)\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m res:\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.on_task_complete(data=data, task=task, result=res)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j_graphrag\\experimental\\pipeline\\orchestrator.py:134\u001b[39m, in \u001b[36mOrchestrator.on_task_complete\u001b[39m\u001b[34m(self, data, task, result)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.add_result_for_component(\n\u001b[32m    130\u001b[39m     task.name, res_to_save, is_final=task.is_leaf()\n\u001b[32m    131\u001b[39m )\n\u001b[32m    132\u001b[39m \u001b[38;5;66;03m# then get the next tasks to be executed\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[38;5;66;03m# and run them in //\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(*[\u001b[38;5;28mself\u001b[39m.run_task(n, data) \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.next(task)])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j_graphrag\\experimental\\pipeline\\orchestrator.py:93\u001b[39m, in \u001b[36mOrchestrator.run_task\u001b[39m\u001b[34m(self, task, data)\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.event_notifier.notify_task_finished(\u001b[38;5;28mself\u001b[39m.run_id, task.name, res)\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m res:\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.on_task_complete(data=data, task=task, result=res)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j_graphrag\\experimental\\pipeline\\orchestrator.py:134\u001b[39m, in \u001b[36mOrchestrator.on_task_complete\u001b[39m\u001b[34m(self, data, task, result)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.add_result_for_component(\n\u001b[32m    130\u001b[39m     task.name, res_to_save, is_final=task.is_leaf()\n\u001b[32m    131\u001b[39m )\n\u001b[32m    132\u001b[39m \u001b[38;5;66;03m# then get the next tasks to be executed\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[38;5;66;03m# and run them in //\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(*[\u001b[38;5;28mself\u001b[39m.run_task(n, data) \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.next(task)])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j_graphrag\\experimental\\pipeline\\orchestrator.py:89\u001b[39m, in \u001b[36mOrchestrator.run_task\u001b[39m\u001b[34m(self, task, data)\u001b[39m\n\u001b[32m     83\u001b[39m notifier = partial(\n\u001b[32m     84\u001b[39m     \u001b[38;5;28mself\u001b[39m.event_notifier.notify_task_progress,\n\u001b[32m     85\u001b[39m     run_id=\u001b[38;5;28mself\u001b[39m.run_id,\n\u001b[32m     86\u001b[39m     task_name=task.name,\n\u001b[32m     87\u001b[39m )\n\u001b[32m     88\u001b[39m context = RunContext(run_id=\u001b[38;5;28mself\u001b[39m.run_id, task_name=task.name, notifier=notifier)\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m res = \u001b[38;5;28;01mawait\u001b[39;00m task.run(context, inputs)\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.set_task_status(task.name, RunStatus.DONE)\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.event_notifier.notify_task_finished(\u001b[38;5;28mself\u001b[39m.run_id, task.name, res)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j_graphrag\\experimental\\pipeline\\pipeline.py:104\u001b[39m, in \u001b[36mTaskPipelineNode.run\u001b[39m\u001b[34m(self, context, inputs)\u001b[39m\n\u001b[32m    102\u001b[39m logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTASK START \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.name\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m input=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprettify(inputs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    103\u001b[39m start_time = default_timer()\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m res = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.execute(context, inputs)\n\u001b[32m    105\u001b[39m end_time = default_timer()\n\u001b[32m    106\u001b[39m logger.debug(\n\u001b[32m    107\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTASK FINISHED \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_time\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m res=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprettify(res)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    108\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j_graphrag\\experimental\\pipeline\\pipeline.py:90\u001b[39m, in \u001b[36mTaskPipelineNode.execute\u001b[39m\u001b[34m(self, context, inputs)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mexecute\u001b[39m(\n\u001b[32m     81\u001b[39m     \u001b[38;5;28mself\u001b[39m, context: RunContext, inputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]\n\u001b[32m     82\u001b[39m ) -> RunResult | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     83\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Execute the task\u001b[39;00m\n\u001b[32m     84\u001b[39m \n\u001b[32m     85\u001b[39m \u001b[33;03m    Returns:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     88\u001b[39m \u001b[33;03m        was unsuccessful.\u001b[39;00m\n\u001b[32m     89\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m     component_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.component.run_with_context(\n\u001b[32m     91\u001b[39m         context_=context, **inputs\n\u001b[32m     92\u001b[39m     )\n\u001b[32m     93\u001b[39m     run_result = RunResult(\n\u001b[32m     94\u001b[39m         result=component_result,\n\u001b[32m     95\u001b[39m     )\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m run_result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j_graphrag\\experimental\\pipeline\\component.py:110\u001b[39m, in \u001b[36mComponent.run_with_context\u001b[39m\u001b[34m(self, context_, *args, **kwargs)\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"This method is called by the pipeline orchestrator.\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03mThe `context_` parameter contains information about\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03mthe pipeline run: the `run_id` and a `notify` function\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    107\u001b[39m \u001b[33;03mIt defaults to calling the `run` method to prevent any breaking change.\u001b[39;00m\n\u001b[32m    108\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    109\u001b[39m \u001b[38;5;66;03m# default behavior to prevent a breaking change\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.run(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\pydantic\\_internal\\_validate_call.py:34\u001b[39m, in \u001b[36mupdate_wrapper_attributes.<locals>.wrapper_function\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(wrapped)\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper_function\u001b[39m(*args, **kwargs):  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m wrapper(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j_graphrag\\experimental\\components\\entity_relation_extractor.py:341\u001b[39m, in \u001b[36mLLMEntityRelationExtractor.run\u001b[39m\u001b[34m(self, chunks, document_info, lexical_graph_config, schema, examples, **kwargs)\u001b[39m\n\u001b[32m    330\u001b[39m sem = asyncio.Semaphore(\u001b[38;5;28mself\u001b[39m.max_concurrency)\n\u001b[32m    331\u001b[39m tasks = [\n\u001b[32m    332\u001b[39m     \u001b[38;5;28mself\u001b[39m.run_for_chunk(\n\u001b[32m    333\u001b[39m         sem,\n\u001b[32m   (...)\u001b[39m\u001b[32m    339\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunks.chunks\n\u001b[32m    340\u001b[39m ]\n\u001b[32m--> \u001b[39m\u001b[32m341\u001b[39m chunk_graphs: \u001b[38;5;28mlist\u001b[39m[Neo4jGraph] = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(*tasks))\n\u001b[32m    342\u001b[39m graph = \u001b[38;5;28mself\u001b[39m.combine_chunk_graphs(lexical_graph, chunk_graphs)\n\u001b[32m    343\u001b[39m logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExtracted graph: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprettify(graph)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mCancelledError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Example usage code\n",
    "async def main():\n",
    "\n",
    "    global df1\n",
    "    \n",
    "    # Find path to config_files folder\n",
    "    config_files_path = os.path.join(os.path.dirname(os.getcwd()), 'config_files')\n",
    "\n",
    "    # Load environment variables from .env file\n",
    "    load_dotenv(os.path.join(config_files_path, '.env'), override=True)\n",
    "    \n",
    "    with open(os.path.join(config_files_path, 'kg_building_config2.json'), 'r') as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    # Neo4j connection\n",
    "    neo4j_uri = os.getenv('NEO4J_URI')\n",
    "    neo4j_username = os.getenv('NEO4J_USERNAME')\n",
    "    neo4j_password = os.getenv('NEO4J_PASSWORD')\n",
    "    gemini_api_key = os.getenv('GEMINI_API_KEY')\n",
    "    \n",
    "    # Check if gemini_api_key is set\n",
    "    if gemini_api_key:\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(\"Gemini API key is not set. Please provide a valid API key.\")\n",
    "\n",
    "    # Initialize LLM\n",
    "    llm = GeminiLLM(\n",
    "        model_name=config['llm_config']['model_name'],\n",
    "        google_api_key=gemini_api_key,\n",
    "        model_params=config['llm_config']['model_params']\n",
    "    )\n",
    "    \n",
    "    # Initialize embedder\n",
    "    embedder = SentenceTransformerEmbeddings(model=config['embedder_config']['model_name'])\n",
    "    \n",
    "    # Configure text splitter\n",
    "    text_splitter_config = config['text_splitter_config'] \n",
    "    \n",
    "    # Create the pipeline - use with statement to ensure proper resource management\n",
    "    # and to ensure the driver is closed after use\n",
    "    all_results = []\n",
    "    with neo4j.GraphDatabase.driver(neo4j_uri, auth=(neo4j_username, neo4j_password)) as driver:\n",
    "        \n",
    "        # Initialize entity resolver\n",
    "        resolver = SpaCySemanticMatchResolver(  # Merge nodes with same label and similar textual properties\n",
    "            driver,\n",
    "            filter_query=None,  # \"WHERE (entity)-[:FROM_CHUNK]->(:Chunk)-[:FROM_DOCUMENT]->(doc:Document {id = 'docId'}\",  # Used to reduce the resolution scope to a specific document\n",
    "            resolve_properties=[\"name\"],  # Properties to use for resolution (default is \"name\")\n",
    "            similarity_threshold=0.8,  # The similarity threshold above which nodes are merged (default is 0.8). Higher threshold will result in less false positives, but may miss some matches. \n",
    "            spacy_model=\"en_core_web_lg\"  # spaCy model to use for resolution (default is \"en_core_web_lg\")\n",
    "        )\n",
    "\n",
    "        # Initialize the custom KG pipeline\n",
    "        kg_pipeline = CustomKGPipeline(\n",
    "            llm=llm,\n",
    "            driver=driver,\n",
    "            embedder=embedder,\n",
    "            schema_config=config['schema_config'],\n",
    "            prompt_template=config['prompt_template_config']['template'] if config['prompt_template_config'].get('use_default') == False else None,\n",
    "            text_splitter_config=text_splitter_config,\n",
    "            resolver=resolver,\n",
    "            examples_config=None,  # Use None if no examples are provided\n",
    "            on_error='RAISE',\n",
    "            batch_size=1000,\n",
    "            max_concurrency=5\n",
    "        )\n",
    "        \n",
    "        # Define metadata mapping (document properties additional to base field \n",
    "        # to dataframe columns)\n",
    "        metadata_mapping1 = {\n",
    "            \"source\": \"Source URL\",\n",
    "            \"published_date\": \"Published date\",\n",
    "            \"country\": \"Country\"\n",
    "        }\n",
    "        \n",
    "        # metadata_mapping2 = {\n",
    "        #     \"name\": \"Name\",\n",
    "        #     \"admin 1\": \"Admin1\",\n",
    "        #     \"country\": \"Country\"\n",
    "        # }\n",
    "        \n",
    "        # Process the First dataframe\n",
    "        print(\"Processing the first dataframe...\")\n",
    "        results_df1 = await build_kg_from_df(\n",
    "            kg_pipeline=kg_pipeline,\n",
    "            df=df1,\n",
    "            document_base_field='id',\n",
    "            text_column='Text',\n",
    "            document_metadata_mapping=metadata_mapping1,\n",
    "            document_id_column=None  # Use default document ID generation\n",
    "        )\n",
    "        all_results.extend(results_df1)\n",
    "\n",
    "    #     # Process the Second dataframe\n",
    "    #     print(\"Processing the second dataframe...\")\n",
    "    #     results_df2 = await build_kg_from_df(\n",
    "    #         kg_pipeline=kg_pipeline,\n",
    "    #         df=df2,\n",
    "    #         document_base_field='id',\n",
    "    #         text_column='Name',\n",
    "    #         document_metadata_mapping=metadata_mapping2,\n",
    "    #         document_id_column=None  # Use default document ID generation\n",
    "    #     )\n",
    "    #     all_results.extend(results_df2)\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Asyncio event loop to run the main function in a Jupyter notebook\n",
    "all_results = await main()\n",
    "print(f\"Processed {len(all_results)} documents\")\n",
    "\n",
    "# # Asyncio event loop to run the main function in a script\n",
    "# if __name__ == \"__main__\":\n",
    "#     results = asyncio.run(main())\n",
    "#     print(f\"Processed {len(results)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67cb35f",
   "metadata": {},
   "source": [
    "# Entity Resolution Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7527a8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APOC is available.\n",
      "Found 206 entities to process\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deb0f13c7bbd497a8a737586e59c8200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing similarity scores:   0%|          | 0/20805 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "BufferError",
     "evalue": "Existing exports of data: object cannot be re-sized",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 92\u001b[39m, in \u001b[36mfind_similar_entities\u001b[39m\u001b[34m(threshold)\u001b[39m\n\u001b[32m     91\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m pair \u001b[38;5;129;01min\u001b[39;00m tqdm.tqdm(similar_pairs, desc=\u001b[33m\"\u001b[39m\u001b[33mComputing similarity scores\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m         \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpair\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m similar_pairs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j\\_sync\\work\\session.py:328\u001b[39m, in \u001b[36mSession.run\u001b[39m\u001b[34m(self, query, parameters, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m parameters = \u001b[38;5;28mdict\u001b[39m(parameters \u001b[38;5;129;01mor\u001b[39;00m {}, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_auto_result\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimpersonated_user\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdefault_access_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbookmarks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnotifications_min_severity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnotifications_disabled_classifications\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._auto_result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j\\_sync\\work\\result.py:236\u001b[39m, in \u001b[36mResult._run\u001b[39m\u001b[34m(self, query, parameters, db, imp_user, access_mode, bookmarks, notifications_min_severity, notifications_disabled_classifications)\u001b[39m\n\u001b[32m    235\u001b[39m \u001b[38;5;28mself\u001b[39m._connection.send_all()\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_attach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j\\_sync\\work\\result.py:430\u001b[39m, in \u001b[36mResult._attach\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._attached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j\\_sync\\io\\_common.py:184\u001b[39m, in \u001b[36mConnectionErrorHandler.__getattr__.<locals>.outer.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    183\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m     \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (Neo4jError, ServiceUnavailable, SessionExpired) \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j\\_sync\\io\\_bolt.py:861\u001b[39m, in \u001b[36mBolt.fetch_message\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    860\u001b[39m \u001b[38;5;66;03m# Receive exactly one message\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m861\u001b[39m tag, fields = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minbox\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    862\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhydration_hooks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresponses\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhydration_hooks\u001b[49m\n\u001b[32m    863\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    864\u001b[39m res = \u001b[38;5;28mself\u001b[39m._process_message(tag, fields)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j\\_sync\\io\\_common.py:77\u001b[39m, in \u001b[36mInbox.pop\u001b[39m\u001b[34m(self, hydration_hooks)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpop\u001b[39m(\u001b[38;5;28mself\u001b[39m, hydration_hooks):\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_buffer_one_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j\\_sync\\io\\_common.py:54\u001b[39m, in \u001b[36mInbox._buffer_one_chunk\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m chunk_size == \u001b[32m0\u001b[39m:\n\u001b[32m     53\u001b[39m     \u001b[38;5;66;03m# Determine the chunk size and skip noop\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     \u001b[43mreceive_into_buffer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_socket\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     chunk_size = \u001b[38;5;28mself\u001b[39m._buffer.pop_u16()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j\\_sync\\io\\_common.py:345\u001b[39m, in \u001b[36mreceive_into_buffer\u001b[39m\u001b[34m(sock, buffer, n_bytes)\u001b[39m\n\u001b[32m    344\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m buffer.used < end:\n\u001b[32m--> \u001b[39m\u001b[32m345\u001b[39m     n = \u001b[43msock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m        \u001b[49m\u001b[43mview\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mused\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mused\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    348\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j\\_async_compat\\network\\_bolt_socket.py:364\u001b[39m, in \u001b[36mBoltSocketBase.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes)\u001b[39m\n\u001b[32m    363\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrecv_into\u001b[39m(\u001b[38;5;28mself\u001b[39m, buffer, nbytes):\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wait_for_io\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_socket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j\\_async_compat\\network\\_bolt_socket.py:339\u001b[39m, in \u001b[36mBoltSocketBase._wait_for_io\u001b[39m\u001b[34m(self, func, *args, **kwargs)\u001b[39m\n\u001b[32m    338\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._deadline \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m339\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    340\u001b[39m timeout = \u001b[38;5;28mself\u001b[39m._socket.gettimeout()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\ssl.py:1252\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1249\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1250\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1251\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1252\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1253\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\ssl.py:1104\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1104\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1105\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mBufferError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j\\_sync\\work\\session.py:222\u001b[39m, in \u001b[36mSession.close\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    221\u001b[39m \u001b[38;5;28mself\u001b[39m._connection.send_all()\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[38;5;66;03m# TODO: Investigate potential non graceful close states\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j\\_sync\\io\\_bolt.py:879\u001b[39m, in \u001b[36mBolt.fetch_all\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    878\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m response.complete:\n\u001b[32m--> \u001b[39m\u001b[32m879\u001b[39m     detail_delta, summary_delta = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfetch_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     detail_count += detail_delta\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j\\_sync\\io\\_bolt.py:861\u001b[39m, in \u001b[36mBolt.fetch_message\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    860\u001b[39m \u001b[38;5;66;03m# Receive exactly one message\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m861\u001b[39m tag, fields = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minbox\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    862\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhydration_hooks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresponses\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhydration_hooks\u001b[49m\n\u001b[32m    863\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    864\u001b[39m res = \u001b[38;5;28mself\u001b[39m._process_message(tag, fields)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j\\_sync\\io\\_common.py:77\u001b[39m, in \u001b[36mInbox.pop\u001b[39m\u001b[34m(self, hydration_hooks)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpop\u001b[39m(\u001b[38;5;28mself\u001b[39m, hydration_hooks):\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_buffer_one_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j\\_sync\\io\\_common.py:59\u001b[39m, in \u001b[36mInbox._buffer_one_chunk\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     57\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33m[#\u001b[39m\u001b[38;5;132;01m%04X\u001b[39;00m\u001b[33m]  S: <NOOP>\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m._local_port)\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[43mreceive_into_buffer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_socket\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\n\u001b[32m     61\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m chunk_size = \u001b[38;5;28mself\u001b[39m._buffer.pop_u16()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j\\_sync\\io\\_common.py:342\u001b[39m, in \u001b[36mreceive_into_buffer\u001b[39m\u001b[34m(sock, buffer, n_bytes)\u001b[39m\n\u001b[32m    341\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m end > \u001b[38;5;28mlen\u001b[39m(buffer.data):\n\u001b[32m--> \u001b[39m\u001b[32m342\u001b[39m     \u001b[43mbuffer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbytearray\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mend\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(buffer.data) \u001b[38;5;28;01mas\u001b[39;00m view:\n",
      "\u001b[31mBufferError\u001b[39m: Existing exports of data: object cannot be re-sized",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mBufferError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 108\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;66;03m# === MAIN EXECUTION ===\u001b[39;00m\n\u001b[32m    107\u001b[39m check_apoc()\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m pairs = \u001b[43mfind_similar_entities\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m pairs = \u001b[38;5;28msorted\u001b[39m(pairs, key=\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[33m\"\u001b[39m\u001b[33msimilarity\u001b[39m\u001b[33m\"\u001b[39m], reverse=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    110\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(pairs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m similar entity pairs.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 90\u001b[39m, in \u001b[36mfind_similar_entities\u001b[39m\u001b[34m(threshold)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m# Create SAME_AS relationships\u001b[39;00m\n\u001b[32m     85\u001b[39m query = \u001b[33m\"\"\"\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[33mMATCH (a), (b)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[33mWHERE elementId(a) = $id1 AND elementId(b) = $id2\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[33mMERGE (a)-[:SAME_AS \u001b[39m\u001b[33m{\u001b[39m\u001b[33msimilarity: $similarity}]->(b)\u001b[39m\n\u001b[32m     89\u001b[39m \u001b[33m\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m.\u001b[49m\u001b[43msession\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msession\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpair\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43msimilar_pairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mComputing similarity scores\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpair\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j\\_sync\\work\\session.py:130\u001b[39m, in \u001b[36mSession.__exit__\u001b[39m\u001b[34m(self, exception_type, exception_value, traceback)\u001b[39m\n\u001b[32m    128\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    129\u001b[39m     \u001b[38;5;28mself\u001b[39m._state_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j\\_sync\\work\\session.py:233\u001b[39m, in \u001b[36mSession.close\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    231\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    232\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m233\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_disconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m     \u001b[38;5;28mself\u001b[39m._state_failed = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    236\u001b[39m \u001b[38;5;28mself\u001b[39m._closed = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j\\_sync\\work\\session.py:145\u001b[39m, in \u001b[36mSession._disconnect\u001b[39m\u001b[34m(self, sync)\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_disconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m, sync=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    144\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_disconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msync\u001b[49m\u001b[43m=\u001b[49m\u001b[43msync\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m asyncio.CancelledError:\n\u001b[32m    147\u001b[39m         \u001b[38;5;28mself\u001b[39m._handle_cancellation(message=\u001b[33m\"\u001b[39m\u001b[33m_disconnect\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j\\_sync\\work\\workspace.py:298\u001b[39m, in \u001b[36mWorkspace._disconnect\u001b[39m\u001b[34m(self, sync)\u001b[39m\n\u001b[32m    296\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    297\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection:\n\u001b[32m--> \u001b[39m\u001b[32m298\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrelease\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    299\u001b[39m     \u001b[38;5;28mself\u001b[39m._connection = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    300\u001b[39m \u001b[38;5;28mself\u001b[39m._connection_access_mode = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j\\_sync\\io\\_pool.py:481\u001b[39m, in \u001b[36mIOPool.release\u001b[39m\u001b[34m(self, *connections)\u001b[39m\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    476\u001b[39m     log.debug(\n\u001b[32m    477\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m[#\u001b[39m\u001b[38;5;132;01m%04X\u001b[39;00m\u001b[33m]  _: <POOL> release unclean connection \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    478\u001b[39m         connection.local_port,\n\u001b[32m    479\u001b[39m         connection.connection_id,\n\u001b[32m    480\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m481\u001b[39m     \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (Neo4jError, DriverError, BoltError) \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    483\u001b[39m     log.debug(\n\u001b[32m    484\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m[#\u001b[39m\u001b[38;5;132;01m%04X\u001b[39;00m\u001b[33m]  _: <POOL> failed to reset connection \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    485\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mon release: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    486\u001b[39m         connection.local_port,\n\u001b[32m    487\u001b[39m         exc,\n\u001b[32m    488\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j\\_sync\\io\\_bolt5.py:447\u001b[39m, in \u001b[36mBolt5x0.reset\u001b[39m\u001b[34m(self, dehydration_hooks, hydration_hooks)\u001b[39m\n\u001b[32m    443\u001b[39m \u001b[38;5;28mself\u001b[39m._append(\n\u001b[32m    444\u001b[39m     \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\x0f\u001b[39;00m\u001b[33m\"\u001b[39m, response=response, dehydration_hooks=dehydration_hooks\n\u001b[32m    445\u001b[39m )\n\u001b[32m    446\u001b[39m \u001b[38;5;28mself\u001b[39m.send_all()\n\u001b[32m--> \u001b[39m\u001b[32m447\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfetch_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j\\_sync\\io\\_bolt.py:879\u001b[39m, in \u001b[36mBolt.fetch_all\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    877\u001b[39m response = \u001b[38;5;28mself\u001b[39m.responses[\u001b[32m0\u001b[39m]\n\u001b[32m    878\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m response.complete:\n\u001b[32m--> \u001b[39m\u001b[32m879\u001b[39m     detail_delta, summary_delta = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfetch_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     detail_count += detail_delta\n\u001b[32m    881\u001b[39m     summary_count += summary_delta\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j\\_sync\\io\\_bolt.py:861\u001b[39m, in \u001b[36mBolt.fetch_message\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    858\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m\n\u001b[32m    860\u001b[39m \u001b[38;5;66;03m# Receive exactly one message\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m861\u001b[39m tag, fields = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minbox\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    862\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhydration_hooks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresponses\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhydration_hooks\u001b[49m\n\u001b[32m    863\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    864\u001b[39m res = \u001b[38;5;28mself\u001b[39m._process_message(tag, fields)\n\u001b[32m    865\u001b[39m \u001b[38;5;28mself\u001b[39m.idle_since = monotonic()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j\\_sync\\io\\_common.py:77\u001b[39m, in \u001b[36mInbox.pop\u001b[39m\u001b[34m(self, hydration_hooks)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpop\u001b[39m(\u001b[38;5;28mself\u001b[39m, hydration_hooks):\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_buffer_one_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     79\u001b[39m         size, tag = \u001b[38;5;28mself\u001b[39m._unpacker.unpack_structure_header()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j\\_sync\\io\\_common.py:59\u001b[39m, in \u001b[36mInbox._buffer_one_chunk\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m chunk_size == \u001b[32m0\u001b[39m:\n\u001b[32m     57\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33m[#\u001b[39m\u001b[38;5;132;01m%04X\u001b[39;00m\u001b[33m]  S: <NOOP>\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m._local_port)\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[43mreceive_into_buffer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_socket\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\n\u001b[32m     61\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m chunk_size = \u001b[38;5;28mself\u001b[39m._buffer.pop_u16()\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunk_size == \u001b[32m0\u001b[39m:\n\u001b[32m     65\u001b[39m     \u001b[38;5;66;03m# chunk_size was the end marker for the message\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j\\_sync\\io\\_common.py:342\u001b[39m, in \u001b[36mreceive_into_buffer\u001b[39m\u001b[34m(sock, buffer, n_bytes)\u001b[39m\n\u001b[32m    340\u001b[39m end = buffer.used + n_bytes\n\u001b[32m    341\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m end > \u001b[38;5;28mlen\u001b[39m(buffer.data):\n\u001b[32m--> \u001b[39m\u001b[32m342\u001b[39m     \u001b[43mbuffer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbytearray\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mend\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(buffer.data) \u001b[38;5;28;01mas\u001b[39;00m view:\n\u001b[32m    344\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m buffer.used < end:\n",
      "\u001b[31mBufferError\u001b[39m: Existing exports of data: object cannot be re-sized"
     ]
    }
   ],
   "source": [
    "# Connect to Neo4j\n",
    "driver = GraphDatabase.driver(\n",
    "    os.getenv(\"NEO4J_URI\"),\n",
    "    auth=(\"neo4j\", os.getenv(\"NEO4J_PASSWORD\"))\n",
    ")\n",
    "\n",
    "# Load embedding model\n",
    "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n",
    "\n",
    "def get_embedding(text):\n",
    "    return model.encode(text).tolist()\n",
    "\n",
    "# List of relevant node labels for deduplication\n",
    "ENTITY_LABELS = [\"Event\", \"Actor\", \"Location\"]\n",
    "\n",
    "def get_all_entities():\n",
    "    query_template = \"\"\"\n",
    "    MATCH (n:{label})\n",
    "    RETURN elementId(n) AS id, n.name AS name, labels(n) AS labels, properties(n) AS properties\n",
    "    \"\"\"\n",
    "    all_entities = []\n",
    "    with driver.session() as session:\n",
    "        for label in ENTITY_LABELS:\n",
    "            result = session.run(query_template.format(label=label)).data()\n",
    "            all_entities.extend(result)\n",
    "    return all_entities\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    if vec1 is None or vec2 is None or len(vec1) == 0 or len(vec2) == 0:\n",
    "        return 0  # If either vector is None or empty, return 0 similarity\n",
    "    dot = np.dot(vec1, vec2)\n",
    "    norm1 = np.linalg.norm(vec1)\n",
    "    norm2 = np.linalg.norm(vec2)\n",
    "    return dot / (norm1 * norm2) if norm1 > 0 and norm2 > 0 else 0\n",
    "\n",
    "def find_similar_entities(threshold=0.7):\n",
    "    entities = get_all_entities()\n",
    "    print(f\"Found {len(entities)} entities to process\")\n",
    "    \n",
    "    # Compute embeddings\n",
    "    for entity in entities:\n",
    "        # Filter out \"__KGBuilder__\" from labels\n",
    "        filtered_labels = [l for l in entity['labels'] if l != \"__KGBuilder__\" and l != \"__Entity__\"]\n",
    "        if filtered_labels:  # If there are any labels left\n",
    "            entity['primary_label'] = filtered_labels[0]\n",
    "        else:\n",
    "            entity['primary_label'] = entity['labels'][0]  # Fallback if only __KGBuilder__ is present\n",
    "            \n",
    "        # Start with entity label/type and name\n",
    "        text = f\"Type: {entity['primary_label']}\\nName: {entity['name']}\\n\"\n",
    "        \n",
    "        # Rest of embedding code remains the same\n",
    "        for key, value in entity['properties'].items():\n",
    "            if key != 'embedding' and value is not None:\n",
    "                if isinstance(value, (list, dict)):\n",
    "                    formatted_value = str(value)\n",
    "                else:\n",
    "                    formatted_value = value\n",
    "                text += f\"{key}: {formatted_value}\\n\"\n",
    "        \n",
    "        entity['embedding'] = get_embedding(text)\n",
    "     \n",
    "    # Find similar pairs with the new label comparison\n",
    "    similar_pairs = []\n",
    "    total_comparisons = sum(range(len(entities)))\n",
    "    for i, e1 in enumerate(entities):\n",
    "        for j, e2 in enumerate(entities[i + 1:], i + 1):\n",
    "            # Only compare if they have the same primary label (excluding __KGBuilder__)\n",
    "            if e1['primary_label'] != e2['primary_label'] or e1['primary_label'] == \"__KGBuilder__\":\n",
    "                continue\n",
    "            \n",
    "            sim = cosine_similarity(e1['embedding'], e2['embedding'])\n",
    "            if sim > threshold:\n",
    "                similar_pairs.append({\n",
    "                    \"id1\": e1['id'],\n",
    "                    \"id2\": e2['id'],\n",
    "                    \"name1\": e1['name'],\n",
    "                    \"name2\": e2['name'],\n",
    "                    \"type1\": e1['primary_label'],\n",
    "                    \"type2\": e2['primary_label'],\n",
    "                    \"similarity\": sim\n",
    "                })\n",
    "    \n",
    "    # Create SAME_AS relationships\n",
    "    query = \"\"\"\n",
    "    MATCH (a), (b)\n",
    "    WHERE elementId(a) = $id1 AND elementId(b) = $id2\n",
    "    MERGE (a)-[:SAME_AS {similarity: $similarity}]->(b)\n",
    "    \"\"\"\n",
    "    with driver.session() as session:\n",
    "        for pair in tqdm.tqdm(similar_pairs, desc=\"Computing similarity scores\"):\n",
    "            session.run(query, pair)\n",
    "    \n",
    "    return similar_pairs\n",
    "\n",
    "def check_apoc():\n",
    "    try:\n",
    "        with driver.session() as session:\n",
    "            session.run(\"CALL apoc.help('create')\")\n",
    "            print(\"APOC is available.\")\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        print(f\"APOC not available: {e}\")\n",
    "        return False\n",
    "\n",
    "# === MAIN EXECUTION ===\n",
    "check_apoc()\n",
    "pairs = find_similar_entities()\n",
    "pairs = sorted(pairs, key=lambda x: x[\"similarity\"], reverse=True)\n",
    "print(f\"Found {len(pairs)} similar entity pairs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4205df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b7bc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_similar_nodes():\n",
    "    merge_query = \"\"\"\n",
    "    // Process one pair of nodes at a time to avoid conflicts\n",
    "    MATCH (n1)-[r:SAME_AS]->(n2)\n",
    "    WHERE n1 IS NOT NULL AND n2 IS NOT NULL\n",
    "    \n",
    "    // Copy properties from n2 to n1 if they don't exist in n1\n",
    "    WITH n1, n2, [key IN keys(n2) WHERE NOT key IN keys(n1)] AS newKeys\n",
    "    FOREACH (key IN newKeys | SET n1[key] = n2[key])\n",
    "    \n",
    "    // Get all outgoing relationships from n2 (except SAME_AS)\n",
    "    WITH n1, n2\n",
    "    OPTIONAL MATCH (n2)-[outRel]->(target)\n",
    "    WHERE target IS NOT NULL AND type(outRel) <> 'SAME_AS'\n",
    "    \n",
    "    // Create equivalent relationships from n1 if they don't already exist\n",
    "    WITH n1, n2, outRel, target, type(outRel) AS relType\n",
    "    WHERE NOT EXISTS((n1)-[:`${relType}`]->(target))\n",
    "    \n",
    "    // Create new relationship with the same properties\n",
    "    FOREACH (_ IN CASE WHEN outRel IS NOT NULL THEN [1] ELSE [] END |\n",
    "        CREATE (n1)-[newRel:`${relType}`]->(target)\n",
    "        SET newRel = properties(outRel)\n",
    "    )\n",
    "    \n",
    "    // Return the node pair for the next phase\n",
    "    WITH DISTINCT n1, n2\n",
    "    \n",
    "    // Handle incoming relationships\n",
    "    OPTIONAL MATCH (source)-[inRel]->(n2)\n",
    "    WHERE source IS NOT NULL AND source <> n1 AND type(inRel) <> 'SAME_AS'\n",
    "    \n",
    "    // Create equivalent relationships to n1 if they don't already exist\n",
    "    WITH n1, n2, inRel, source, type(inRel) AS relType\n",
    "    WHERE NOT EXISTS((source)-[:`${relType}`]->(n1))\n",
    "    \n",
    "    // Create new relationship with the same properties\n",
    "    FOREACH (_ IN CASE WHEN inRel IS NOT NULL THEN [1] ELSE [] END |\n",
    "        CREATE (source)-[newRel:`${relType}`]->(n1)\n",
    "        SET newRel = properties(inRel)\n",
    "    )\n",
    "    \n",
    "    // Return distinct pairs for deletion phase\n",
    "    WITH DISTINCT n1, n2\n",
    "    \n",
    "    // Delete the second node and all its relationships\n",
    "    DETACH DELETE n2\n",
    "    \n",
    "    RETURN count(n2) AS mergedCount\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        with driver.session() as session:\n",
    "            result = session.run(merge_query)\n",
    "            record = result.single()\n",
    "            return record[\"mergedCount\"] if record else 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error during node merging: {e}\")\n",
    "        return 0\n",
    "\n",
    "merged = merge_similar_nodes()\n",
    "print(f\"Merged {merged} nodes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c57305",
   "metadata": {},
   "source": [
    "### Node similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc40641b",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_similarity_query = \"\"\"\n",
    "CALL gds.nodeSimilarity.stream('amazonGraph')\n",
    "YIELD node1, node2, similarity as node_similarity\n",
    "WHERE 'Company' IN labels(gds.util.asNode(node1)) AND 'Company' IN labels(gds.util.asNode(node2))\n",
    "AND node_similarity < 1\n",
    "RETURN gds.util.asNode(node1).name AS Company1, gds.util.asNode(node2).name AS Company2, node_similarity\n",
    "ORDER BY node_similarity DESCENDING, Company1, Company2\n",
    "\"\"\"\n",
    "\n",
    "def results_to_df(query: str) -> pd.DataFrame:\n",
    "    results = gds.execute_query(query)[0]\n",
    "    df = pd.DataFrame(results, columns=results[0].keys())\n",
    "    return df\n",
    "\n",
    "df_node_similarity = results_to_df(node_similarity_query)\n",
    "print(df_node_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb28fec",
   "metadata": {},
   "source": [
    "### Create SAME_AS relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e661965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_same_as_relationship(df, column_name):\n",
    "    # Iterate over the DataFrame rows\n",
    "    for _, row in df.iterrows():\n",
    "        node1 = row[column_name + '1']\n",
    "        node2 = row[column_name + '2']\n",
    "\n",
    "        # Run Cypher query to create 'SAME_AS' relationship\n",
    "        score = row[\"combined_score\"]\n",
    "        if score > 0.20:\n",
    "            query = f\"MATCH (n1), (n2) WHERE n1.name = '{node1}' AND n2.name = '{node2}' CREATE (n1)-[:SAME_AS]->(n2)\"\n",
    "            gds.execute_query(query)\n",
    "\n",
    "create_same_as_relationship(selected_df, \"Company\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52dbed9",
   "metadata": {},
   "source": [
    "### Merge nodes with SAME_AS relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260a3328",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_query = \"\"\"\n",
    "MATCH (n1)-[r:SAME_AS]->(n2)\n",
    "WITH n1, n2, collect(r) as relsToDel\n",
    "\n",
    "FOREACH (rel IN relsToDel | DELETE rel)\n",
    "WITH collect(DISTINCT n1) + collect(DISTINCT n2) AS nodesToMerge\n",
    "\n",
    "UNWIND nodesToMerge AS node\n",
    "\n",
    "WITH collect(DISTINCT node) AS uniqueNodesToMerge\n",
    "CALL apoc.refactor.mergeNodes(uniqueNodesToMerge, {mergeRels:true}) YIELD node\n",
    "RETURN node\n",
    "\"\"\"\n",
    "\n",
    "gds.execute_query(merge_query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
