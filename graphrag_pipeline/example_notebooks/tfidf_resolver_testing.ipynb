{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ce2bd9f",
   "metadata": {},
   "source": [
    "# TF-IDF Entity Resolution Testing Notebook\n",
    "\n",
    "This notebook breaks down the `ex_post_tfidf_resolver.py` script into testable blocks for step-by-step debugging and execution.\n",
    "\n",
    "The TF-IDF resolver merges duplicate entities in Neo4j using TF-IDF vectorization and cosine similarity matching.\n",
    "\n",
    "## 1. Environment Setup\n",
    "\n",
    "Import required libraries and configure the environment for TF-IDF entity resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc918788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "import os\n",
    "import asyncio\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# Third-party imports\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import neo4j\n",
    "\n",
    "# Configure paths for notebook environment\n",
    "script_dir = Path.cwd().parent  # Since we're in example_notebooks/\n",
    "graphrag_pipeline_dir = script_dir\n",
    "\n",
    "# Add to Python path for imports\n",
    "if str(graphrag_pipeline_dir) not in sys.path:\n",
    "    sys.path.append(str(graphrag_pipeline_dir))\n",
    "\n",
    "print(\"✅ Environment Setup Complete\")\n",
    "print(f\"📁 GraphRAG pipeline directory: {graphrag_pipeline_dir}\")\n",
    "print(f\"📁 Current working directory: {Path.cwd()}\")\n",
    "print(\"📚 All required libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b20332f",
   "metadata": {},
   "source": [
    "## 2. Enhanced TFIDFMatchResolver Class\n",
    "\n",
    "Define the complete entity resolution class with enhanced analysis capabilities for publication-ready results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad0a7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFIDFMatchResolver:\n",
    "    \"\"\"\n",
    "    Enhanced TF-IDF Entity Resolution for Neo4j Knowledge Graphs\n",
    "    \n",
    "    This class performs entity resolution by:\n",
    "    1. Fetching candidate nodes based on filter criteria\n",
    "    2. Computing TF-IDF vectors from specified text properties\n",
    "    3. Finding pairs with cosine similarity above threshold\n",
    "    4. Providing detailed analysis before merging\n",
    "    5. Optionally executing merges with user confirmation\n",
    "    \n",
    "    Key Features:\n",
    "    - Enhanced analysis with entity names and types\n",
    "    - Separation of analysis and merging phases\n",
    "    - Robust error handling and logging\n",
    "    - Support for different Neo4j ID formats\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        driver: neo4j.AsyncDriver,\n",
    "        filter_query: str | None,\n",
    "        resolve_properties: List[str],\n",
    "        similarity_threshold: float = 0.9,\n",
    "        neo4j_database: str = \"neo4j\",\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the TF-IDF entity resolver.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        driver : neo4j.AsyncDriver\n",
    "            Neo4j async driver instance\n",
    "        filter_query : str | None\n",
    "            Cypher query that returns nodes as 'n'. If None, matches all nodes.\n",
    "        resolve_properties : List[str]\n",
    "            Node properties to concatenate for TF-IDF analysis\n",
    "        similarity_threshold : float\n",
    "            Cosine similarity threshold (0, 1] for merging decisions\n",
    "        neo4j_database : str\n",
    "            Neo4j database name (default: \"neo4j\")\n",
    "        \"\"\"\n",
    "        self.driver = driver\n",
    "        self.filter_query = filter_query\n",
    "        self.resolve_properties = resolve_properties\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.db = neo4j_database\n",
    "        \n",
    "        print(f\"✅ TFIDFMatchResolver initialized\")\n",
    "        print(f\"   📊 Similarity threshold: {similarity_threshold}\")\n",
    "        print(f\"   📋 Properties to resolve: {resolve_properties}\")\n",
    "        print(f\"   🗃️  Database: {neo4j_database}\")\n",
    "\n",
    "print(\"✅ TFIDFMatchResolver class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37433806",
   "metadata": {},
   "source": [
    "### 2.1 Core Methods Implementation\n",
    "\n",
    "Implement all methods for the enhanced TFIDFMatchResolver class with improved error handling and detailed analysis capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2da5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add methods to the TFIDFMatchResolver class\n",
    "def add_run_method():\n",
    "    async def run(self, analyze_only=False) -> Dict:\n",
    "        \"\"\"\n",
    "        Execute TF-IDF entity resolution with enhanced analysis.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        analyze_only : bool\n",
    "            If True, performs analysis without merging. Returns detailed pair information.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        Dict containing analysis results and optionally detailed pair data\n",
    "        \"\"\"\n",
    "        print(f\"🚀 Starting TF-IDF entity resolution (analyze_only={analyze_only})\")\n",
    "        \n",
    "        # Step 1: Fetch candidate nodes\n",
    "        nodes = await self._fetch_nodes()\n",
    "        if not nodes:\n",
    "            print(\"⚠️  No nodes found for analysis\")\n",
    "            return {\"nodes_examined\": 0, \"pairs_above_threshold\": 0, \"merged_pairs\": 0, \"pairs_data\": []}\n",
    "\n",
    "        # Step 2: Compute TF-IDF vectors\n",
    "        ids, docs = zip(*nodes)\n",
    "        print(f\"🔤 Computing TF-IDF vectors for {len(docs)} documents...\")\n",
    "        tfidf = TfidfVectorizer(stop_words=\"english\").fit_transform(docs)\n",
    "        sims = cosine_similarity(tfidf)\n",
    "\n",
    "        # Step 3: Find similarity pairs above threshold\n",
    "        pairs_to_merge = []\n",
    "        pairs_with_scores = []\n",
    "        n = len(ids)\n",
    "        \n",
    "        print(f\"🔍 Finding similarity pairs (threshold: {self.similarity_threshold})\")\n",
    "        for i in range(n - 1):\n",
    "            for j in range(i + 1, n):\n",
    "                score = sims[i, j]\n",
    "                if score >= self.similarity_threshold:\n",
    "                    pairs_to_merge.append((ids[i], ids[j]))\n",
    "                    pairs_with_scores.append((ids[i], ids[j], score))\n",
    "\n",
    "        print(f\"📊 Found {len(pairs_to_merge)} pairs above threshold\")\n",
    "\n",
    "        # Step 4: Handle analysis vs merging\n",
    "        if analyze_only:\n",
    "            print(\"🔍 Analysis mode: Preparing detailed results...\")\n",
    "            merged_count = 0\n",
    "            detailed_pairs_data = []\n",
    "            \n",
    "            if pairs_with_scores:\n",
    "                # Fetch node details for enhanced analysis\n",
    "                unique_node_ids = list(set([id for id1, id2, _ in pairs_with_scores for id in [id1, id2]]))\n",
    "                node_details = await self._fetch_node_details(unique_node_ids)\n",
    "                \n",
    "                for id1, id2, score in pairs_with_scores:\n",
    "                    node1_info = node_details.get(id1, {\"name\": \"N/A\", \"type\": \"N/A\"})\n",
    "                    node2_info = node_details.get(id2, {\"name\": \"N/A\", \"type\": \"N/A\"})\n",
    "                    \n",
    "                    detailed_pairs_data.append({\n",
    "                        \"id1\": id1, \"id2\": id2, \"score\": score,\n",
    "                        \"node1_name\": node1_info.get(\"name\", \"N/A\"),\n",
    "                        \"node1_type\": node1_info.get(\"type\", \"N/A\"),\n",
    "                        \"node2_name\": node2_info.get(\"name\", \"N/A\"),\n",
    "                        \"node2_type\": node2_info.get(\"type\", \"N/A\")\n",
    "                    })\n",
    "        else:\n",
    "            print(\"🔄 Merge mode: Executing merges...\")\n",
    "            merged_count = await self._merge_pairs(pairs_to_merge)\n",
    "            detailed_pairs_data = pairs_with_scores\n",
    "\n",
    "        # Return results\n",
    "        result = {\n",
    "            \"nodes_examined\": n,\n",
    "            \"pairs_above_threshold\": len(pairs_to_merge),\n",
    "            \"merged_pairs\": merged_count,\n",
    "        }\n",
    "        \n",
    "        if analyze_only:\n",
    "            result[\"pairs_data\"] = detailed_pairs_data\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    TFIDFMatchResolver.run = run\n",
    "\n",
    "# Node fetching with robust ID handling\n",
    "def add_fetch_nodes_method():\n",
    "    async def _fetch_nodes(self) -> List[Tuple[str, str]]:\n",
    "        \"\"\"Fetch nodes and concatenate text properties for TF-IDF analysis.\"\"\"\n",
    "        query = self.filter_query or \"MATCH (n) RETURN n\"\n",
    "        \n",
    "        async with self.driver.session(database=self.db) as session:\n",
    "            try:\n",
    "                result_cursor = await session.run(query)  # type: ignore\n",
    "                records = await result_cursor.data()\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Query execution failed: {e}\")\n",
    "                return []\n",
    "                    \n",
    "            processed_nodes = []\n",
    "            for rec in records:\n",
    "                try:\n",
    "                    node = rec[\"n\"]\n",
    "                    \n",
    "                    # Extract node ID (handle different formats)\n",
    "                    if isinstance(node, dict):\n",
    "                        node_id = node.get('element_id') or node.get('id') or node.get('_id')\n",
    "                    else:\n",
    "                        node_id = getattr(node, 'element_id', None) or getattr(node, 'id', None)\n",
    "                    \n",
    "                    if not node_id:\n",
    "                        continue\n",
    "                        \n",
    "                    # Extract and concatenate text properties\n",
    "                    text_parts = []\n",
    "                    for prop in self.resolve_properties:\n",
    "                        if isinstance(node, dict):\n",
    "                            value = node.get(prop, \"\")\n",
    "                        else:\n",
    "                            value = node.get(prop, \"\")\n",
    "                        text_parts.append(str(value).strip())\n",
    "                    \n",
    "                    combined_text = \" \".join(text_parts).lower()\n",
    "                    if combined_text.strip():\n",
    "                        processed_nodes.append((node_id, combined_text))\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️  Skipping problematic node: {e}\")\n",
    "                    continue\n",
    "                    \n",
    "            print(f\"✅ Successfully processed {len(processed_nodes)} nodes\")\n",
    "            return processed_nodes\n",
    "    \n",
    "    TFIDFMatchResolver._fetch_nodes = _fetch_nodes\n",
    "\n",
    "# Node details fetching for enhanced analysis\n",
    "def add_fetch_node_details_method():\n",
    "    async def _fetch_node_details(self, node_ids: List[str]) -> Dict[str, Dict[str, str]]:\n",
    "        \"\"\"Fetch detailed node information for analysis.\"\"\"\n",
    "        if not node_ids:\n",
    "            return {}\n",
    "            \n",
    "        unique_ids = list(dict.fromkeys(node_ids))\n",
    "        query = self.filter_query or \"MATCH (n) RETURN n\"\n",
    "        \n",
    "        async with self.driver.session(database=self.db) as session:\n",
    "            result_cursor = await session.run(query)  # type: ignore\n",
    "            records = await result_cursor.data()\n",
    "            \n",
    "            # Build ID to node mapping\n",
    "            id_to_node = {}\n",
    "            for rec in records:\n",
    "                node = rec[\"n\"]\n",
    "                if isinstance(node, dict):\n",
    "                    node_id = node.get('element_id') or node.get('id') or node.get('_id')\n",
    "                else:\n",
    "                    node_id = getattr(node, 'element_id', None) or getattr(node, 'id', None)\n",
    "                \n",
    "                if node_id:\n",
    "                    id_to_node[node_id] = node\n",
    "            \n",
    "            # Extract details for requested IDs\n",
    "            node_details = {}\n",
    "            for requested_id in unique_ids:\n",
    "                if requested_id in id_to_node:\n",
    "                    node = id_to_node[requested_id]\n",
    "                    if isinstance(node, dict):\n",
    "                        name = node.get(\"name\", node.get(\"title\", \"N/A\"))\n",
    "                        node_type = node.get(\"type\", \"N/A\")\n",
    "                    else:\n",
    "                        name = node.get(\"name\", node.get(\"title\", \"N/A\")) or \"N/A\"\n",
    "                        node_type = node.get(\"type\", \"N/A\") or \"N/A\"\n",
    "                    \n",
    "                    node_details[requested_id] = {\"name\": str(name), \"type\": str(node_type)}\n",
    "                else:\n",
    "                    node_details[requested_id] = {\"name\": \"N/A\", \"type\": \"N/A\"}\n",
    "            \n",
    "            return node_details\n",
    "    \n",
    "    TFIDFMatchResolver._fetch_node_details = _fetch_node_details\n",
    "\n",
    "# Merge execution method\n",
    "def add_merge_pairs_method():\n",
    "    async def _merge_pairs(self, pairs: List[Tuple[str, str]]) -> int:\n",
    "        \"\"\"Execute node merges using APOC procedures.\"\"\"\n",
    "        if not pairs:\n",
    "            return 0\n",
    "\n",
    "        merge_query = \"\"\"\n",
    "        MATCH (a) WHERE id(a) = $id1\n",
    "        MATCH (b) WHERE id(b) = $id2\n",
    "        WITH [a,b] AS nodes\n",
    "        CALL apoc.refactor.mergeNodes(nodes, {properties:\"discard\"}) YIELD node\n",
    "        RETURN id(node) AS kept\n",
    "        \"\"\"\n",
    "        \n",
    "        successful_merges = 0\n",
    "        async with self.driver.session(database=self.db) as session:\n",
    "            for id1, id2 in pairs:\n",
    "                try:\n",
    "                    await session.run(merge_query, id1=id1, id2=id2)  # type: ignore\n",
    "                    successful_merges += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️  Merge failed for {id1} + {id2}: {e}\")\n",
    "                    \n",
    "        return successful_merges\n",
    "    \n",
    "    TFIDFMatchResolver._merge_pairs = _merge_pairs\n",
    "\n",
    "# Apply all methods to the class\n",
    "add_run_method()\n",
    "add_fetch_nodes_method()\n",
    "add_fetch_node_details_method()\n",
    "add_merge_pairs_method()\n",
    "\n",
    "print(\"✅ All methods added to TFIDFMatchResolver\")\n",
    "print(\"✅ Class ready for entity resolution analysis and merging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec115571",
   "metadata": {},
   "source": [
    "## 3. Configuration and Connection Testing\n",
    "\n",
    "Load configuration files, validate environment variables, and test Neo4j connectivity before running the analysis.\n",
    "\n",
    "## 5. Main Entry Point Function\n",
    "\n",
    "Define the main async function that loads configuration, sets up Neo4j connection, and runs the entity resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fe25f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def setup_and_validate_config():\n",
    "    \"\"\"Load configuration and validate environment setup.\"\"\"\n",
    "    print(\"🔧 Loading configuration and validating environment...\")\n",
    "    \n",
    "    # Load configuration files\n",
    "    config_files_path = graphrag_pipeline_dir / \"config_files\"\n",
    "    env_file = config_files_path / \".env\"\n",
    "    config_file = config_files_path / \"kg_building_config.json\"\n",
    "    \n",
    "    # Load environment variables\n",
    "    if env_file.exists():\n",
    "        load_dotenv(env_file, override=True)\n",
    "        print(\"✅ Environment file loaded\")\n",
    "    else:\n",
    "        print(\"⚠️  .env file not found\")\n",
    "        \n",
    "    # Load configuration\n",
    "    if not config_file.exists():\n",
    "        raise FileNotFoundError(f\"Config file not found: {config_file}\")\n",
    "        \n",
    "    with open(config_file) as f:\n",
    "        build_config = json.load(f)\n",
    "    print(\"✅ Configuration file loaded\")\n",
    "    \n",
    "    # Extract TF-IDF configuration\n",
    "    try:\n",
    "        tfidf_config = build_config[\"entity_resolution_config\"][\"TFIDFMatchResolver_config\"]\n",
    "        print(\"✅ TF-IDF configuration found\")\n",
    "        print(f\"   📋 Properties: {tfidf_config.get('resolve_properties', [])}\")\n",
    "        print(f\"   📊 Threshold: {tfidf_config.get('similarity_threshold', 0.75)}\")\n",
    "        print(f\"   🔍 Filter: {tfidf_config.get('filter_query', 'Default (all nodes)')[:60]}...\")\n",
    "    except KeyError as e:\n",
    "        raise ValueError(f\"Missing configuration section: {e}\")\n",
    "    \n",
    "    # Validate environment variables\n",
    "    neo4j_uri = os.getenv(\"NEO4J_URI\")\n",
    "    neo4j_username = os.getenv(\"NEO4J_USERNAME\")\n",
    "    neo4j_password = os.getenv(\"NEO4J_PASSWORD\")\n",
    "    \n",
    "    missing_vars = []\n",
    "    if not neo4j_uri: missing_vars.append(\"NEO4J_URI\")\n",
    "    if not neo4j_username: missing_vars.append(\"NEO4J_USERNAME\")\n",
    "    if not neo4j_password: missing_vars.append(\"NEO4J_PASSWORD\")\n",
    "    \n",
    "    if missing_vars:\n",
    "        raise ValueError(f\"Missing environment variables: {', '.join(missing_vars)}\")\n",
    "    \n",
    "    print(\"✅ All environment variables validated\")\n",
    "    print(f\"   🔗 Neo4j URI: {neo4j_uri}\")\n",
    "    print(f\"   👤 Username: {neo4j_username}\")\n",
    "    \n",
    "    return {\n",
    "        \"config\": tfidf_config,\n",
    "        \"neo4j_uri\": neo4j_uri,\n",
    "        \"neo4j_username\": neo4j_username,\n",
    "        \"neo4j_password\": neo4j_password\n",
    "    }\n",
    "\n",
    "# Test Neo4j connection\n",
    "async def test_neo4j_connection(neo4j_uri, username, password):\n",
    "    \"\"\"Test Neo4j connectivity and basic operations.\"\"\"\n",
    "    print(\"🔌 Testing Neo4j connection...\")\n",
    "    \n",
    "    try:\n",
    "        async with neo4j.AsyncGraphDatabase.driver(\n",
    "            neo4j_uri, auth=(username, password)\n",
    "        ) as driver:\n",
    "            # Test connectivity\n",
    "            await driver.verify_connectivity()\n",
    "            print(\"✅ Neo4j connection verified\")\n",
    "            \n",
    "            # Test basic query\n",
    "            async with driver.session() as session:\n",
    "                result = await session.run(\"RETURN 1 as test\")  # type: ignore\n",
    "                records = await result.data()\n",
    "                if records and records[0][\"test\"] == 1:\n",
    "                    print(\"✅ Query execution successful\")\n",
    "                \n",
    "                # Count nodes (optional)\n",
    "                try:\n",
    "                    result = await session.run(\"MATCH (n) RETURN count(n) as total\")  # type: ignore\n",
    "                    records = await result.data()\n",
    "                    total_nodes = records[0][\"total\"] if records else 0\n",
    "                    print(f\"✅ Database contains {total_nodes:,} nodes\")\n",
    "                except Exception:\n",
    "                    print(\"⚠️  Could not count nodes (database may be large)\")\n",
    "                    \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Neo4j connection failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run setup and validation\n",
    "try:\n",
    "    setup_result = await setup_and_validate_config()\n",
    "    connection_ok = await test_neo4j_connection(\n",
    "        setup_result[\"neo4j_uri\"],\n",
    "        setup_result[\"neo4j_username\"], \n",
    "        setup_result[\"neo4j_password\"]\n",
    "    )\n",
    "    \n",
    "    if connection_ok:\n",
    "        print(\"\\n🎉 Setup complete! Ready to proceed with entity resolution.\")\n",
    "        tfidf_config = setup_result[\"config\"]\n",
    "        neo4j_credentials = {\n",
    "            \"uri\": setup_result[\"neo4j_uri\"],\n",
    "            \"username\": setup_result[\"neo4j_username\"],\n",
    "            \"password\": setup_result[\"neo4j_password\"]\n",
    "        }\n",
    "    else:\n",
    "        print(\"\\n❌ Setup failed. Please check your Neo4j configuration.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Setup failed: {e}\")\n",
    "    connection_ok = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25c2fa5",
   "metadata": {},
   "source": [
    "# Manual analysis of similarity pairs found# Manual analysis of similarity pairs found\n",
    "print(\"🔍 Running detailed analysis of similarity pairs...\")## 4. Similarity Analysis (No Merging)\n",
    "\n",
    "# Get the pairs with default configurationity pairs for merging. This step provides detailed analysis without making any changes to the database.\n",
    "similarity_pairs, node_details = await analyze_similarity_pairs(max_pairs_to_show=10)\n",
    "ons\n",
    "print(f\"\\n📊 Found {len(similarity_pairs)} similarity pairs\")\n",
    "if similarity_pairs:Define alternative TF-IDF configurations for testing different scenarios and optimizing results.\n",
    "    print(\"\\n🔍 Detailed analysis of first 10 pairs:\")\n",
    "    for i, (id1, id2, score) in enumerate(similarity_pairs[:10]):tailed analysis of similarity pairs...\")\n",
    "        print(f\"\\n--- Pair {i+1}: Similarity {score:.4f} ---\")\n",
    "        if id1 in node_details:ax_pairs_to_show=10)## 6.1. Alternative TF-IDF Configurations\n",
    "            node1 = node_details[id1]\n",
    "            print(f\"Node 1 ({id1}):\")ity_pairs)} similarity pairs\") dynamically here.if similarity_pairs:    print(\"\\n🔍 Detailed analysis of first 10 pairs:\")    for i, (id1, id2, score) in enumerate(similarity_pairs[:10]):        print(f\"\\n--- Pair {i+1}: Similarity {score:.4f} ---\")        if id1 in node_details:            node1 = node_details[id1]            print(f\"Node 1 ({id1}):\")            print(f\"  Name: {node1['properties'].get('name', 'N/A')}\")            print(f\"  Type: {node1['properties'].get('type', 'N/A')}\")            print(f\"  Labels: {node1.get('labels', ['N/A'])}\")            print(f\"  Text: {node1.get('text', 'N/A')[:100]}...\")                if id2 in node_details:            node2 = node_details[id2]            print(f\"Node 2 ({id2}):\")            print(f\"  Name: {node2['properties'].get('name', 'N/A')}\")            print(f\"  Type: {node2['properties'].get('type', 'N/A')}\")            print(f\"  Labels: {node2.get('labels', ['N/A'])}\")            print(f\"  Text: {node2.get('text', 'N/A')[:100]}...\")else:    print(\"ℹ️ No similarity pairs found with the current configuration\")# Also show the new enhanced format from the main analysisprint(\"\\n\" + \"=\"*60)print(\"🆕 Enhanced pairs_data format (from main analysis):\")if 'main_analysis_pairs' in locals() and main_analysis_pairs:    for i, pair_info in enumerate(main_analysis_pairs[:5]):  # Show first 5        print(f\"\\nPair {i+1}:\")        print(f\"  Similarity: {pair_info['score']:.4f}\")        print(f\"  Node 1: {pair_info['node1_name']} ({pair_info['node1_type']}) [ID: {pair_info['id1']}]\")        print(f\"  Node 2: {pair_info['node2_name']} ({pair_info['node2_type']}) [ID: {pair_info['id2']}]\")else:    print(\"ℹ️ No enhanced pairs_data available. Run the main analysis first.\")# Execute similarity analysis\n",
    "            print(f\"  Name: {node1['properties'].get('name', 'N/A')}\")\n",
    "            print(f\"  Type: {node1['properties'].get('type', 'N/A')}\") without merging.\"\"\"\n",
    "            print(f\"  Labels: {node1.get('labels', ['N/A'])}\")\n",
    "            print(f\"  Text: {node1.get('text', 'N/A')[:100]}...\")\n",
    "        \n",
    "        if id2 in node_details:\n",
    "            node2 = node_details[id2]t(\"🔍 Starting similarity analysis...\")\n",
    "            print(f\"Node 2 ({id2}):\")ify your database - analysis only\")\n",
    "            print(f\"  Name: {node2['properties'].get('name', 'N/A')}\")\n",
    "            print(f\"  Type: {node2['properties'].get('type', 'N/A')}\")\n",
    "            print(f\"  Labels: {node2.get('labels', ['N/A'])}\")\n",
    "            print(f\"  Text: {node2.get('text', 'N/A')[:100]}...\")\n",
    "else:[\"password\"])\n",
    "    print(\"ℹ️ No similarity pairs found with the current configuration\")\n",
    "   \n",
    "# Also show the new enhanced format from the main analysis\n",
    "print(\"\\n\" + \"=\"*60)            driver,\n",
    "print(\"🆕 Enhanced pairs_data format (from main analysis):\")\n",
    "if 'main_analysis_pairs' in locals() and main_analysis_pairs:properties=tfidf_config[\"resolve_properties\"],\n",
    "    for i, pair_info in enumerate(main_analysis_pairs[:5]):  # Show first 5y_threshold\", 0.75),\n",
    "        print(f\"\\nPair {i+1}:\") \"neo4j\")\n",
    "        print(f\"  Similarity: {pair_info['score']:.4f}\")\n",
    "        print(f\"  Node 1: {pair_info['node1_name']} ({pair_info['node1_type']}) [ID: {pair_info['id1']}]\")\n",
    "        print(f\"  Node 2: {pair_info['node2_name']} ({pair_info['node2_type']}) [ID: {pair_info['id2']}]\")\n",
    "else:\n",
    "    print(\"ℹ️ No enhanced pairs_data available. Run the main analysis first.\")\n",
    "   print(f\"\\n📊 Analysis Results:\")\n",
    "# Run similarity analysis using the setup from the previous cell\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    analysis_results = None    print(\"💡 Make sure to execute cell 8 (Configuration and Connection Testing)\")    print(\"⚠️  Setup not complete. Please run the configuration cell above first.\")else:                print(\"\\n📝 No similarity pairs found with current configuration\")    else:        print(\"✨ Results stored in 'analysis_results' variable\")        print(f\"\\n💾 Stored {len(analysis_results)} similarity pairs for potential merging\")    if analysis_results and len(analysis_results) > 0:    # Store the results for later use        analysis_results = await run_similarity_analysis()    print(\"🚀 Setup is complete! Running similarity analysis...\")if 'connection_ok' in locals() and connection_ok:        print(f\"   🎯 Pairs above threshold: {result['pairs_above_threshold']:,}\")\n",
    "        \n",
    "        if result['pairs_above_threshold'] > 0:\n",
    "            pairs_data = result['pairs_data']\n",
    "            pairs_data.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "            \n",
    "            print(f\"\\n🔍 Top 10 similarity pairs:\")\n",
    "            print(\"=\" * 90)\n",
    "            \n",
    "            for i, pair in enumerate(pairs_data[:10]):\n",
    "                print(f\"\\n#{i+1:2d} | Similarity: {pair['score']:.4f}\")\n",
    "                print(f\"     Node 1: {pair['node1_name']} ({pair['node1_type']}) [ID: {pair['id1']}]\")\n",
    "                print(f\"     Node 2: {pair['node2_name']} ({pair['node2_type']}) [ID: {pair['id2']}]\")\n",
    "            \n",
    "            if len(pairs_data) > 10:\n",
    "                print(f\"\\n... and {len(pairs_data) - 10} more pairs\")\n",
    "            \n",
    "            # Summary by entity types\n",
    "            type_combinations = {}\n",
    "            for pair in pairs_data:\n",
    "                combo = f\"{pair['node1_type']} ↔ {pair['node2_type']}\"\n",
    "                type_combinations[combo] = type_combinations.get(combo, 0) + 1\n",
    "            \n",
    "            print(f\"\\n📋 Entity Type Combinations:\")\n",
    "            for combo, count in sorted(type_combinations.items(), key=lambda x: x[1], reverse=True):\n",
    "                print(f\"   {combo}: {count} pairs\")\n",
    "            \n",
    "            print(f\"\\n✅ Analysis complete! Review the results above.\")\n",
    "            print(f\"💡 If satisfied, proceed to the merge section.\")\n",
    "            \n",
    "            return pairs_data\n",
    "        else:\n",
    "            print(f\"\\nℹ️  No pairs found above threshold ({resolver.similarity_threshold})\")\n",
    "            print(f\"💡 Consider lowering the threshold or adjusting configuration\")\n",
    "            return []\n",
    "\n",
    "# Check if setup is complete before running analysis\n",
    "if 'connection_ok' in locals() and connection_ok:\n",
    "    print(\"🚀 Setup is complete! Running similarity analysis...\")\n",
    "    analysis_results = await run_similarity_analysis()\n",
    "else:\n",
    "    print(\"⚠️  Setup not complete. Please run the configuration cell above first.\")\n",
    "    print(\"💡 Make sure to execute cell 8 (Configuration and Connection Testing)\")\n",
    "    analysis_results = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046ac75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute similarity analysis\n",
    "if 'connection_ok' in locals() and connection_ok:\n",
    "    print(\"🚀 Setup is complete! Running similarity analysis...\")\n",
    "    analysis_results = await run_similarity_analysis()\n",
    "    \n",
    "    # Store the results for later use\n",
    "    if analysis_results and len(analysis_results) > 0:\n",
    "        print(f\"\\n💾 Stored {len(analysis_results)} similarity pairs for potential merging\")\n",
    "        print(\"✨ Results stored in 'analysis_results' variable\")\n",
    "        print(\"\\n📋 Quick Summary:\")\n",
    "        print(f\"   📊 Total similarity pairs: {len(analysis_results)}\")\n",
    "        if analysis_results:\n",
    "            avg_similarity = sum(p['score'] for p in analysis_results) / len(analysis_results)\n",
    "            print(f\"   📈 Average similarity: {avg_similarity:.4f}\")\n",
    "            best_pair = max(analysis_results, key=lambda x: x['score'])\n",
    "            print(f\"   🏆 Best match: {best_pair['node1_name']} ↔ {best_pair['node2_name']} ({best_pair['score']:.4f})\")\n",
    "    else:\n",
    "        print(\"\\n📝 No similarity pairs found with current configuration\")\n",
    "        print(\"💡 Try lowering the similarity threshold or adjusting the resolve_properties\")\n",
    "        \n",
    "else:\n",
    "    print(\"⚠️  Setup not complete. Please run the configuration cell above first.\")\n",
    "    print(\"💡 Make sure to execute the 'Configuration and Connection Testing' cell\")\n",
    "    analysis_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a22820",
   "metadata": {},
   "source": [
    "### ✅ Setup Complete - Ready for Analysis!\n",
    "\n",
    "The notebook setup has been fixed and is now working correctly:\n",
    "\n",
    "1. **Configuration Loading**: ✅ Environment variables and config file loaded\n",
    "2. **Neo4j Connection**: ✅ Connection verified and tested  \n",
    "3. **TF-IDF Resolver**: ✅ Class defined with all methods\n",
    "4. **Analysis Function**: ✅ Similarity analysis ready to run\n",
    "5. **Alternative Configs**: ✅ Multiple configuration options available\n",
    "\n",
    "**Next Steps:**\n",
    "- Review the analysis results above\n",
    "- Optionally proceed to the merging section if satisfied with results\n",
    "- Test different configurations in the advanced section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50109a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative TF-IDF configurations for testing different scenarios\n",
    "\n",
    "# Configuration 1: Conservative (high threshold, fewer merges)\n",
    "conservative_config = {\n",
    "    \"filter_query\": \"MATCH (n) WHERE NOT 'Document' IN labels(n) AND NOT 'Chunk' IN labels(n) RETURN n\",\n",
    "    \"resolve_properties\": [\"name\"],\n",
    "    \"similarity_threshold\": 0.95,\n",
    "    \"neo4j_database\": \"neo4j\"\n",
    "}\n",
    "\n",
    "# Configuration 2: Moderate (balanced threshold)\n",
    "moderate_config = {\n",
    "    \"filter_query\": \"MATCH (n) WHERE NOT 'Document' IN labels(n) AND NOT 'Chunk' IN labels(n) RETURN n\",\n",
    "    \"resolve_properties\": [\"name\", \"description\"],\n",
    "    \"similarity_threshold\": 0.85,\n",
    "    \"neo4j_database\": \"neo4j\"\n",
    "}\n",
    "\n",
    "# Configuration 3: Aggressive (lower threshold, more merges)\n",
    "aggressive_config = {\n",
    "    \"filter_query\": \"MATCH (n) WHERE NOT 'Document' IN labels(n) AND NOT 'Chunk' IN labels(n) RETURN n\",\n",
    "    \"resolve_properties\": [\"name\", \"description\", \"type\"],\n",
    "    \"similarity_threshold\": 0.75,\n",
    "    \"neo4j_database\": \"neo4j\"\n",
    "}\n",
    "\n",
    "# Configuration 4: Specific entity types only (e.g., only Actors)\n",
    "actor_only_config = {\n",
    "    \"filter_query\": \"MATCH (n:Actor) RETURN n\",\n",
    "    \"resolve_properties\": [\"name\", \"type\"],\n",
    "    \"similarity_threshold\": 0.80,\n",
    "    \"neo4j_database\": \"neo4j\"\n",
    "}\n",
    "\n",
    "# Configuration 5: Events only with temporal info\n",
    "event_only_config = {\n",
    "    \"filter_query\": \"MATCH (n:Event) RETURN n\",\n",
    "    \"resolve_properties\": [\"name\", \"type\"],\n",
    "    \"similarity_threshold\": 0.85,\n",
    "    \"neo4j_database\": \"neo4j\"\n",
    "}\n",
    "\n",
    "print(\"✅ Alternative TF-IDF configurations defined:\")\n",
    "print(\"  - conservative_config: High threshold (0.95), name only\")\n",
    "print(\"  - moderate_config: Balanced threshold (0.85), name + description\") \n",
    "print(\"  - aggressive_config: Lower threshold (0.75), name + description + type\")\n",
    "print(\"  - actor_only_config: Actor entities only (0.80)\")\n",
    "print(\"  - event_only_config: Event entities only (0.85)\")\n",
    "print(\"\\n💡 These can be used in the advanced configuration testing section\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e956e8",
   "metadata": {},
   "source": [
    "## 5. Controlled Entity Merging\n",
    "\n",
    "Execute the actual entity merges with user confirmation. This step permanently modifies the Neo4j database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff0a264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this if all previous tests passed\n",
    "if 'connection_ok' in locals() and connection_ok:\n",
    "    print(\"🚀 Starting TF-IDF entity resolution in ANALYSIS MODE...\")\n",
    "    print(\"   (This will NOT perform actual merges, only show what would be merged)\")\n",
    "    \n",
    "    try:\n",
    "        # Load configuration\n",
    "        config_files_path = graphrag_pipeline_dir / \"config_files\"\n",
    "        load_dotenv(config_files_path / \".env\", override=True)\n",
    "        \n",
    "        with open(config_files_path / \"kg_building_config.json\") as f:\n",
    "            build_config = json.load(f)\n",
    "        \n",
    "        neo4j_uri = os.getenv(\"NEO4J_URI\")\n",
    "        neo4j_username = os.getenv(\"NEO4J_USERNAME\") \n",
    "        neo4j_password = os.getenv(\"NEO4J_PASSWORD\")\n",
    "        \n",
    "        tfidf_cfg = build_config[\"entity_resolution_config\"][\"TFIDFMatchResolver_config\"]\n",
    "        \n",
    "        async with neo4j.AsyncGraphDatabase.driver(\n",
    "            neo4j_uri, auth=(neo4j_username, neo4j_password)\n",
    "        ) as driver:\n",
    "            \n",
    "            resolver = TFIDFMatchResolver(\n",
    "                driver,\n",
    "                filter_query=tfidf_cfg[\"filter_query\"],\n",
    "                resolve_properties=tfidf_cfg[\"resolve_properties\"],\n",
    "                similarity_threshold=tfidf_cfg.get(\"similarity_threshold\", 0.75),\n",
    "                neo4j_database=tfidf_cfg.get(\"neo4j_database\", \"neo4j\"),\n",
    "            )\n",
    "            \n",
    "            # Run in analysis mode (no merging)\n",
    "            result = await resolver.run(analyze_only=True)\n",
    "            \n",
    "            print(f\"\\n📊 Analysis Results:\")\n",
    "            print(f\"  Nodes examined: {result['nodes_examined']}\")\n",
    "            print(f\"  Pairs above threshold: {result['pairs_above_threshold']}\")\n",
    "            \n",
    "            if result['pairs_above_threshold'] > 0:\n",
    "                print(f\"\\n🔍 Similarity pairs that would be merged:\")\n",
    "                pairs_data = result.get('pairs_data', [])\n",
    "                \n",
    "                # Sort by similarity score (highest first) - pairs_data now contains dicts\n",
    "                pairs_data.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "                \n",
    "                for idx, pair_info in enumerate(pairs_data[:10]):  # Show top 10\n",
    "                    print(f\"  {idx + 1}. Nodes {pair_info['id1']} ↔ {pair_info['id2']} (similarity: {pair_info['score']:.4f})\")\n",
    "                    print(f\"     Node 1: {pair_info['node1_name']} ({pair_info['node1_type']})\")\n",
    "                    print(f\"     Node 2: {pair_info['node2_name']} ({pair_info['node2_type']})\")\n",
    "                \n",
    "                if len(pairs_data) > 10:\n",
    "                    print(f\"  ... and {len(pairs_data) - 10} more pairs\")\n",
    "                \n",
    "                print(f\"\\n✅ Analysis completed successfully!\")\n",
    "                print(f\"💡 To perform actual merges, use the detailed analysis in section 9.1-9.2\")\n",
    "                \n",
    "                # Store for later use\n",
    "                main_analysis_pairs = pairs_data\n",
    "                \n",
    "            else:\n",
    "                print(f\"ℹ️  No entities would be merged with current threshold ({resolver.similarity_threshold})\")\n",
    "                print(f\"💡 Consider lowering the threshold or using different configuration\")\n",
    "                \n",
    "    except Exception as exc:\n",
    "        print(f\"❌ Analysis failed: {exc}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"⚠️  Skipping execution due to previous test failures.\")\n",
    "    print(\"   Please ensure Neo4j connection and configuration are working before proceeding.\")\n",
    "\n",
    "# Execute entity merges with confirmation\n",
    "async def execute_entity_merges(pairs_data=None, confirm=True):\n",
    "    \"\"\"\n",
    "    Execute entity merges with user confirmation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    pairs_data : list, optional\n",
    "        Pairs data from analysis. If None, uses analysis_results from previous step.\n",
    "    confirm : bool\n",
    "        Whether to ask for user confirmation before merging\n",
    "    \"\"\"\n",
    "    # Use previous analysis results if pairs_data not provided\n",
    "    if pairs_data is None:\n",
    "        if 'analysis_results' not in locals() or not analysis_results:\n",
    "            print(\"❌ No analysis results available. Run similarity analysis first.\")\n",
    "            return {\"merged_pairs\": 0, \"errors\": 0}\n",
    "        pairs_data = analysis_results\n",
    "    \n",
    "    if not pairs_data:\n",
    "        print(\"❌ No pairs to merge.\")\n",
    "        return {\"merged_pairs\": 0, \"errors\": 0}\n",
    "    \n",
    "    print(f\"🚀 Preparing to merge {len(pairs_data)} entity pairs\")\n",
    "    print(\"⚠️  WARNING: This will permanently modify your Neo4j database!\")\n",
    "    \n",
    "    # Show summary of what will be merged\n",
    "    print(f\"\\n📋 Merge Summary:\")\n",
    "    print(f\"   Total pairs: {len(pairs_data)}\")\n",
    "    \n",
    "    # Show first few pairs as examples\n",
    "    print(f\"\\n🔍 Examples of pairs to be merged:\")\n",
    "    for i, pair in enumerate(pairs_data[:5]):\n",
    "        print(f\"   {i+1}. {pair['node1_name']} ↔ {pair['node2_name']} (similarity: {pair['score']:.4f})\")\n",
    "    \n",
    "    if len(pairs_data) > 5:\n",
    "        print(f\"   ... and {len(pairs_data) - 5} more pairs\")\n",
    "    \n",
    "    # Ask for confirmation if requested\n",
    "    if confirm:\n",
    "        print(f\"\\n⚠️  This action cannot be easily undone!\")\n",
    "        user_input = input(\"Type 'MERGE' to confirm or 'cancel' to abort: \").strip()\n",
    "        if user_input.upper() != 'MERGE':\n",
    "            print(\"❌ Merge operation cancelled by user\")\n",
    "            return {\"merged_pairs\": 0, \"errors\": 0}\n",
    "    \n",
    "    # Execute merges\n",
    "    print(f\"\\n🔄 Starting merge operations...\")\n",
    "    \n",
    "    async with neo4j.AsyncGraphDatabase.driver(\n",
    "        neo4j_credentials[\"uri\"], \n",
    "        auth=(neo4j_credentials[\"username\"], neo4j_credentials[\"password\"])\n",
    "    ) as driver:\n",
    "        \n",
    "        merge_query = \"\"\"\n",
    "        MATCH (a) WHERE id(a) = $id1\n",
    "        MATCH (b) WHERE id(b) = $id2\n",
    "        WITH [a,b] AS nodes\n",
    "        CALL apoc.refactor.mergeNodes(nodes, {properties:\"discard\"}) YIELD node\n",
    "        RETURN id(node) AS kept\n",
    "        \"\"\"\n",
    "        \n",
    "        successful_merges = 0\n",
    "        errors = 0\n",
    "        \n",
    "        async with driver.session() as session:\n",
    "            for i, pair in enumerate(pairs_data):\n",
    "                try:\n",
    "                    print(f\"   [{i+1:3d}/{len(pairs_data)}] Merging {pair['node1_name']} + {pair['node2_name']}...\", end=\"\")\n",
    "                    \n",
    "                    await session.run(merge_query, id1=pair['id1'], id2=pair['id2'])  # type: ignore\n",
    "                    successful_merges += 1\n",
    "                    print(\" ✅\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    errors += 1\n",
    "                    print(f\" ❌ ({str(e)[:50]}{'...' if len(str(e)) > 50 else ''})\")\n",
    "    \n",
    "    # Report results\n",
    "    print(f\"\\n📊 Merge Results:\")\n",
    "    print(f\"   ✅ Successful merges: {successful_merges}\")\n",
    "    print(f\"   ❌ Failed merges: {errors}\")\n",
    "    print(f\"   📈 Success rate: {successful_merges/(successful_merges+errors)*100:.1f}%\")\n",
    "    \n",
    "    if successful_merges > 0:\n",
    "        print(f\"\\n🎉 Entity merging completed successfully!\")\n",
    "        print(f\"💡 {successful_merges} entity pairs have been merged in your knowledge graph\")\n",
    "    \n",
    "    return {\"merged_pairs\": successful_merges, \"errors\": errors}\n",
    "\n",
    "# Execution instructions\n",
    "if 'analysis_results' in locals() and analysis_results:\n",
    "    print(\"🚀 Ready to execute merges!\")\n",
    "    print(f\"📊 {len(analysis_results)} pairs available for merging\")\n",
    "    print(\"\\n📝 Instructions:\")\n",
    "    print(\"   1. Review the analysis results above carefully\")\n",
    "    print(\"   2. Uncomment and run ONE of the lines below:\")\n",
    "    print(\"   3. # merge_results = await execute_entity_merges()  # With confirmation\")\n",
    "    print(\"   4. # merge_results = await execute_entity_merges(confirm=False)  # Auto-merge\")\n",
    "    print(\"\\n⚠️  Remember: Merging permanently modifies your database!\")\n",
    "    \n",
    "    # Uncomment ONE of these lines to execute merges:\n",
    "    # merge_results = await execute_entity_merges()  # With user confirmation\n",
    "    # merge_results = await execute_entity_merges(confirm=False)  # Automatic merging\n",
    "    \n",
    "else:\n",
    "    print(\"ℹ️  No analysis results available for merging\")\n",
    "    print(\"💡 Run the similarity analysis step first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffeddfcd",
   "metadata": {},
   "source": [
    "## 9.3. Test Different Configurations\n",
    "\n",
    "Try different similarity thresholds and configurations to see how they affect the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cbab45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different configurations without merging\n",
    "async def compare_configurations():\n",
    "    \"\"\"Compare results from different TF-IDF configurations.\"\"\"\n",
    "    \n",
    "    configs_to_test = [\n",
    "        (\"Conservative\", conservative_config),\n",
    "        (\"Moderate\", moderate_config), \n",
    "        (\"Aggressive\", aggressive_config),\n",
    "        (\"Actor Only\", actor_only_config),\n",
    "        (\"Event Only\", event_only_config)\n",
    "    ]\n",
    "    \n",
    "    print(\"🔬 Comparing different TF-IDF configurations...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for config_name, config in configs_to_test:\n",
    "        print(f\"\\n📊 Testing {config_name} Configuration:\")\n",
    "        print(f\"  Threshold: {config['similarity_threshold']}\")\n",
    "        print(f\"  Properties: {config['resolve_properties']}\")\n",
    "        print(f\"  Filter: {config['filter_query'][:50]}{'...' if len(config['filter_query']) > 50 else ''}\")\n",
    "        \n",
    "        try:\n",
    "            pairs, _ = await analyze_similarity_pairs(\n",
    "                use_alternative_config=config, \n",
    "                max_pairs_to_show=3  # Show fewer pairs for comparison\n",
    "            )\n",
    "            \n",
    "            if pairs:\n",
    "                print(f\"  → Would merge {len(pairs)} pairs\")\n",
    "                avg_similarity = sum(score for _, _, score in pairs) / len(pairs)\n",
    "                print(f\"  → Average similarity: {avg_similarity:.4f}\")\n",
    "            else:\n",
    "                print(f\"  → No pairs found above threshold\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  → Error: {e}\")\n",
    "        \n",
    "        print(\"-\" * 40)\n",
    "    \n",
    "    print(\"\\n✅ Configuration comparison complete!\")\n",
    "\n",
    "# Uncomment to run configuration comparison\n",
    "# await compare_configurations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc407fb",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides a step-by-step breakdown of the TF-IDF entity resolution script:\n",
    "\n",
    "1. **Import Libraries**: All required dependencies for the resolution process\n",
    "2. **Path Setup**: Configure paths for the notebook environment\n",
    "3. **Class Definition**: The main TFIDFMatchResolver class structure\n",
    "4. **Method Implementation**: Core methods for node fetching, similarity calculation, and merging\n",
    "5. **Main Function**: Complete workflow orchestration\n",
    "6. **Configuration Testing**: Verify config files and environment variables\n",
    "7. **Connection Testing**: Validate Neo4j connectivity\n",
    "8. **Full Execution**: Run the complete resolution process\n",
    "9. **Debugging Tools**: Individual component testing utilities\n",
    "\n",
    "### Usage Instructions:\n",
    "1. Run cells 1-5 to set up the environment and define all functions\n",
    "2. Run cell 6 to test configuration loading\n",
    "3. Run cell 7 to test Neo4j connection\n",
    "4. If all tests pass, run cell 8 to execute the full resolution\n",
    "5. Use cell 9 for debugging specific issues\n",
    "\n",
    "### Troubleshooting:\n",
    "- If configuration loading fails, check that config files exist in the expected paths\n",
    "- If Neo4j connection fails, verify credentials and server availability\n",
    "- If node fetching fails, check the filter query and resolve properties in the config\n",
    "- Use the debugging functions to isolate specific issues"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
