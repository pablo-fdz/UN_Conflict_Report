{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35ae0c9b",
   "metadata": {},
   "source": [
    "# 0. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2a8341e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory (graphrag_pipeline) to the Python path (needed for importing\n",
    "# modules in parent directory)\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "# Utilities\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "from google import genai\n",
    "import polars as pl\n",
    "from library.kg_builder import CustomKGPipeline, build_kg_from_df\n",
    "from library.kg_builder.utilities import GeminiLLM\n",
    "from neo4j_graphrag.experimental.components.resolver import (\n",
    "    SpaCySemanticMatchResolver, FuzzyMatchResolver, SinglePropertyExactMatchResolver\n",
    ")\n",
    "\n",
    "from neo4j_graphrag.embeddings import SentenceTransformerEmbeddings\n",
    "import neo4j\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "# Import for embedding model (if needed for entity similarity)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import tqdm.notebook as tqdm\n",
    "\n",
    "# Neo4j and Neo4j GraphRAG imports\n",
    "import neo4j\n",
    "from neo4j_graphrag.embeddings import SentenceTransformerEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5182b7fc",
   "metadata": {},
   "source": [
    "Let's first check the available Gemini models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03b114e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv('.env', override=True)\n",
    "\n",
    "gemini_api_key = os.getenv('GEMINI_API_KEY')\n",
    "\n",
    "if gemini_api_key:\n",
    "    client = genai.Client(api_key=gemini_api_key)  # Configure the API key for genai\n",
    "else:\n",
    "    raise ValueError(\"GEMINI_API_KEY environment variable is not set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ad2805",
   "metadata": {},
   "source": [
    "We also have to make sure that the corresponding SpaCy model for text embedding used at the resolving step is installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43270dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 'en_core_web_lg' is already installed.\n"
     ]
    }
   ],
   "source": [
    "import importlib.util\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import spacy\n",
    "\n",
    "\n",
    "def ensure_spacy_model(model_name):\n",
    "    if importlib.util.find_spec(model_name) is None:\n",
    "        print(f\"Model '{model_name}' not found. Installing...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", model_name])\n",
    "    else:\n",
    "        print(f\"Model '{model_name}' is already installed.\")\n",
    "\n",
    "# Use it for 'en_core_web_lg'\n",
    "ensure_spacy_model(\"en_core_web_lg\")  # Model used for resolving entities in the KG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a20a0a27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'use_default': False,\n",
       " 'template': \"For each row in the DataFrame, create a node with the label 'Location'. Do not create nodes for Document and Text chunk. The node should have three properties: 'name', 'admin 1', and 'country'. These should be populated with the values from the DataFrame columns 'Name', 'Admin1', and 'Country' respectively.\"}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_files_path = os.path.join(os.path.dirname(os.getcwd()), 'config_files')\n",
    "\n",
    "with open(os.path.join(config_files_path, 'kg_building_config_loc.json'), 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "config['prompt_template_config']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e39afcd",
   "metadata": {},
   "source": [
    "# 1. Loading the data\n",
    "\n",
    "The data is loaded here as a reference, but it is loaded again inside the pipeline below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4320d1",
   "metadata": {},
   "source": [
    "## 1.2. Factal sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d29a4e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 7)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>title</th><th>google_link</th><th>source</th><th>id</th><th>date</th><th>decoded_url</th><th>full_text</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>date</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;Healthcare System Collapses in…</td><td>&quot;https://news.google.com/rss/ar…</td><td>&quot;NPR&quot;</td><td>&quot;GN_SUD12068&quot;</td><td>2025-05-28</td><td>&quot;https://www.npr.org/2025/05/28…</td><td>&quot;Healthcare System Collapses in…</td></tr><tr><td>&quot;US imposes sanctions against t…</td><td>&quot;https://news.google.com/rss/ar…</td><td>&quot;EU Reporter&quot;</td><td>&quot;GN_SUD15276&quot;</td><td>2025-05-28</td><td>&quot;https://www.eureporter.co/worl…</td><td>&quot;Sudan\n",
       "US imposes sanctions aga…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 7)\n",
       "┌──────────────┬──────────────┬─────────────┬─────────────┬────────────┬─────────────┬─────────────┐\n",
       "│ title        ┆ google_link  ┆ source      ┆ id          ┆ date       ┆ decoded_url ┆ full_text   │\n",
       "│ ---          ┆ ---          ┆ ---         ┆ ---         ┆ ---        ┆ ---         ┆ ---         │\n",
       "│ str          ┆ str          ┆ str         ┆ str         ┆ date       ┆ str         ┆ str         │\n",
       "╞══════════════╪══════════════╪═════════════╪═════════════╪════════════╪═════════════╪═════════════╡\n",
       "│ Healthcare   ┆ https://news ┆ NPR         ┆ GN_SUD12068 ┆ 2025-05-28 ┆ https://www ┆ Healthcare  │\n",
       "│ System       ┆ .google.com/ ┆             ┆             ┆            ┆ .npr.org/20 ┆ System      │\n",
       "│ Collapses    ┆ rss/ar…      ┆             ┆             ┆            ┆ 25/05/28…   ┆ Collapses   │\n",
       "│ in…          ┆              ┆             ┆             ┆            ┆             ┆ in…         │\n",
       "│ US imposes   ┆ https://news ┆ EU Reporter ┆ GN_SUD15276 ┆ 2025-05-28 ┆ https://www ┆ Sudan       │\n",
       "│ sanctions    ┆ .google.com/ ┆             ┆             ┆            ┆ .eureporter ┆ US imposes  │\n",
       "│ against t…   ┆ rss/ar…      ┆             ┆             ┆            ┆ .co/worl…   ┆ sanctions   │\n",
       "│              ┆              ┆             ┆             ┆            ┆             ┆ aga…        │\n",
       "└──────────────┴──────────────┴─────────────┴─────────────┴────────────┴─────────────┴─────────────┘"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = r'C:\\Users\\blanc\\OneDrive\\Desktop\\DSDM\\3-TFM\\repo\\UN_Conflict_Report\\graphrag_pipeline\\data\\google_news\\google_news_Sudan_2025-05-01_2025-06-01.parquet'\n",
    "df1 = pl.read_parquet(path)\n",
    "df1=df1.head(20)\n",
    "\n",
    "# # Create an index for each row\n",
    "# df1 = df1.with_row_index(name=\"id\", offset=1)\n",
    "# # Convert the \"id\" to a string to ensure it is treated as a document ID\n",
    "# df1 = df1.with_columns(pl.col('id').cast(pl.String))\n",
    "    \n",
    "df1.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c640450",
   "metadata": {},
   "source": [
    "### Load Admin1 locations from HDX database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0646aee4",
   "metadata": {},
   "source": [
    "# 2. Running the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e872a9",
   "metadata": {},
   "source": [
    "## 2.2. With a data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c023659",
   "metadata": {},
   "source": [
    "### A. Using the `SpaCySemanticMatchResolver`\n",
    "\n",
    "More useful information about the resolvers can be found in the [user guide](https://neo4j.com/docs/neo4j-graphrag-python/current/user_guide_kg_builder.html#entity-resolver). Below, we use different resolvers (from the most aggressive - spaCy to the most conservative - exact matching) to get a broad overview of the performance results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d30b0b9",
   "metadata": {},
   "source": [
    "### Embed Locations from list of Admin units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7af0861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neo4j+s://a89fb0ac.databases.neo4j.io\n",
      "neo4j\n",
      "r53cBG8rAv04u26gYyvFdtVJ9QEHIFwjhQARRLqp3wc\n"
     ]
    }
   ],
   "source": [
    "neo4j_uri = 'neo4j+s://a89fb0ac.databases.neo4j.io' # os.getenv('NEO4J_URI')\n",
    "neo4j_username = os.getenv('NEO4J_USERNAME')\n",
    "neo4j_password = 'r53cBG8rAv04u26gYyvFdtVJ9QEHIFwjhQARRLqp3wc' # os.getenv('NEO4J_PASSWORD')\n",
    "gemini_api_key = os.getenv('GEMINI_API_KEY')\n",
    "\n",
    "print(neo4j_uri)\n",
    "print(neo4j_username)\n",
    "print(neo4j_password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5bff7dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing the first dataframe...\n",
      "Processing row 1 of 20\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'\"nodes\"'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 107\u001b[39m\n\u001b[32m    104\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m all_results\n\u001b[32m    106\u001b[39m \u001b[38;5;66;03m# Asyncio event loop to run the main function in a Jupyter notebook\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m all_results = \u001b[38;5;28;01mawait\u001b[39;00m main()\n\u001b[32m    108\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProcessed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(all_results)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m documents\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    110\u001b[39m \u001b[38;5;66;03m# # Asyncio event loop to run the main function in a script\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[38;5;66;03m# if __name__ == \"__main__\":\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[38;5;66;03m#     results = asyncio.run(main())\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[38;5;66;03m#     print(f\"Processed {len(results)} documents\")\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 82\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     80\u001b[39m     \u001b[38;5;66;03m# Process the First dataframe\u001b[39;00m\n\u001b[32m     81\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mProcessing the first dataframe...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     results_df1 = \u001b[38;5;28;01mawait\u001b[39;00m build_kg_from_df(\n\u001b[32m     83\u001b[39m         kg_pipeline=kg_pipeline,\n\u001b[32m     84\u001b[39m         df=df1,\n\u001b[32m     85\u001b[39m         document_base_field=\u001b[33m'\u001b[39m\u001b[33mid\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     86\u001b[39m         text_column=\u001b[33m'\u001b[39m\u001b[33mfull_text\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     87\u001b[39m         document_metadata_mapping=metadata_mapping1,\n\u001b[32m     88\u001b[39m         document_id_column=\u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# Use default document ID generation\u001b[39;00m\n\u001b[32m     89\u001b[39m     )\n\u001b[32m     90\u001b[39m     all_results.extend(results_df1)\n\u001b[32m     92\u001b[39m \u001b[38;5;66;03m#     # Process the Second dataframe\u001b[39;00m\n\u001b[32m     93\u001b[39m \u001b[38;5;66;03m#     print(\"Processing the second dataframe...\")\u001b[39;00m\n\u001b[32m     94\u001b[39m \u001b[38;5;66;03m#     results_df2 = await build_kg_from_df(\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    101\u001b[39m \u001b[38;5;66;03m#     )\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;66;03m#     all_results.extend(results_df2)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\blanc\\OneDrive\\Desktop\\DSDM\\3-TFM\\repo\\UN_Conflict_Report\\graphrag_pipeline\\library\\kg_builder\\build_kg_from_df.py:72\u001b[39m, in \u001b[36mbuild_kg_from_df\u001b[39m\u001b[34m(kg_pipeline, df, document_base_field, text_column, document_metadata_mapping, document_id_column)\u001b[39m\n\u001b[32m     69\u001b[39m doc_id = row.get(document_id_column) \u001b[38;5;28;01mif\u001b[39;00m document_id_column \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(uuid.uuid4())\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# Process the text with the pipeline\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m kg_pipeline.run_async(\n\u001b[32m     73\u001b[39m     text=text,\n\u001b[32m     74\u001b[39m     document_base_field=doc_base_field,\n\u001b[32m     75\u001b[39m     document_metadata=processed_metadata, \n\u001b[32m     76\u001b[39m     document_id=doc_id\n\u001b[32m     77\u001b[39m )\n\u001b[32m     78\u001b[39m results.append(result)\n\u001b[32m     79\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mResult: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\blanc\\OneDrive\\Desktop\\DSDM\\3-TFM\\repo\\UN_Conflict_Report\\graphrag_pipeline\\library\\kg_builder\\custom_kg_pipeline.py:209\u001b[39m, in \u001b[36mCustomKGPipeline.run_async\u001b[39m\u001b[34m(self, text, document_base_field, document_metadata, document_id)\u001b[39m\n\u001b[32m    201\u001b[39m pipe_inputs[\u001b[33m\"\u001b[39m\u001b[33mextractor\u001b[39m\u001b[33m\"\u001b[39m] = {  \u001b[38;5;66;03m# Define additional inputs for the entity relation extractor component\u001b[39;00m\n\u001b[32m    202\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mexamples\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.examples,  \u001b[38;5;66;03m# Examples for few-shot learning in the prompt\u001b[39;00m\n\u001b[32m    203\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mlexical_graph_config\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.lexical_graph_config,  \u001b[38;5;66;03m# Lexical graph configuration. Oddly enough, this must be included even if we are not creating a lexical graph in the extractor component, but this is needed to link the lexical graph created in the LexicalGraphBuilder component to the entity graph created in the LLMEntityRelationExtractor component.\u001b[39;00m\n\u001b[32m    204\u001b[39m     }\n\u001b[32m    206\u001b[39m \u001b[38;5;66;03m# Run the pipeline asynchronously (.run() method of the Pipeline class)\u001b[39;00m\n\u001b[32m    207\u001b[39m \u001b[38;5;66;03m# already handles the execution of all components in the pipeline in\u001b[39;00m\n\u001b[32m    208\u001b[39m \u001b[38;5;66;03m# an asynchronous manner, so we don't need to worry about that.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m pipe.run(pipe_inputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\blanc\\miniconda3\\envs\\tfm\\Lib\\site-packages\\neo4j_graphrag\\experimental\\pipeline\\pipeline.py:573\u001b[39m, in \u001b[36mPipeline.run\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m    571\u001b[39m orchestrator = Orchestrator(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    572\u001b[39m logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPIPELINE ORCHESTRATOR: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00morchestrator.run_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m573\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m orchestrator.run(data)\n\u001b[32m    574\u001b[39m end_time = default_timer()\n\u001b[32m    575\u001b[39m logger.debug(\n\u001b[32m    576\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPIPELINE FINISHED \u001b[39m\u001b[38;5;132;01m{\u001b[39;00morchestrator.run_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_time\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    577\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\blanc\\miniconda3\\envs\\tfm\\Lib\\site-packages\\neo4j_graphrag\\experimental\\pipeline\\orchestrator.py:270\u001b[39m, in \u001b[36mOrchestrator.run\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.event_notifier.notify_pipeline_started(\u001b[38;5;28mself\u001b[39m.run_id, data)\n\u001b[32m    269\u001b[39m tasks = [\u001b[38;5;28mself\u001b[39m.run_task(root, data) \u001b[38;5;28;01mfor\u001b[39;00m root \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pipeline.roots()]\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(*tasks)\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.event_notifier.notify_pipeline_finished(\n\u001b[32m    272\u001b[39m     \u001b[38;5;28mself\u001b[39m.run_id, \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pipeline.get_final_results(\u001b[38;5;28mself\u001b[39m.run_id)\n\u001b[32m    273\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\blanc\\miniconda3\\envs\\tfm\\Lib\\site-packages\\neo4j_graphrag\\experimental\\pipeline\\orchestrator.py:93\u001b[39m, in \u001b[36mOrchestrator.run_task\u001b[39m\u001b[34m(self, task, data)\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.event_notifier.notify_task_finished(\u001b[38;5;28mself\u001b[39m.run_id, task.name, res)\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m res:\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.on_task_complete(data=data, task=task, result=res)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\blanc\\miniconda3\\envs\\tfm\\Lib\\site-packages\\neo4j_graphrag\\experimental\\pipeline\\orchestrator.py:134\u001b[39m, in \u001b[36mOrchestrator.on_task_complete\u001b[39m\u001b[34m(self, data, task, result)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.add_result_for_component(\n\u001b[32m    130\u001b[39m     task.name, res_to_save, is_final=task.is_leaf()\n\u001b[32m    131\u001b[39m )\n\u001b[32m    132\u001b[39m \u001b[38;5;66;03m# then get the next tasks to be executed\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[38;5;66;03m# and run them in //\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(*[\u001b[38;5;28mself\u001b[39m.run_task(n, data) \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.next(task)])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\blanc\\miniconda3\\envs\\tfm\\Lib\\site-packages\\neo4j_graphrag\\experimental\\pipeline\\orchestrator.py:93\u001b[39m, in \u001b[36mOrchestrator.run_task\u001b[39m\u001b[34m(self, task, data)\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.event_notifier.notify_task_finished(\u001b[38;5;28mself\u001b[39m.run_id, task.name, res)\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m res:\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.on_task_complete(data=data, task=task, result=res)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\blanc\\miniconda3\\envs\\tfm\\Lib\\site-packages\\neo4j_graphrag\\experimental\\pipeline\\orchestrator.py:134\u001b[39m, in \u001b[36mOrchestrator.on_task_complete\u001b[39m\u001b[34m(self, data, task, result)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.add_result_for_component(\n\u001b[32m    130\u001b[39m     task.name, res_to_save, is_final=task.is_leaf()\n\u001b[32m    131\u001b[39m )\n\u001b[32m    132\u001b[39m \u001b[38;5;66;03m# then get the next tasks to be executed\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[38;5;66;03m# and run them in //\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(*[\u001b[38;5;28mself\u001b[39m.run_task(n, data) \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.next(task)])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\blanc\\miniconda3\\envs\\tfm\\Lib\\site-packages\\neo4j_graphrag\\experimental\\pipeline\\orchestrator.py:89\u001b[39m, in \u001b[36mOrchestrator.run_task\u001b[39m\u001b[34m(self, task, data)\u001b[39m\n\u001b[32m     83\u001b[39m notifier = partial(\n\u001b[32m     84\u001b[39m     \u001b[38;5;28mself\u001b[39m.event_notifier.notify_task_progress,\n\u001b[32m     85\u001b[39m     run_id=\u001b[38;5;28mself\u001b[39m.run_id,\n\u001b[32m     86\u001b[39m     task_name=task.name,\n\u001b[32m     87\u001b[39m )\n\u001b[32m     88\u001b[39m context = RunContext(run_id=\u001b[38;5;28mself\u001b[39m.run_id, task_name=task.name, notifier=notifier)\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m res = \u001b[38;5;28;01mawait\u001b[39;00m task.run(context, inputs)\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.set_task_status(task.name, RunStatus.DONE)\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.event_notifier.notify_task_finished(\u001b[38;5;28mself\u001b[39m.run_id, task.name, res)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\blanc\\miniconda3\\envs\\tfm\\Lib\\site-packages\\neo4j_graphrag\\experimental\\pipeline\\pipeline.py:104\u001b[39m, in \u001b[36mTaskPipelineNode.run\u001b[39m\u001b[34m(self, context, inputs)\u001b[39m\n\u001b[32m    102\u001b[39m logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTASK START \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.name\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m input=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprettify(inputs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    103\u001b[39m start_time = default_timer()\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m res = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.execute(context, inputs)\n\u001b[32m    105\u001b[39m end_time = default_timer()\n\u001b[32m    106\u001b[39m logger.debug(\n\u001b[32m    107\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTASK FINISHED \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_time\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m res=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprettify(res)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    108\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\blanc\\miniconda3\\envs\\tfm\\Lib\\site-packages\\neo4j_graphrag\\experimental\\pipeline\\pipeline.py:90\u001b[39m, in \u001b[36mTaskPipelineNode.execute\u001b[39m\u001b[34m(self, context, inputs)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mexecute\u001b[39m(\n\u001b[32m     81\u001b[39m     \u001b[38;5;28mself\u001b[39m, context: RunContext, inputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]\n\u001b[32m     82\u001b[39m ) -> RunResult | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     83\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Execute the task\u001b[39;00m\n\u001b[32m     84\u001b[39m \n\u001b[32m     85\u001b[39m \u001b[33;03m    Returns:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     88\u001b[39m \u001b[33;03m        was unsuccessful.\u001b[39;00m\n\u001b[32m     89\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m     component_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.component.run_with_context(\n\u001b[32m     91\u001b[39m         context_=context, **inputs\n\u001b[32m     92\u001b[39m     )\n\u001b[32m     93\u001b[39m     run_result = RunResult(\n\u001b[32m     94\u001b[39m         result=component_result,\n\u001b[32m     95\u001b[39m     )\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m run_result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\blanc\\miniconda3\\envs\\tfm\\Lib\\site-packages\\neo4j_graphrag\\experimental\\pipeline\\component.py:110\u001b[39m, in \u001b[36mComponent.run_with_context\u001b[39m\u001b[34m(self, context_, *args, **kwargs)\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"This method is called by the pipeline orchestrator.\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03mThe `context_` parameter contains information about\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03mthe pipeline run: the `run_id` and a `notify` function\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    107\u001b[39m \u001b[33;03mIt defaults to calling the `run` method to prevent any breaking change.\u001b[39;00m\n\u001b[32m    108\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    109\u001b[39m \u001b[38;5;66;03m# default behavior to prevent a breaking change\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.run(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\blanc\\miniconda3\\envs\\tfm\\Lib\\site-packages\\pydantic\\_internal\\_validate_call.py:34\u001b[39m, in \u001b[36mupdate_wrapper_attributes.<locals>.wrapper_function\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(wrapped)\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper_function\u001b[39m(*args, **kwargs):  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m wrapper(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\blanc\\miniconda3\\envs\\tfm\\Lib\\site-packages\\neo4j_graphrag\\experimental\\components\\entity_relation_extractor.py:341\u001b[39m, in \u001b[36mLLMEntityRelationExtractor.run\u001b[39m\u001b[34m(self, chunks, document_info, lexical_graph_config, schema, examples, **kwargs)\u001b[39m\n\u001b[32m    330\u001b[39m sem = asyncio.Semaphore(\u001b[38;5;28mself\u001b[39m.max_concurrency)\n\u001b[32m    331\u001b[39m tasks = [\n\u001b[32m    332\u001b[39m     \u001b[38;5;28mself\u001b[39m.run_for_chunk(\n\u001b[32m    333\u001b[39m         sem,\n\u001b[32m   (...)\u001b[39m\u001b[32m    339\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunks.chunks\n\u001b[32m    340\u001b[39m ]\n\u001b[32m--> \u001b[39m\u001b[32m341\u001b[39m chunk_graphs: \u001b[38;5;28mlist\u001b[39m[Neo4jGraph] = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(*tasks))\n\u001b[32m    342\u001b[39m graph = \u001b[38;5;28mself\u001b[39m.combine_chunk_graphs(lexical_graph, chunk_graphs)\n\u001b[32m    343\u001b[39m logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExtracted graph: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprettify(graph)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\blanc\\miniconda3\\envs\\tfm\\Lib\\site-packages\\neo4j_graphrag\\experimental\\components\\entity_relation_extractor.py:284\u001b[39m, in \u001b[36mLLMEntityRelationExtractor.run_for_chunk\u001b[39m\u001b[34m(self, sem, chunk, schema, examples, lexical_graph_builder)\u001b[39m\n\u001b[32m    282\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Run extraction, validation and post processing for a single chunk\"\"\"\u001b[39;00m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m sem:\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m     chunk_graph = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.extract_for_chunk(schema, examples, chunk)\n\u001b[32m    285\u001b[39m     final_chunk_graph = \u001b[38;5;28mself\u001b[39m.validate_chunk(chunk_graph, schema)\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.post_process_chunk(\n\u001b[32m    287\u001b[39m         final_chunk_graph,\n\u001b[32m    288\u001b[39m         chunk,\n\u001b[32m    289\u001b[39m         lexical_graph_builder,\n\u001b[32m    290\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\blanc\\miniconda3\\envs\\tfm\\Lib\\site-packages\\neo4j_graphrag\\experimental\\components\\entity_relation_extractor.py:215\u001b[39m, in \u001b[36mLLMEntityRelationExtractor.extract_for_chunk\u001b[39m\u001b[34m(self, schema, examples, chunk)\u001b[39m\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mextract_for_chunk\u001b[39m(\n\u001b[32m    212\u001b[39m     \u001b[38;5;28mself\u001b[39m, schema: SchemaConfig, examples: \u001b[38;5;28mstr\u001b[39m, chunk: TextChunk\n\u001b[32m    213\u001b[39m ) -> Neo4jGraph:\n\u001b[32m    214\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Run entity extraction for a given text chunk.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m     prompt = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprompt_template\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexamples\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    218\u001b[39m     llm_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.llm.ainvoke(prompt)\n\u001b[32m    219\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\blanc\\miniconda3\\envs\\tfm\\Lib\\site-packages\\neo4j_graphrag\\generation\\prompts.py:92\u001b[39m, in \u001b[36mPromptTemplate.format\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     90\u001b[39m data = kwargs\n\u001b[32m     91\u001b[39m data.update({k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m.expected_inputs, args)})\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\blanc\\miniconda3\\envs\\tfm\\Lib\\site-packages\\neo4j_graphrag\\generation\\prompts.py:61\u001b[39m, in \u001b[36mPromptTemplate._format\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m     59\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m e \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[32m     60\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m PromptMissingInputError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing input \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyError\u001b[39m: '\"nodes\"'"
     ]
    }
   ],
   "source": [
    "# Example usage code\n",
    "async def main():\n",
    "\n",
    "    global df1\n",
    "    \n",
    "    # Find path to config_files folder\n",
    "    config_files_path = os.path.join(os.path.dirname(os.getcwd()), 'config_files')\n",
    "\n",
    "    # Load environment variables from .env file\n",
    "    load_dotenv(os.path.join(config_files_path, '.env'), override=True)\n",
    "    \n",
    "    with open(os.path.join(config_files_path, 'kg_building_config2.json'), 'r') as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    # Neo4j connection\n",
    "    neo4j_uri = 'neo4j+s://a89fb0ac.databases.neo4j.io' # os.getenv('NEO4J_URI')\n",
    "    neo4j_username = os.getenv('NEO4J_USERNAME')\n",
    "    neo4j_password = 'r53cBG8rAv04u26gYyvFdtVJ9QEHIFwjhQARRLqp3wc' # os.getenv('NEO4J_PASSWORD')\n",
    "    gemini_api_key = os.getenv('GEMINI_API_KEY')\n",
    "\n",
    "    # Check if gemini_api_key is set\n",
    "    if gemini_api_key:\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(\"Gemini API key is not set. Please provide a valid API key.\")\n",
    "\n",
    "    # Initialize LLM\n",
    "    llm = GeminiLLM(\n",
    "        model_name=config['llm_config']['model_name'],\n",
    "        google_api_key=gemini_api_key,\n",
    "        model_params=config['llm_config']['model_params']\n",
    "    )\n",
    "    \n",
    "    # Initialize embedder\n",
    "    embedder = SentenceTransformerEmbeddings(model=config['embedder_config']['model_name'])\n",
    "    \n",
    "    # Configure text splitter\n",
    "    text_splitter_config = config['text_splitter_config'] \n",
    "    \n",
    "    # Create the pipeline - use with statement to ensure proper resource management\n",
    "    # and to ensure the driver is closed after use\n",
    "    all_results = []\n",
    "    with neo4j.GraphDatabase.driver(neo4j_uri, auth=(neo4j_username, neo4j_password)) as driver:\n",
    "        \n",
    "        # Initialize entity resolver\n",
    "        resolver = SpaCySemanticMatchResolver(  # Merge nodes with same label and similar textual properties\n",
    "            driver,\n",
    "            filter_query=None,  # \"WHERE (entity)-[:FROM_CHUNK]->(:Chunk)-[:FROM_DOCUMENT]->(doc:Document {id = 'docId'}\",  # Used to reduce the resolution scope to a specific document\n",
    "            resolve_properties=[\"name\"],  # Properties to use for resolution (default is \"name\")\n",
    "            similarity_threshold=0.8,  # The similarity threshold above which nodes are merged (default is 0.8). Higher threshold will result in less false positives, but may miss some matches. \n",
    "            spacy_model=\"en_core_web_lg\"  # spaCy model to use for resolution (default is \"en_core_web_lg\")\n",
    "        )\n",
    "\n",
    "        # Initialize the custom KG pipeline\n",
    "        kg_pipeline = CustomKGPipeline(\n",
    "            llm=llm,\n",
    "            driver=driver,\n",
    "            embedder=embedder,\n",
    "            schema_config=config['schema_config'],\n",
    "            prompt_template=config['prompt_template_config']['template'] if config['prompt_template_config'].get('use_default') == False else None,\n",
    "            text_splitter_config=text_splitter_config,\n",
    "            resolver=resolver,\n",
    "            examples_config=None,  # Use None if no examples are provided\n",
    "            on_error='RAISE',\n",
    "            batch_size=1000,\n",
    "            max_concurrency=5\n",
    "        )\n",
    "        \n",
    "        # Define metadata mapping (document properties additional to base field \n",
    "        # to dataframe columns)\n",
    "        metadata_mapping1 = {\n",
    "            \"source\": \"source\",\n",
    "            \"published_date\": \"date\"\n",
    "        }\n",
    "        \n",
    "        if df1['date'].dtype == pl.Date:\n",
    "            # Convert date column to string if it is of type Date\n",
    "            df1 = df1.with_columns(pl.col('date').cast(pl.String))\n",
    "        \n",
    "        # Process the First dataframe\n",
    "        print(\"Processing the first dataframe...\")\n",
    "        results_df1 = await build_kg_from_df(\n",
    "            kg_pipeline=kg_pipeline,\n",
    "            df=df1,\n",
    "            document_base_field='id',\n",
    "            text_column='full_text',\n",
    "            document_metadata_mapping=metadata_mapping1,\n",
    "            document_id_column=None  # Use default document ID generation\n",
    "        )\n",
    "        all_results.extend(results_df1)\n",
    "\n",
    "    #     # Process the Second dataframe\n",
    "    #     print(\"Processing the second dataframe...\")\n",
    "    #     results_df2 = await build_kg_from_df(\n",
    "    #         kg_pipeline=kg_pipeline,\n",
    "    #         df=df2,\n",
    "    #         document_base_field='id',\n",
    "    #         text_column='Name',\n",
    "    #         document_metadata_mapping=metadata_mapping2,\n",
    "    #         document_id_column=None  # Use default document ID generation\n",
    "    #     )\n",
    "    #     all_results.extend(results_df2)\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Asyncio event loop to run the main function in a Jupyter notebook\n",
    "all_results = await main()\n",
    "print(f\"Processed {len(all_results)} documents\")\n",
    "\n",
    "# # Asyncio event loop to run the main function in a script\n",
    "# if __name__ == \"__main__\":\n",
    "#     results = asyncio.run(main())\n",
    "#     print(f\"Processed {len(results)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67cb35f",
   "metadata": {},
   "source": [
    "# Entity Resolution Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7527a8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APOC is available.\n",
      "Found 206 entities to process\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deb0f13c7bbd497a8a737586e59c8200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing similarity scores:   0%|          | 0/20805 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "BufferError",
     "evalue": "Existing exports of data: object cannot be re-sized",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 92\u001b[39m, in \u001b[36mfind_similar_entities\u001b[39m\u001b[34m(threshold)\u001b[39m\n\u001b[32m     91\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m pair \u001b[38;5;129;01min\u001b[39;00m tqdm.tqdm(similar_pairs, desc=\u001b[33m\"\u001b[39m\u001b[33mComputing similarity scores\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m         \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpair\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m similar_pairs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j\\_sync\\work\\session.py:328\u001b[39m, in \u001b[36mSession.run\u001b[39m\u001b[34m(self, query, parameters, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m parameters = \u001b[38;5;28mdict\u001b[39m(parameters \u001b[38;5;129;01mor\u001b[39;00m {}, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_auto_result\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimpersonated_user\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdefault_access_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbookmarks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnotifications_min_severity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnotifications_disabled_classifications\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._auto_result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j\\_sync\\work\\result.py:236\u001b[39m, in \u001b[36mResult._run\u001b[39m\u001b[34m(self, query, parameters, db, imp_user, access_mode, bookmarks, notifications_min_severity, notifications_disabled_classifications)\u001b[39m\n\u001b[32m    235\u001b[39m \u001b[38;5;28mself\u001b[39m._connection.send_all()\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_attach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j\\_sync\\work\\result.py:430\u001b[39m, in \u001b[36mResult._attach\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._attached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j\\_sync\\io\\_common.py:184\u001b[39m, in \u001b[36mConnectionErrorHandler.__getattr__.<locals>.outer.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    183\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m     \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (Neo4jError, ServiceUnavailable, SessionExpired) \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j\\_sync\\io\\_bolt.py:861\u001b[39m, in \u001b[36mBolt.fetch_message\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    860\u001b[39m \u001b[38;5;66;03m# Receive exactly one message\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m861\u001b[39m tag, fields = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minbox\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    862\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhydration_hooks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresponses\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhydration_hooks\u001b[49m\n\u001b[32m    863\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    864\u001b[39m res = \u001b[38;5;28mself\u001b[39m._process_message(tag, fields)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j\\_sync\\io\\_common.py:77\u001b[39m, in \u001b[36mInbox.pop\u001b[39m\u001b[34m(self, hydration_hooks)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpop\u001b[39m(\u001b[38;5;28mself\u001b[39m, hydration_hooks):\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_buffer_one_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j\\_sync\\io\\_common.py:54\u001b[39m, in \u001b[36mInbox._buffer_one_chunk\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m chunk_size == \u001b[32m0\u001b[39m:\n\u001b[32m     53\u001b[39m     \u001b[38;5;66;03m# Determine the chunk size and skip noop\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     \u001b[43mreceive_into_buffer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_socket\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     chunk_size = \u001b[38;5;28mself\u001b[39m._buffer.pop_u16()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j\\_sync\\io\\_common.py:345\u001b[39m, in \u001b[36mreceive_into_buffer\u001b[39m\u001b[34m(sock, buffer, n_bytes)\u001b[39m\n\u001b[32m    344\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m buffer.used < end:\n\u001b[32m--> \u001b[39m\u001b[32m345\u001b[39m     n = \u001b[43msock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m        \u001b[49m\u001b[43mview\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mused\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mused\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    348\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j\\_async_compat\\network\\_bolt_socket.py:364\u001b[39m, in \u001b[36mBoltSocketBase.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes)\u001b[39m\n\u001b[32m    363\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrecv_into\u001b[39m(\u001b[38;5;28mself\u001b[39m, buffer, nbytes):\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wait_for_io\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_socket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j\\_async_compat\\network\\_bolt_socket.py:339\u001b[39m, in \u001b[36mBoltSocketBase._wait_for_io\u001b[39m\u001b[34m(self, func, *args, **kwargs)\u001b[39m\n\u001b[32m    338\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._deadline \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m339\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    340\u001b[39m timeout = \u001b[38;5;28mself\u001b[39m._socket.gettimeout()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\ssl.py:1252\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1249\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1250\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1251\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1252\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1253\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\ssl.py:1104\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1104\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1105\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mBufferError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j\\_sync\\work\\session.py:222\u001b[39m, in \u001b[36mSession.close\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    221\u001b[39m \u001b[38;5;28mself\u001b[39m._connection.send_all()\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[38;5;66;03m# TODO: Investigate potential non graceful close states\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j\\_sync\\io\\_bolt.py:879\u001b[39m, in \u001b[36mBolt.fetch_all\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    878\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m response.complete:\n\u001b[32m--> \u001b[39m\u001b[32m879\u001b[39m     detail_delta, summary_delta = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfetch_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     detail_count += detail_delta\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j\\_sync\\io\\_bolt.py:861\u001b[39m, in \u001b[36mBolt.fetch_message\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    860\u001b[39m \u001b[38;5;66;03m# Receive exactly one message\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m861\u001b[39m tag, fields = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minbox\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    862\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhydration_hooks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresponses\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhydration_hooks\u001b[49m\n\u001b[32m    863\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    864\u001b[39m res = \u001b[38;5;28mself\u001b[39m._process_message(tag, fields)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j\\_sync\\io\\_common.py:77\u001b[39m, in \u001b[36mInbox.pop\u001b[39m\u001b[34m(self, hydration_hooks)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpop\u001b[39m(\u001b[38;5;28mself\u001b[39m, hydration_hooks):\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_buffer_one_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j\\_sync\\io\\_common.py:59\u001b[39m, in \u001b[36mInbox._buffer_one_chunk\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     57\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33m[#\u001b[39m\u001b[38;5;132;01m%04X\u001b[39;00m\u001b[33m]  S: <NOOP>\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m._local_port)\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[43mreceive_into_buffer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_socket\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\n\u001b[32m     61\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m chunk_size = \u001b[38;5;28mself\u001b[39m._buffer.pop_u16()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j\\_sync\\io\\_common.py:342\u001b[39m, in \u001b[36mreceive_into_buffer\u001b[39m\u001b[34m(sock, buffer, n_bytes)\u001b[39m\n\u001b[32m    341\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m end > \u001b[38;5;28mlen\u001b[39m(buffer.data):\n\u001b[32m--> \u001b[39m\u001b[32m342\u001b[39m     \u001b[43mbuffer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbytearray\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mend\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(buffer.data) \u001b[38;5;28;01mas\u001b[39;00m view:\n",
      "\u001b[31mBufferError\u001b[39m: Existing exports of data: object cannot be re-sized",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mBufferError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 108\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;66;03m# === MAIN EXECUTION ===\u001b[39;00m\n\u001b[32m    107\u001b[39m check_apoc()\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m pairs = \u001b[43mfind_similar_entities\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m pairs = \u001b[38;5;28msorted\u001b[39m(pairs, key=\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[33m\"\u001b[39m\u001b[33msimilarity\u001b[39m\u001b[33m\"\u001b[39m], reverse=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    110\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(pairs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m similar entity pairs.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 90\u001b[39m, in \u001b[36mfind_similar_entities\u001b[39m\u001b[34m(threshold)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m# Create SAME_AS relationships\u001b[39;00m\n\u001b[32m     85\u001b[39m query = \u001b[33m\"\"\"\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[33mMATCH (a), (b)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[33mWHERE elementId(a) = $id1 AND elementId(b) = $id2\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[33mMERGE (a)-[:SAME_AS \u001b[39m\u001b[33m{\u001b[39m\u001b[33msimilarity: $similarity}]->(b)\u001b[39m\n\u001b[32m     89\u001b[39m \u001b[33m\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m.\u001b[49m\u001b[43msession\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msession\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpair\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43msimilar_pairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mComputing similarity scores\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpair\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j\\_sync\\work\\session.py:130\u001b[39m, in \u001b[36mSession.__exit__\u001b[39m\u001b[34m(self, exception_type, exception_value, traceback)\u001b[39m\n\u001b[32m    128\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    129\u001b[39m     \u001b[38;5;28mself\u001b[39m._state_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j\\_sync\\work\\session.py:233\u001b[39m, in \u001b[36mSession.close\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    231\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    232\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m233\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_disconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m     \u001b[38;5;28mself\u001b[39m._state_failed = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    236\u001b[39m \u001b[38;5;28mself\u001b[39m._closed = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j\\_sync\\work\\session.py:145\u001b[39m, in \u001b[36mSession._disconnect\u001b[39m\u001b[34m(self, sync)\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_disconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m, sync=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    144\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_disconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msync\u001b[49m\u001b[43m=\u001b[49m\u001b[43msync\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m asyncio.CancelledError:\n\u001b[32m    147\u001b[39m         \u001b[38;5;28mself\u001b[39m._handle_cancellation(message=\u001b[33m\"\u001b[39m\u001b[33m_disconnect\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j\\_sync\\work\\workspace.py:298\u001b[39m, in \u001b[36mWorkspace._disconnect\u001b[39m\u001b[34m(self, sync)\u001b[39m\n\u001b[32m    296\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    297\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection:\n\u001b[32m--> \u001b[39m\u001b[32m298\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrelease\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    299\u001b[39m     \u001b[38;5;28mself\u001b[39m._connection = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    300\u001b[39m \u001b[38;5;28mself\u001b[39m._connection_access_mode = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j\\_sync\\io\\_pool.py:481\u001b[39m, in \u001b[36mIOPool.release\u001b[39m\u001b[34m(self, *connections)\u001b[39m\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    476\u001b[39m     log.debug(\n\u001b[32m    477\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m[#\u001b[39m\u001b[38;5;132;01m%04X\u001b[39;00m\u001b[33m]  _: <POOL> release unclean connection \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    478\u001b[39m         connection.local_port,\n\u001b[32m    479\u001b[39m         connection.connection_id,\n\u001b[32m    480\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m481\u001b[39m     \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (Neo4jError, DriverError, BoltError) \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    483\u001b[39m     log.debug(\n\u001b[32m    484\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m[#\u001b[39m\u001b[38;5;132;01m%04X\u001b[39;00m\u001b[33m]  _: <POOL> failed to reset connection \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    485\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mon release: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    486\u001b[39m         connection.local_port,\n\u001b[32m    487\u001b[39m         exc,\n\u001b[32m    488\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j\\_sync\\io\\_bolt5.py:447\u001b[39m, in \u001b[36mBolt5x0.reset\u001b[39m\u001b[34m(self, dehydration_hooks, hydration_hooks)\u001b[39m\n\u001b[32m    443\u001b[39m \u001b[38;5;28mself\u001b[39m._append(\n\u001b[32m    444\u001b[39m     \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\x0f\u001b[39;00m\u001b[33m\"\u001b[39m, response=response, dehydration_hooks=dehydration_hooks\n\u001b[32m    445\u001b[39m )\n\u001b[32m    446\u001b[39m \u001b[38;5;28mself\u001b[39m.send_all()\n\u001b[32m--> \u001b[39m\u001b[32m447\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfetch_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j\\_sync\\io\\_bolt.py:879\u001b[39m, in \u001b[36mBolt.fetch_all\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    877\u001b[39m response = \u001b[38;5;28mself\u001b[39m.responses[\u001b[32m0\u001b[39m]\n\u001b[32m    878\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m response.complete:\n\u001b[32m--> \u001b[39m\u001b[32m879\u001b[39m     detail_delta, summary_delta = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfetch_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     detail_count += detail_delta\n\u001b[32m    881\u001b[39m     summary_count += summary_delta\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j\\_sync\\io\\_bolt.py:861\u001b[39m, in \u001b[36mBolt.fetch_message\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    858\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m\n\u001b[32m    860\u001b[39m \u001b[38;5;66;03m# Receive exactly one message\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m861\u001b[39m tag, fields = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minbox\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    862\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhydration_hooks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresponses\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhydration_hooks\u001b[49m\n\u001b[32m    863\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    864\u001b[39m res = \u001b[38;5;28mself\u001b[39m._process_message(tag, fields)\n\u001b[32m    865\u001b[39m \u001b[38;5;28mself\u001b[39m.idle_since = monotonic()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j\\_sync\\io\\_common.py:77\u001b[39m, in \u001b[36mInbox.pop\u001b[39m\u001b[34m(self, hydration_hooks)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpop\u001b[39m(\u001b[38;5;28mself\u001b[39m, hydration_hooks):\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_buffer_one_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     79\u001b[39m         size, tag = \u001b[38;5;28mself\u001b[39m._unpacker.unpack_structure_header()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j\\_sync\\io\\_common.py:59\u001b[39m, in \u001b[36mInbox._buffer_one_chunk\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m chunk_size == \u001b[32m0\u001b[39m:\n\u001b[32m     57\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33m[#\u001b[39m\u001b[38;5;132;01m%04X\u001b[39;00m\u001b[33m]  S: <NOOP>\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m._local_port)\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[43mreceive_into_buffer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_socket\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\n\u001b[32m     61\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m chunk_size = \u001b[38;5;28mself\u001b[39m._buffer.pop_u16()\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunk_size == \u001b[32m0\u001b[39m:\n\u001b[32m     65\u001b[39m     \u001b[38;5;66;03m# chunk_size was the end marker for the message\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matia\\OneDrive\\Escritorio\\Nastia_BSE\\UN_Conflict_Report\\.venv\\Lib\\site-packages\\neo4j\\_sync\\io\\_common.py:342\u001b[39m, in \u001b[36mreceive_into_buffer\u001b[39m\u001b[34m(sock, buffer, n_bytes)\u001b[39m\n\u001b[32m    340\u001b[39m end = buffer.used + n_bytes\n\u001b[32m    341\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m end > \u001b[38;5;28mlen\u001b[39m(buffer.data):\n\u001b[32m--> \u001b[39m\u001b[32m342\u001b[39m     \u001b[43mbuffer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbytearray\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mend\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(buffer.data) \u001b[38;5;28;01mas\u001b[39;00m view:\n\u001b[32m    344\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m buffer.used < end:\n",
      "\u001b[31mBufferError\u001b[39m: Existing exports of data: object cannot be re-sized"
     ]
    }
   ],
   "source": [
    "# Connect to Neo4j\n",
    "driver = GraphDatabase.driver(\n",
    "    os.getenv(\"NEO4J_URI\"),\n",
    "    auth=(\"neo4j\", os.getenv(\"NEO4J_PASSWORD\"))\n",
    ")\n",
    "\n",
    "# Load embedding model\n",
    "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n",
    "\n",
    "def get_embedding(text):\n",
    "    return model.encode(text).tolist()\n",
    "\n",
    "# List of relevant node labels for deduplication\n",
    "ENTITY_LABELS = [\"Event\", \"Actor\", \"Location\"]\n",
    "\n",
    "def get_all_entities():\n",
    "    query_template = \"\"\"\n",
    "    MATCH (n:{label})\n",
    "    RETURN elementId(n) AS id, n.name AS name, labels(n) AS labels, properties(n) AS properties\n",
    "    \"\"\"\n",
    "    all_entities = []\n",
    "    with driver.session() as session:\n",
    "        for label in ENTITY_LABELS:\n",
    "            result = session.run(query_template.format(label=label)).data()\n",
    "            all_entities.extend(result)\n",
    "    return all_entities\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    if vec1 is None or vec2 is None or len(vec1) == 0 or len(vec2) == 0:\n",
    "        return 0  # If either vector is None or empty, return 0 similarity\n",
    "    dot = np.dot(vec1, vec2)\n",
    "    norm1 = np.linalg.norm(vec1)\n",
    "    norm2 = np.linalg.norm(vec2)\n",
    "    return dot / (norm1 * norm2) if norm1 > 0 and norm2 > 0 else 0\n",
    "\n",
    "def find_similar_entities(threshold=0.7):\n",
    "    entities = get_all_entities()\n",
    "    print(f\"Found {len(entities)} entities to process\")\n",
    "    \n",
    "    # Compute embeddings\n",
    "    for entity in entities:\n",
    "        # Filter out \"__KGBuilder__\" from labels\n",
    "        filtered_labels = [l for l in entity['labels'] if l != \"__KGBuilder__\" and l != \"__Entity__\"]\n",
    "        if filtered_labels:  # If there are any labels left\n",
    "            entity['primary_label'] = filtered_labels[0]\n",
    "        else:\n",
    "            entity['primary_label'] = entity['labels'][0]  # Fallback if only __KGBuilder__ is present\n",
    "            \n",
    "        # Start with entity label/type and name\n",
    "        text = f\"Type: {entity['primary_label']}\\nName: {entity['name']}\\n\"\n",
    "        \n",
    "        # Rest of embedding code remains the same\n",
    "        for key, value in entity['properties'].items():\n",
    "            if key != 'embedding' and value is not None:\n",
    "                if isinstance(value, (list, dict)):\n",
    "                    formatted_value = str(value)\n",
    "                else:\n",
    "                    formatted_value = value\n",
    "                text += f\"{key}: {formatted_value}\\n\"\n",
    "        \n",
    "        entity['embedding'] = get_embedding(text)\n",
    "     \n",
    "    # Find similar pairs with the new label comparison\n",
    "    similar_pairs = []\n",
    "    total_comparisons = sum(range(len(entities)))\n",
    "    for i, e1 in enumerate(entities):\n",
    "        for j, e2 in enumerate(entities[i + 1:], i + 1):\n",
    "            # Only compare if they have the same primary label (excluding __KGBuilder__)\n",
    "            if e1['primary_label'] != e2['primary_label'] or e1['primary_label'] == \"__KGBuilder__\":\n",
    "                continue\n",
    "            \n",
    "            sim = cosine_similarity(e1['embedding'], e2['embedding'])\n",
    "            if sim > threshold:\n",
    "                similar_pairs.append({\n",
    "                    \"id1\": e1['id'],\n",
    "                    \"id2\": e2['id'],\n",
    "                    \"name1\": e1['name'],\n",
    "                    \"name2\": e2['name'],\n",
    "                    \"type1\": e1['primary_label'],\n",
    "                    \"type2\": e2['primary_label'],\n",
    "                    \"similarity\": sim\n",
    "                })\n",
    "    \n",
    "    # Create SAME_AS relationships\n",
    "    query = \"\"\"\n",
    "    MATCH (a), (b)\n",
    "    WHERE elementId(a) = $id1 AND elementId(b) = $id2\n",
    "    MERGE (a)-[:SAME_AS {similarity: $similarity}]->(b)\n",
    "    \"\"\"\n",
    "    with driver.session() as session:\n",
    "        for pair in tqdm.tqdm(similar_pairs, desc=\"Computing similarity scores\"):\n",
    "            session.run(query, pair)\n",
    "    \n",
    "    return similar_pairs\n",
    "\n",
    "def check_apoc():\n",
    "    try:\n",
    "        with driver.session() as session:\n",
    "            session.run(\"CALL apoc.help('create')\")\n",
    "            print(\"APOC is available.\")\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        print(f\"APOC not available: {e}\")\n",
    "        return False\n",
    "\n",
    "# === MAIN EXECUTION ===\n",
    "check_apoc()\n",
    "pairs = find_similar_entities()\n",
    "pairs = sorted(pairs, key=lambda x: x[\"similarity\"], reverse=True)\n",
    "print(f\"Found {len(pairs)} similar entity pairs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4205df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b7bc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_similar_nodes():\n",
    "    merge_query = \"\"\"\n",
    "    // Process one pair of nodes at a time to avoid conflicts\n",
    "    MATCH (n1)-[r:SAME_AS]->(n2)\n",
    "    WHERE n1 IS NOT NULL AND n2 IS NOT NULL\n",
    "    \n",
    "    // Copy properties from n2 to n1 if they don't exist in n1\n",
    "    WITH n1, n2, [key IN keys(n2) WHERE NOT key IN keys(n1)] AS newKeys\n",
    "    FOREACH (key IN newKeys | SET n1[key] = n2[key])\n",
    "    \n",
    "    // Get all outgoing relationships from n2 (except SAME_AS)\n",
    "    WITH n1, n2\n",
    "    OPTIONAL MATCH (n2)-[outRel]->(target)\n",
    "    WHERE target IS NOT NULL AND type(outRel) <> 'SAME_AS'\n",
    "    \n",
    "    // Create equivalent relationships from n1 if they don't already exist\n",
    "    WITH n1, n2, outRel, target, type(outRel) AS relType\n",
    "    WHERE NOT EXISTS((n1)-[:`${relType}`]->(target))\n",
    "    \n",
    "    // Create new relationship with the same properties\n",
    "    FOREACH (_ IN CASE WHEN outRel IS NOT NULL THEN [1] ELSE [] END |\n",
    "        CREATE (n1)-[newRel:`${relType}`]->(target)\n",
    "        SET newRel = properties(outRel)\n",
    "    )\n",
    "    \n",
    "    // Return the node pair for the next phase\n",
    "    WITH DISTINCT n1, n2\n",
    "    \n",
    "    // Handle incoming relationships\n",
    "    OPTIONAL MATCH (source)-[inRel]->(n2)\n",
    "    WHERE source IS NOT NULL AND source <> n1 AND type(inRel) <> 'SAME_AS'\n",
    "    \n",
    "    // Create equivalent relationships to n1 if they don't already exist\n",
    "    WITH n1, n2, inRel, source, type(inRel) AS relType\n",
    "    WHERE NOT EXISTS((source)-[:`${relType}`]->(n1))\n",
    "    \n",
    "    // Create new relationship with the same properties\n",
    "    FOREACH (_ IN CASE WHEN inRel IS NOT NULL THEN [1] ELSE [] END |\n",
    "        CREATE (source)-[newRel:`${relType}`]->(n1)\n",
    "        SET newRel = properties(inRel)\n",
    "    )\n",
    "    \n",
    "    // Return distinct pairs for deletion phase\n",
    "    WITH DISTINCT n1, n2\n",
    "    \n",
    "    // Delete the second node and all its relationships\n",
    "    DETACH DELETE n2\n",
    "    \n",
    "    RETURN count(n2) AS mergedCount\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        with driver.session() as session:\n",
    "            result = session.run(merge_query)\n",
    "            record = result.single()\n",
    "            return record[\"mergedCount\"] if record else 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error during node merging: {e}\")\n",
    "        return 0\n",
    "\n",
    "merged = merge_similar_nodes()\n",
    "print(f\"Merged {merged} nodes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c57305",
   "metadata": {},
   "source": [
    "### Node similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc40641b",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_similarity_query = \"\"\"\n",
    "CALL gds.nodeSimilarity.stream('amazonGraph')\n",
    "YIELD node1, node2, similarity as node_similarity\n",
    "WHERE 'Company' IN labels(gds.util.asNode(node1)) AND 'Company' IN labels(gds.util.asNode(node2))\n",
    "AND node_similarity < 1\n",
    "RETURN gds.util.asNode(node1).name AS Company1, gds.util.asNode(node2).name AS Company2, node_similarity\n",
    "ORDER BY node_similarity DESCENDING, Company1, Company2\n",
    "\"\"\"\n",
    "\n",
    "def results_to_df(query: str) -> pd.DataFrame:\n",
    "    results = gds.execute_query(query)[0]\n",
    "    df = pd.DataFrame(results, columns=results[0].keys())\n",
    "    return df\n",
    "\n",
    "df_node_similarity = results_to_df(node_similarity_query)\n",
    "print(df_node_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb28fec",
   "metadata": {},
   "source": [
    "### Create SAME_AS relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e661965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_same_as_relationship(df, column_name):\n",
    "    # Iterate over the DataFrame rows\n",
    "    for _, row in df.iterrows():\n",
    "        node1 = row[column_name + '1']\n",
    "        node2 = row[column_name + '2']\n",
    "\n",
    "        # Run Cypher query to create 'SAME_AS' relationship\n",
    "        score = row[\"combined_score\"]\n",
    "        if score > 0.20:\n",
    "            query = f\"MATCH (n1), (n2) WHERE n1.name = '{node1}' AND n2.name = '{node2}' CREATE (n1)-[:SAME_AS]->(n2)\"\n",
    "            gds.execute_query(query)\n",
    "\n",
    "create_same_as_relationship(selected_df, \"Company\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52dbed9",
   "metadata": {},
   "source": [
    "### Merge nodes with SAME_AS relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260a3328",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_query = \"\"\"\n",
    "MATCH (n1)-[r:SAME_AS]->(n2)\n",
    "WITH n1, n2, collect(r) as relsToDel\n",
    "\n",
    "FOREACH (rel IN relsToDel | DELETE rel)\n",
    "WITH collect(DISTINCT n1) + collect(DISTINCT n2) AS nodesToMerge\n",
    "\n",
    "UNWIND nodesToMerge AS node\n",
    "\n",
    "WITH collect(DISTINCT node) AS uniqueNodesToMerge\n",
    "CALL apoc.refactor.mergeNodes(uniqueNodesToMerge, {mergeRels:true}) YIELD node\n",
    "RETURN node\n",
    "\"\"\"\n",
    "\n",
    "gds.execute_query(merge_query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
