{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63555e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up paths\n",
    "data_dir = Path(\"../data\")\n",
    "print(f\"Data directory: {data_dir.absolute()}\")\n",
    "print(f\"Directory exists: {data_dir.exists()}\")\n",
    "\n",
    "# Find all parquet files\n",
    "parquet_files = list(data_dir.rglob(\"*.parquet\"))\n",
    "print(f\"\\nFound {len(parquet_files)} parquet files:\")\n",
    "for file in parquet_files:\n",
    "    print(f\"  - {file.relative_to(data_dir)}\")\n",
    "    print(f\"    Size: {file.stat().st_size / (1024*1024):.2f} MB\")\n",
    "    print(f\"    Modified: {datetime.fromtimestamp(file.stat().st_mtime)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99c9a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and explore ACLED data\n",
    "acled_files = [f for f in parquet_files if 'acled' in f.name.lower()]\n",
    "if acled_files:\n",
    "    print(f\"Loading ACLED data from: {acled_files[0].name}\")\n",
    "    acled_df = pl.read_parquet(acled_files[0])\n",
    "    \n",
    "    print(f\"\\nACLED Dataset Overview:\")\n",
    "    print(f\"Shape: {acled_df.shape}\")\n",
    "    print(f\"Columns: {acled_df.columns}\")\n",
    "    \n",
    "    # Convert to pandas for plotting\n",
    "    acled_pd = acled_df.to_pandas()\n",
    "    \n",
    "    # Display basic info\n",
    "    print(f\"\\nData types:\")\n",
    "    for col in acled_df.columns:\n",
    "        print(f\"  {col}: {acled_df[col].dtype}\")\n",
    "    \n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    print(acled_pd.head())\n",
    "else:\n",
    "    print(\"No ACLED files found\")\n",
    "    acled_df = None\n",
    "    acled_pd = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c4c420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal Distribution Analysis\n",
    "if acled_pd is not None and 'date' in acled_pd.columns:\n",
    "    # Convert date column if it's not already datetime\n",
    "    if not pd.api.types.is_datetime64_any_dtype(acled_pd['date']):\n",
    "        acled_pd['date'] = pd.to_datetime(acled_pd['date'])\n",
    "    \n",
    "    # Create temporal plots\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Events Over Time', 'Events by Month', 'Events by Year', 'Events by Day of Week'),\n",
    "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "    )\n",
    "    \n",
    "    # 1. Time series plot\n",
    "    daily_counts = acled_pd.groupby(acled_pd['date'].dt.date).size()\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=daily_counts.index, y=daily_counts.values, mode='lines', name='Daily Events'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # 2. Monthly distribution\n",
    "    monthly_counts = acled_pd.groupby(acled_pd['date'].dt.month).size()\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=monthly_counts.index, y=monthly_counts.values, name='Monthly Distribution'),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # 3. Yearly distribution\n",
    "    yearly_counts = acled_pd.groupby(acled_pd['date'].dt.year).size()\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=yearly_counts.index, y=yearly_counts.values, name='Yearly Distribution'),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # 4. Day of week distribution\n",
    "    dow_counts = acled_pd.groupby(acled_pd['date'].dt.day_name()).size()\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=dow_counts.index, y=dow_counts.values, name='Day of Week Distribution'),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(height=800, title_text=\"ACLED Temporal Distribution Analysis\", showlegend=False)\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"Date column not found or ACLED data not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f265fb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Event Type and Geographical Distribution\n",
    "if acled_pd is not None:\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Event Types', 'Sub Event Types (Top 15)', 'Admin1 Regions (Top 15)', 'Fatalities Distribution'),\n",
    "        specs=[[{\"type\": \"pie\"}, {\"type\": \"bar\"}],\n",
    "               [{\"type\": \"bar\"}, {\"type\": \"histogram\"}]]\n",
    "    )\n",
    "    \n",
    "    # 1. Event types pie chart\n",
    "    if 'event_type' in acled_pd.columns:\n",
    "        event_type_counts = acled_pd['event_type'].value_counts()\n",
    "        fig.add_trace(\n",
    "            go.Pie(labels=event_type_counts.index, values=event_type_counts.values, name=\"Event Types\"),\n",
    "            row=1, col=1\n",
    "        )\n",
    "    \n",
    "    # 2. Sub event types bar chart (top 15)\n",
    "    if 'sub_event_type' in acled_pd.columns:\n",
    "        sub_event_counts = acled_pd['sub_event_type'].value_counts().head(15)\n",
    "        fig.add_trace(\n",
    "            go.Bar(x=sub_event_counts.values, y=sub_event_counts.index, orientation='h', name='Sub Event Types'),\n",
    "            row=1, col=2\n",
    "        )\n",
    "    \n",
    "    # 3. Admin1 regions bar chart (top 15)\n",
    "    if 'admin1' in acled_pd.columns:\n",
    "        admin1_counts = acled_pd['admin1'].value_counts().head(15)\n",
    "        fig.add_trace(\n",
    "            go.Bar(x=admin1_counts.values, y=admin1_counts.index, orientation='h', name='Admin1 Regions'),\n",
    "            row=2, col=1\n",
    "        )\n",
    "    \n",
    "    # 4. Fatalities distribution\n",
    "    if 'fatalities' in acled_pd.columns:\n",
    "        # Filter out extreme outliers for better visualization\n",
    "        fatalities_filtered = acled_pd['fatalities'][acled_pd['fatalities'] <= 100]\n",
    "        fig.add_trace(\n",
    "            go.Histogram(x=fatalities_filtered, name='Fatalities Distribution', nbinsx=50),\n",
    "            row=2, col=2\n",
    "        )\n",
    "    \n",
    "    fig.update_layout(height=1000, title_text=\"ACLED Event Type and Geographical Distribution\", showlegend=False)\n",
    "    fig.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\n=== ACLED Summary Statistics ===\")\n",
    "    if 'fatalities' in acled_pd.columns:\n",
    "        print(f\"Total fatalities: {acled_pd['fatalities'].sum():,}\")\n",
    "        print(f\"Average fatalities per event: {acled_pd['fatalities'].mean():.2f}\")\n",
    "        print(f\"Median fatalities per event: {acled_pd['fatalities'].median():.2f}\")\n",
    "        print(f\"Max fatalities in single event: {acled_pd['fatalities'].max():,}\")\n",
    "    \n",
    "    print(f\"Total events: {len(acled_pd):,}\")\n",
    "    if 'date' in acled_pd.columns:\n",
    "        print(f\"Date range: {acled_pd['date'].min()} to {acled_pd['date'].max()}\")\n",
    "    \n",
    "    if 'country' in acled_pd.columns:\n",
    "        print(f\"Countries covered: {acled_pd['country'].nunique()}\")\n",
    "        print(f\"Most active country: {acled_pd['country'].value_counts().index[0]} ({acled_pd['country'].value_counts().iloc[0]:,} events)\")\n",
    "else:\n",
    "    print(\"ACLED data not available for analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f001afa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and analyze Factal data\n",
    "factal_files = [f for f in parquet_files if 'factal' in f.name.lower()]\n",
    "if factal_files:\n",
    "    print(f\"Loading Factal data from: {factal_files[0].name}\")\n",
    "    factal_df = pl.read_parquet(factal_files[0])\n",
    "    factal_pd = factal_df.to_pandas()\n",
    "    \n",
    "    print(f\"\\nFactal Dataset Overview:\")\n",
    "    print(f\"Shape: {factal_df.shape}\")\n",
    "    print(f\"Columns: {factal_df.columns}\")\n",
    "    \n",
    "    # Factal-specific visualizations\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Severity Distribution', 'Theme Distribution', 'Content Length Distribution', 'Temporal Distribution'),\n",
    "        specs=[[{\"type\": \"bar\"}, {\"type\": \"pie\"}],\n",
    "               [{\"type\": \"histogram\"}, {\"type\": \"scatter\"}]]\n",
    "    )\n",
    "    \n",
    "    # 1. Severity distribution\n",
    "    if 'severity' in factal_pd.columns:\n",
    "        severity_counts = factal_pd['severity'].value_counts().sort_index()\n",
    "        fig.add_trace(\n",
    "            go.Bar(x=severity_counts.index, y=severity_counts.values, name='Severity Distribution'),\n",
    "            row=1, col=1\n",
    "        )\n",
    "    \n",
    "    # 2. Theme distribution\n",
    "    if 'theme' in factal_pd.columns:\n",
    "        theme_counts = factal_pd['theme'].value_counts().head(10)\n",
    "        fig.add_trace(\n",
    "            go.Pie(labels=theme_counts.index, values=theme_counts.values, name=\"Themes\"),\n",
    "            row=1, col=2\n",
    "        )\n",
    "    \n",
    "    # 3. Content length distribution\n",
    "    if 'text' in factal_pd.columns:\n",
    "        text_lengths = factal_pd['text'].str.len()\n",
    "        fig.add_trace(\n",
    "            go.Histogram(x=text_lengths, name='Text Length Distribution', nbinsx=50),\n",
    "            row=2, col=1\n",
    "        )\n",
    "    \n",
    "    # 4. Temporal scatter plot\n",
    "    if 'date' in factal_pd.columns and 'severity' in factal_pd.columns:\n",
    "        if not pd.api.types.is_datetime64_any_dtype(factal_pd['date']):\n",
    "            factal_pd['date'] = pd.to_datetime(factal_pd['date'])\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=factal_pd['date'], y=factal_pd['severity'], mode='markers', \n",
    "                      name='Severity over Time', opacity=0.6),\n",
    "            row=2, col=2\n",
    "        )\n",
    "    \n",
    "    fig.update_layout(height=1000, title_text=\"Factal Data Distribution Analysis\", showlegend=False)\n",
    "    fig.show()\n",
    "    \n",
    "    # Print Factal summary statistics\n",
    "    print(\"\\n=== Factal Summary Statistics ===\")\n",
    "    print(f\"Total items: {len(factal_pd):,}\")\n",
    "    if 'date' in factal_pd.columns:\n",
    "        print(f\"Date range: {factal_pd['date'].min()} to {factal_pd['date'].max()}\")\n",
    "    if 'severity' in factal_pd.columns:\n",
    "        print(f\"Average severity: {factal_pd['severity'].mean():.2f}\")\n",
    "        print(f\"Severity distribution: {factal_pd['severity'].value_counts().sort_index().to_dict()}\")\n",
    "    if 'text' in factal_pd.columns:\n",
    "        print(f\"Average text length: {factal_pd['text'].str.len().mean():.0f} characters\")\n",
    "        \n",
    "else:\n",
    "    print(\"No Factal files found\")\n",
    "    factal_df = None\n",
    "    factal_pd = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624e8bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and analyze Google News data\n",
    "google_files = [f for f in parquet_files if 'google' in f.name.lower()]\n",
    "if google_files:\n",
    "    print(f\"Loading Google News data from: {google_files[0].name}\")\n",
    "    google_df = pl.read_parquet(google_files[0])\n",
    "    google_pd = google_df.to_pandas()\n",
    "    \n",
    "    print(f\"\\nGoogle News Dataset Overview:\")\n",
    "    print(f\"Shape: {google_df.shape}\")\n",
    "    print(f\"Columns: {google_df.columns}\")\n",
    "    \n",
    "    # Google News specific visualizations\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Article Length Distribution', 'Source Distribution (Top 15)', 'Publication Timeline', 'Articles per Day'),\n",
    "        specs=[[{\"type\": \"histogram\"}, {\"type\": \"bar\"}],\n",
    "               [{\"type\": \"scatter\"}, {\"type\": \"bar\"}]]\n",
    "    )\n",
    "    \n",
    "    # 1. Article length distribution\n",
    "    if 'text' in google_pd.columns:\n",
    "        text_lengths = google_pd['text'].str.len()\n",
    "        fig.add_trace(\n",
    "            go.Histogram(x=text_lengths, name='Article Length Distribution', nbinsx=50),\n",
    "            row=1, col=1\n",
    "        )\n",
    "    \n",
    "    # 2. Source distribution\n",
    "    if 'source' in google_pd.columns:\n",
    "        source_counts = google_pd['source'].value_counts().head(15)\n",
    "        fig.add_trace(\n",
    "            go.Bar(x=source_counts.values, y=source_counts.index, orientation='h', name='Source Distribution'),\n",
    "            row=1, col=2\n",
    "        )\n",
    "    \n",
    "    # 3. Publication timeline\n",
    "    if 'date' in google_pd.columns:\n",
    "        if not pd.api.types.is_datetime64_any_dtype(google_pd['date']):\n",
    "            google_pd['date'] = pd.to_datetime(google_pd['date'])\n",
    "        \n",
    "        # Cumulative articles over time\n",
    "        daily_counts = google_pd.groupby(google_pd['date'].dt.date).size().sort_index()\n",
    "        cumulative_counts = daily_counts.cumsum()\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=cumulative_counts.index, y=cumulative_counts.values, mode='lines', \n",
    "                      name='Cumulative Articles'),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # Daily article counts\n",
    "        fig.add_trace(\n",
    "            go.Bar(x=daily_counts.index, y=daily_counts.values, name='Daily Articles'),\n",
    "            row=2, col=2\n",
    "        )\n",
    "    \n",
    "    fig.update_layout(height=1000, title_text=\"Google News Data Distribution Analysis\", showlegend=False)\n",
    "    fig.show()\n",
    "    \n",
    "    # Print Google News summary statistics\n",
    "    print(\"\\n=== Google News Summary Statistics ===\")\n",
    "    print(f\"Total articles: {len(google_pd):,}\")\n",
    "    if 'date' in google_pd.columns:\n",
    "        print(f\"Date range: {google_pd['date'].min()} to {google_pd['date'].max()}\")\n",
    "    if 'source' in google_pd.columns:\n",
    "        print(f\"Number of unique sources: {google_pd['source'].nunique()}\")\n",
    "        print(f\"Most active source: {google_pd['source'].value_counts().index[0]} ({google_pd['source'].value_counts().iloc[0]:,} articles)\")\n",
    "    if 'text' in google_pd.columns:\n",
    "        print(f\"Average article length: {google_pd['text'].str.len().mean():.0f} characters\")\n",
    "        \n",
    "else:\n",
    "    print(\"No Google News files found\")\n",
    "    google_df = None\n",
    "    google_pd = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ba58c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparative Analysis Across Data Sources\n",
    "available_sources = []\n",
    "source_info = {}\n",
    "\n",
    "if acled_pd is not None:\n",
    "    available_sources.append('ACLED')\n",
    "    source_info['ACLED'] = {\n",
    "        'count': len(acled_pd),\n",
    "        'date_range': (acled_pd['date'].min(), acled_pd['date'].max()) if 'date' in acled_pd.columns else (None, None),\n",
    "        'avg_text_length': acled_pd['text'].str.len().mean() if 'text' in acled_pd.columns else None\n",
    "    }\n",
    "\n",
    "if factal_pd is not None:\n",
    "    available_sources.append('Factal')\n",
    "    source_info['Factal'] = {\n",
    "        'count': len(factal_pd),\n",
    "        'date_range': (factal_pd['date'].min(), factal_pd['date'].max()) if 'date' in factal_pd.columns else (None, None),\n",
    "        'avg_text_length': factal_pd['text'].str.len().mean() if 'text' in factal_pd.columns else None\n",
    "    }\n",
    "\n",
    "if google_pd is not None:\n",
    "    available_sources.append('Google News')\n",
    "    source_info['Google News'] = {\n",
    "        'count': len(google_pd),\n",
    "        'date_range': (google_pd['date'].min(), google_pd['date'].max()) if 'date' in google_pd.columns else (None, None),\n",
    "        'avg_text_length': google_pd['text'].str.len().mean() if 'text' in google_pd.columns else None\n",
    "    }\n",
    "\n",
    "if available_sources:\n",
    "    # Create comparative visualization\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Data Volume Comparison', 'Average Text Length Comparison', \n",
    "                       'Temporal Coverage', 'Combined Timeline'),\n",
    "        specs=[[{\"type\": \"bar\"}, {\"type\": \"bar\"}],\n",
    "               [{\"type\": \"bar\"}, {\"type\": \"scatter\"}]]\n",
    "    )\n",
    "    \n",
    "    # 1. Data volume comparison\n",
    "    volumes = [source_info[source]['count'] for source in available_sources]\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=available_sources, y=volumes, name='Record Count'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # 2. Average text length comparison\n",
    "    text_lengths = [source_info[source]['avg_text_length'] for source in available_sources \n",
    "                   if source_info[source]['avg_text_length'] is not None]\n",
    "    sources_with_text = [source for source in available_sources \n",
    "                        if source_info[source]['avg_text_length'] is not None]\n",
    "    \n",
    "    if text_lengths:\n",
    "        fig.add_trace(\n",
    "            go.Bar(x=sources_with_text, y=text_lengths, name='Avg Text Length'),\n",
    "            row=1, col=2\n",
    "        )\n",
    "    \n",
    "    # 3. Temporal coverage (days covered)\n",
    "    coverage_days = []\n",
    "    for source in available_sources:\n",
    "        if source_info[source]['date_range'][0] is not None:\n",
    "            start, end = source_info[source]['date_range']\n",
    "            days = (end - start).days\n",
    "            coverage_days.append(days)\n",
    "        else:\n",
    "            coverage_days.append(0)\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(x=available_sources, y=coverage_days, name='Days Covered'),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # 4. Combined timeline (if date data available)\n",
    "    colors = ['blue', 'red', 'green']\n",
    "    for i, source in enumerate(available_sources):\n",
    "        if source == 'ACLED' and acled_pd is not None and 'date' in acled_pd.columns:\n",
    "            daily_counts = acled_pd.groupby(acled_pd['date'].dt.date).size()\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=daily_counts.index, y=daily_counts.values, mode='lines', \n",
    "                          name='ACLED', line=dict(color=colors[i])),\n",
    "                row=2, col=2\n",
    "            )\n",
    "        elif source == 'Factal' and factal_pd is not None and 'date' in factal_pd.columns:\n",
    "            daily_counts = factal_pd.groupby(factal_pd['date'].dt.date).size()\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=daily_counts.index, y=daily_counts.values, mode='lines', \n",
    "                          name='Factal', line=dict(color=colors[i])),\n",
    "                row=2, col=2\n",
    "            )\n",
    "        elif source == 'Google News' and google_pd is not None and 'date' in google_pd.columns:\n",
    "            daily_counts = google_pd.groupby(google_pd['date'].dt.date).size()\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=daily_counts.index, y=daily_counts.values, mode='lines', \n",
    "                          name='Google News', line=dict(color=colors[i])),\n",
    "                row=2, col=2\n",
    "            )\n",
    "    \n",
    "    fig.update_layout(height=1000, title_text=\"Comparative Analysis of Data Sources\", showlegend=True)\n",
    "    fig.show()\n",
    "    \n",
    "    # Print comprehensive summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"COMPREHENSIVE DATA SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for source in available_sources:\n",
    "        info = source_info[source]\n",
    "        print(f\"\\n{source}:\")\n",
    "        print(f\"  Records: {info['count']:,}\")\n",
    "        if info['date_range'][0] is not None:\n",
    "            print(f\"  Date Range: {info['date_range'][0]} to {info['date_range'][1]}\")\n",
    "            print(f\"  Coverage: {(info['date_range'][1] - info['date_range'][0]).days} days\")\n",
    "        if info['avg_text_length'] is not None:\n",
    "            print(f\"  Average Text Length: {info['avg_text_length']:.0f} characters\")\n",
    "    \n",
    "    total_records = sum(volumes)\n",
    "    print(f\"\\nTOTAL RECORDS ACROSS ALL SOURCES: {total_records:,}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No data sources available for comparative analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7dd045",
   "metadata": {},
   "source": [
    "# Data Distribution Analysis Summary\n",
    "\n",
    "This notebook provides comprehensive exploratory data analysis (EDA) of the conflict-related datasets stored in the GraphRAG pipeline data folder. The analysis covers:\n",
    "\n",
    "## Key Insights:\n",
    "\n",
    "### ðŸ“Š **ACLED Data**\n",
    "- **Temporal patterns**: Shows conflict event distributions over time, seasonal patterns, and day-of-week variations\n",
    "- **Event classification**: Breaks down different types of conflict events and their frequency\n",
    "- **Geographic distribution**: Identifies hotspot regions and conflict concentration areas\n",
    "- **Severity metrics**: Analyzes fatality distributions and event impact\n",
    "\n",
    "### ðŸš¨ **Factal Data** \n",
    "- **Intelligence severity**: Tracks threat level distributions on a 1-4 scale\n",
    "- **Thematic analysis**: Shows categorization of different threat types\n",
    "- **Content analysis**: Examines the depth and length of intelligence reports\n",
    "- **Real-time patterns**: Reveals temporal patterns in threat detection\n",
    "\n",
    "### ðŸ“° **Google News Data**\n",
    "- **Media coverage**: Analyzes news article volume and source diversity\n",
    "- **Content characteristics**: Examines article length and coverage patterns\n",
    "- **Publication timeline**: Shows news reporting patterns over time\n",
    "- **Source analysis**: Identifies most active news sources and coverage distribution\n",
    "\n",
    "### ðŸ”„ **Cross-Source Comparison**\n",
    "- **Data volume comparison**: Shows relative contribution of each data source\n",
    "- **Temporal alignment**: Identifies overlap and gaps in temporal coverage\n",
    "- **Content depth**: Compares text length and information density across sources\n",
    "- **Complementary coverage**: Demonstrates how different sources provide unique perspectives\n",
    "\n",
    "## Usage for GraphRAG:\n",
    "This EDA provides the foundation for understanding data characteristics that will influence:\n",
    "- **Entity extraction strategies** (based on text length and complexity)\n",
    "- **Temporal modeling approaches** (based on coverage patterns)\n",
    "- **Geographic analysis scope** (based on spatial distribution)\n",
    "- **Multi-source integration methods** (based on complementary patterns)\n",
    "\n",
    "The insights from this analysis inform the preprocessing, knowledge graph construction, and query optimization strategies in the broader GraphRAG security reporting system."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
